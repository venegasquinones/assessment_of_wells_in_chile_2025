{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7817d1cf-8456-44f3-962e-69a056adc84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import warnings\n",
    "import os\n",
    "import gc\n",
    "from datetime import datetime\n",
    "import json\n",
    "import logging\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['CMDSTAN_VERBOSE'] = 'false'\n",
    "\n",
    "logging.getLogger('cmdstanpy').setLevel(logging.CRITICAL)\n",
    "logging.getLogger('prophet').setLevel(logging.CRITICAL)\n",
    "logging.getLogger('fbprophet').setLevel(logging.CRITICAL)\n",
    "logging.getLogger('tensorflow').setLevel(logging.CRITICAL)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.autograph.set_verbosity(0)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "try:\n",
    "    from prophet import Prophet\n",
    "    PROPHET_AVAILABLE = True\n",
    "except ImportError:\n",
    "    try:\n",
    "        from fbprophet import Prophet\n",
    "        PROPHET_AVAILABLE = True\n",
    "    except ImportError:\n",
    "        PROPHET_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import pymannkendall as mk\n",
    "    MK_AVAILABLE = True\n",
    "except ImportError:\n",
    "    MK_AVAILABLE = False\n",
    "\n",
    "MIN_RECENT_YEAR = 2020\n",
    "MIN_RECORDS = 24\n",
    "MIN_YEARS_SPAN = 5\n",
    "PREDICTION_YEAR = 2030\n",
    "\n",
    "LSTM_LOOKBACK = 12\n",
    "LSTM_EPOCHS = 150\n",
    "LSTM_BATCH_SIZE = 16\n",
    "\n",
    "SIGNIFICANCE_LEVEL = 0.05\n",
    "CONFIDENCE_LEVEL = 0.95\n",
    "\n",
    "output_folder = r\"\\assessment_of_wells_chile\\data\\DGA\\Trend_Analysis_Predictions_Complete_v2\"\n",
    "\n",
    "for folder in ['Excel', 'Figures', 'Text_Output', 'Models', 'Individual_Wells']:\n",
    "    path = os.path.join(output_folder, folder)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "log_file_path = os.path.join(output_folder, 'Text_Output', 'process_log_realtime.txt')\n",
    "checkpoint_file = os.path.join(output_folder, 'checkpoint_well_results.csv')\n",
    "progress_file = os.path.join(output_folder, 'progress_tracker.json')\n",
    "\n",
    "def load_progress():\n",
    "    processed_wells = set()\n",
    "    well_results = []\n",
    "    if os.path.exists(progress_file):\n",
    "        try:\n",
    "            with open(progress_file, 'r') as f:\n",
    "                progress_data = json.load(f)\n",
    "                processed_wells = set(progress_data.get('processed_wells', []))\n",
    "        except:\n",
    "            processed_wells = set()\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        try:\n",
    "            df_checkpoint = pd.read_csv(checkpoint_file)\n",
    "            well_results = df_checkpoint.to_dict('records')\n",
    "            for result in well_results:\n",
    "                processed_wells.add(result['Station_Code'])\n",
    "        except:\n",
    "            well_results = []\n",
    "    return processed_wells, well_results\n",
    "\n",
    "def save_progress(processed_wells, well_results, current_idx, total_wells):\n",
    "    try:\n",
    "        progress_data = {\n",
    "            'processed_wells': list(processed_wells),\n",
    "            'last_index': current_idx,\n",
    "            'total_wells': total_wells,\n",
    "            'last_update': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "        with open(progress_file, 'w') as f:\n",
    "            json.dump(progress_data, f)\n",
    "        if well_results:\n",
    "            pd.DataFrame(well_results).to_csv(checkpoint_file, index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving progress: {e}\")\n",
    "\n",
    "def write_output(text):\n",
    "    print(text)\n",
    "    try:\n",
    "        with open(log_file_path, 'a', encoding='utf-8') as f:\n",
    "            f.write(text + \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to log file: {e}\")\n",
    "\n",
    "if not os.path.exists(log_file_path):\n",
    "    with open(log_file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"STARTING PROCESS AT: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "else:\n",
    "    with open(log_file_path, 'a', encoding='utf-8') as f:\n",
    "        f.write(f\"\\nRESUMING PROCESS AT: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "\n",
    "path_piezometric = r\"\\assessment_of_wells_chile\\data\\DGA_aguas_subterraneas_12_2025\\niveles_estaticos_pozos_historico\\output\\niveles_estaticos_todos.shp\"\n",
    "\n",
    "piez_codigo_col = 'COD_EST'\n",
    "piez_nombre_col = 'NOM_EST'\n",
    "piez_fecha_col = 'FECHA_US'\n",
    "piez_nivel_col = 'NIVEL'\n",
    "piez_lat_col = 'LAT_WGS84'\n",
    "piez_lon_col = 'LON_WGS84'\n",
    "\n",
    "def normalize_id_with_padding(id_value, target_length=8):\n",
    "    id_str = str(id_value).strip()\n",
    "    if '.' in id_str:\n",
    "        id_str = id_str.split('.')[0]\n",
    "    try:\n",
    "        if 'e' in id_str.lower() or 'E' in id_str:\n",
    "            id_str = str(int(float(id_str)))\n",
    "    except:\n",
    "        pass\n",
    "    if id_str.isdigit():\n",
    "        id_str = id_str.zfill(target_length)\n",
    "    return id_str\n",
    "\n",
    "def nash_sutcliffe_efficiency(observed, predicted):\n",
    "    observed = np.array(observed).flatten()\n",
    "    predicted = np.array(predicted).flatten()\n",
    "    mask = ~(np.isnan(observed) | np.isnan(predicted))\n",
    "    observed = observed[mask]\n",
    "    predicted = predicted[mask]\n",
    "    if len(observed) < 2:\n",
    "        return np.nan\n",
    "    numerator = np.sum((observed - predicted) ** 2)\n",
    "    denominator = np.sum((observed - np.mean(observed)) ** 2)\n",
    "    if denominator == 0:\n",
    "        return np.nan\n",
    "    return 1 - (numerator / denominator)\n",
    "\n",
    "def parse_date(date_val):\n",
    "    if pd.isna(date_val):\n",
    "        return pd.NaT\n",
    "    date_str = str(date_val).strip()\n",
    "    formats = ['%d/%m/%Y', '%m/%d/%Y', '%Y-%m-%d', '%d-%m-%Y', '%Y/%m/%d',\n",
    "               '%d/%m/%y', '%m/%d/%y', '%Y%m%d']\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            return pd.to_datetime(date_str, format=fmt)\n",
    "        except:\n",
    "            continue\n",
    "    try:\n",
    "        return pd.to_datetime(date_str, dayfirst=True)\n",
    "    except:\n",
    "        try:\n",
    "            return pd.to_datetime(date_str)\n",
    "        except:\n",
    "            return pd.NaT\n",
    "\n",
    "def calculate_linear_trend(dates, values):\n",
    "    try:\n",
    "        if len(dates) < 5 or len(values) < 5:\n",
    "            return None\n",
    "        years = (dates - dates.min()).dt.days / 365.25\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(years.values, values.values)\n",
    "        n = len(years)\n",
    "        t_critical = stats.t.ppf((1 + CONFIDENCE_LEVEL) / 2, n - 2)\n",
    "        slope_ci_lower = slope - t_critical * std_err\n",
    "        slope_ci_upper = slope + t_critical * std_err\n",
    "        years_to_prediction = PREDICTION_YEAR - dates.max().year\n",
    "        predicted_change = slope * years_to_prediction\n",
    "        return {\n",
    "            'method': 'Linear_OLS',\n",
    "            'slope_m_per_year': slope,\n",
    "            'intercept': intercept,\n",
    "            'r_squared': r_value**2,\n",
    "            'p_value': p_value,\n",
    "            'std_error': std_err,\n",
    "            'slope_ci_lower': slope_ci_lower,\n",
    "            'slope_ci_upper': slope_ci_upper,\n",
    "            'significant': p_value < SIGNIFICANCE_LEVEL,\n",
    "            'trend_direction': 'Decreasing' if slope > 0 else 'Increasing',\n",
    "            'predicted_change': predicted_change\n",
    "        }\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def mann_kendall_test(values):\n",
    "    try:\n",
    "        if not MK_AVAILABLE:\n",
    "            return None\n",
    "        values_clean = values.dropna()\n",
    "        if len(values_clean) < 10:\n",
    "            return None\n",
    "        result = mk.original_test(values_clean)\n",
    "        return {\n",
    "            'method': 'Mann_Kendall',\n",
    "            'trend': result.trend,\n",
    "            'h': result.h,\n",
    "            'p_value': result.p,\n",
    "            'z_score': result.z,\n",
    "            'tau': result.Tau,\n",
    "            's': result.s,\n",
    "            'significant': result.h,\n",
    "            'trend_direction': 'Decreasing' if result.slope > 0 else 'Increasing' if result.slope < 0 else 'No Trend'\n",
    "        }\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def sens_slope_estimator(dates, values):\n",
    "    try:\n",
    "        if not MK_AVAILABLE:\n",
    "            values_clean = values.dropna()\n",
    "            dates_clean = dates[values.notna()]\n",
    "            if len(values_clean) < 5:\n",
    "                return None\n",
    "            years = (dates_clean - dates_clean.min()).dt.days / 365.25\n",
    "            slopes = []\n",
    "            for i in range(len(years)):\n",
    "                for j in range(i + 1, len(years)):\n",
    "                    if years.iloc[j] != years.iloc[i]:\n",
    "                        slope = (values_clean.iloc[j] - values_clean.iloc[i]) / (years.iloc[j] - years.iloc[i])\n",
    "                        slopes.append(slope)\n",
    "            if len(slopes) == 0:\n",
    "                return None\n",
    "            sen_slope = np.median(slopes)\n",
    "        else:\n",
    "            result = mk.original_test(values.dropna())\n",
    "            sen_slope = result.slope\n",
    "        years_to_prediction = PREDICTION_YEAR - dates.max().year\n",
    "        predicted_change = sen_slope * years_to_prediction\n",
    "        return {\n",
    "            'method': 'Sens_Slope',\n",
    "            'slope_m_per_year': sen_slope,\n",
    "            'trend_direction': 'Decreasing' if sen_slope > 0 else 'Increasing',\n",
    "            'predicted_change': predicted_change\n",
    "        }\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def seasonal_kendall_test(values, period=12):\n",
    "    try:\n",
    "        if not MK_AVAILABLE:\n",
    "            return None\n",
    "        values_clean = values.dropna()\n",
    "        if len(values_clean) < period * 2:\n",
    "            return None\n",
    "        result = mk.seasonal_test(values_clean, period=period)\n",
    "        return {\n",
    "            'method': 'Seasonal_Kendall',\n",
    "            'trend': result.trend,\n",
    "            'h': result.h,\n",
    "            'p_value': result.p,\n",
    "            'z_score': result.z,\n",
    "            'significant': result.h,\n",
    "            'slope': result.slope,\n",
    "            'trend_direction': 'Decreasing' if result.slope > 0 else 'Increasing' if result.slope < 0 else 'No Trend'\n",
    "        }\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def get_last_value(data):\n",
    "    if data is None:\n",
    "        return np.nan\n",
    "    if isinstance(data, pd.Series):\n",
    "        return float(data.iloc[-1]) if len(data) > 0 else np.nan\n",
    "    elif isinstance(data, np.ndarray):\n",
    "        return float(data[-1]) if len(data) > 0 else np.nan\n",
    "    elif isinstance(data, (list, tuple)):\n",
    "        return float(data[-1]) if len(data) > 0 else np.nan\n",
    "    else:\n",
    "        return float(data)\n",
    "\n",
    "def fit_arima_model(series, forecast_periods=60):\n",
    "    try:\n",
    "        series = series.dropna().astype(float)\n",
    "        if len(series) < 24:\n",
    "            return None\n",
    "        try:\n",
    "            adf_result = adfuller(series, autolag='AIC')\n",
    "            d = 0 if adf_result[1] < 0.05 else 1\n",
    "        except:\n",
    "            d = 1\n",
    "        best_aic = np.inf\n",
    "        best_model = None\n",
    "        best_order = None\n",
    "        orders_to_try = [\n",
    "            (2, d, 2), (1, d, 1), (2, d, 1), (1, d, 2),\n",
    "            (0, d, 1), (1, d, 0), (0, d, 2), (2, d, 0),\n",
    "            (3, d, 1), (1, d, 3), (3, d, 2), (2, d, 3)\n",
    "        ]\n",
    "        for order in orders_to_try:\n",
    "            try:\n",
    "                model = ARIMA(series, order=order)\n",
    "                fitted = model.fit()\n",
    "                if fitted.aic < best_aic:\n",
    "                    best_aic = fitted.aic\n",
    "                    best_model = fitted\n",
    "                    best_order = order\n",
    "            except:\n",
    "                continue\n",
    "        if best_model is None:\n",
    "            return None\n",
    "        forecast_result = best_model.get_forecast(steps=forecast_periods)\n",
    "        forecast = forecast_result.predicted_mean\n",
    "        conf_int = forecast_result.conf_int(alpha=1-CONFIDENCE_LEVEL)\n",
    "        in_sample_pred = best_model.fittedvalues\n",
    "        rmse = np.sqrt(mean_squared_error(series[1:], in_sample_pred[1:]))\n",
    "        mae = mean_absolute_error(series[1:], in_sample_pred[1:])\n",
    "        nse = nash_sutcliffe_efficiency(series[1:], in_sample_pred[1:])\n",
    "        forecast_values = forecast.values if hasattr(forecast, 'values') else np.array(forecast)\n",
    "        forecast_lower_values = conf_int.iloc[:, 0].values if hasattr(conf_int.iloc[:, 0], 'values') else np.array(conf_int.iloc[:, 0])\n",
    "        forecast_upper_values = conf_int.iloc[:, 1].values if hasattr(conf_int.iloc[:, 1], 'values') else np.array(conf_int.iloc[:, 1])\n",
    "        return {\n",
    "            'forecast': forecast_values,\n",
    "            'forecast_lower': forecast_lower_values,\n",
    "            'forecast_upper': forecast_upper_values,\n",
    "            'aic': best_aic,\n",
    "            'bic': best_model.bic,\n",
    "            'order': best_order,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'nse': nse\n",
    "        }\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def fit_holtwinters_model(series, forecast_periods=60, seasonal_periods=12):\n",
    "    try:\n",
    "        series = series.dropna().astype(float)\n",
    "        if len(series) < seasonal_periods * 2:\n",
    "            try:\n",
    "                model = ExponentialSmoothing(\n",
    "                    series,\n",
    "                    trend='add',\n",
    "                    seasonal=None\n",
    "                )\n",
    "                fitted = model.fit()\n",
    "            except:\n",
    "                return None\n",
    "        else:\n",
    "            try:\n",
    "                model = ExponentialSmoothing(\n",
    "                    series,\n",
    "                    trend='add',\n",
    "                    seasonal='add',\n",
    "                    seasonal_periods=seasonal_periods\n",
    "                )\n",
    "                fitted = model.fit()\n",
    "            except:\n",
    "                try:\n",
    "                    model = ExponentialSmoothing(\n",
    "                        series,\n",
    "                        trend='add',\n",
    "                        seasonal=None\n",
    "                    )\n",
    "                    fitted = model.fit()\n",
    "                except:\n",
    "                    return None\n",
    "        forecast = fitted.forecast(steps=forecast_periods)\n",
    "        residual_std = fitted.resid.std()\n",
    "        forecast_lower = forecast - 1.96 * residual_std\n",
    "        forecast_upper = forecast + 1.96 * residual_std\n",
    "        in_sample_pred = fitted.fittedvalues\n",
    "        rmse = np.sqrt(mean_squared_error(series, in_sample_pred))\n",
    "        mae = mean_absolute_error(series, in_sample_pred)\n",
    "        nse = nash_sutcliffe_efficiency(series, in_sample_pred)\n",
    "        forecast_values = forecast.values if hasattr(forecast, 'values') else np.array(forecast)\n",
    "        forecast_lower_values = forecast_lower.values if hasattr(forecast_lower, 'values') else np.array(forecast_lower)\n",
    "        forecast_upper_values = forecast_upper.values if hasattr(forecast_upper, 'values') else np.array(forecast_upper)\n",
    "        return {\n",
    "            'forecast': forecast_values,\n",
    "            'forecast_lower': forecast_lower_values,\n",
    "            'forecast_upper': forecast_upper_values,\n",
    "            'aic': fitted.aic if hasattr(fitted, 'aic') else np.nan,\n",
    "            'residual_std': residual_std,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'nse': nse\n",
    "        }\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def fit_prophet_model(df_series, forecast_periods=60):\n",
    "    try:\n",
    "        if not PROPHET_AVAILABLE:\n",
    "            return None\n",
    "        df_prophet = df_series.reset_index()\n",
    "        df_prophet.columns = ['ds', 'y']\n",
    "        df_prophet['ds'] = pd.to_datetime(df_prophet['ds'])\n",
    "        df_prophet = df_prophet.dropna()\n",
    "        if len(df_prophet) < 24:\n",
    "            return None\n",
    "        \n",
    "        logging.getLogger('cmdstanpy').disabled = True\n",
    "        logging.getLogger('prophet').disabled = True\n",
    "        logging.getLogger('stan').disabled = True\n",
    "        \n",
    "        model = Prophet(\n",
    "            yearly_seasonality=True,\n",
    "            weekly_seasonality=False,\n",
    "            daily_seasonality=False,\n",
    "            seasonality_mode='multiplicative',\n",
    "            interval_width=CONFIDENCE_LEVEL\n",
    "        )\n",
    "        model.fit(df_prophet)\n",
    "        last_date = df_prophet['ds'].max()\n",
    "        future_dates = pd.date_range(start=last_date, periods=forecast_periods+1, freq='MS')[1:]\n",
    "        future = pd.DataFrame({'ds': future_dates})\n",
    "        forecast = model.predict(future)\n",
    "        in_sample = model.predict(df_prophet[['ds']])\n",
    "        rmse = np.sqrt(mean_squared_error(df_prophet['y'], in_sample['yhat']))\n",
    "        mae = mean_absolute_error(df_prophet['y'], in_sample['yhat'])\n",
    "        nse = nash_sutcliffe_efficiency(df_prophet['y'], in_sample['yhat'])\n",
    "        return {\n",
    "            'forecast': forecast['yhat'].values,\n",
    "            'forecast_lower': forecast['yhat_lower'].values,\n",
    "            'forecast_upper': forecast['yhat_upper'].values,\n",
    "            'trend': forecast['trend'].values,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'nse': nse\n",
    "        }\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def fit_lstm_model(series, forecast_periods=60, lookback=12):\n",
    "    try:\n",
    "        series = series.dropna().astype(float)\n",
    "        if len(series) < lookback + 36:\n",
    "            return None\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled_data = scaler.fit_transform(series.values.reshape(-1, 1))\n",
    "        X, y = [], []\n",
    "        for i in range(lookback, len(scaled_data)):\n",
    "            X.append(scaled_data[i-lookback:i, 0])\n",
    "            y.append(scaled_data[i, 0])\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "        if len(X) < 20:\n",
    "            return None\n",
    "        train_size = int(len(X) * 0.8)\n",
    "        X_train, X_test = X[:train_size], X[train_size:]\n",
    "        y_train, y_test = y[:train_size], y[train_size:]\n",
    "        model = Sequential([\n",
    "            LSTM(64, return_sequences=True, input_shape=(lookback, 1)),\n",
    "            Dropout(0.2),\n",
    "            LSTM(32, return_sequences=False),\n",
    "            Dropout(0.2),\n",
    "            Dense(16, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='mean_squared_error'\n",
    "        )\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            batch_size=LSTM_BATCH_SIZE,\n",
    "            epochs=LSTM_EPOCHS,\n",
    "            validation_split=0.15,\n",
    "            callbacks=[early_stop, reduce_lr],\n",
    "            verbose=0\n",
    "        )\n",
    "        if len(X_test) > 0:\n",
    "            predictions_test = model.predict(X_test, verbose=0)\n",
    "            predictions_test = scaler.inverse_transform(predictions_test)\n",
    "            y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "            rmse = np.sqrt(mean_squared_error(y_test_inv, predictions_test))\n",
    "            mae = mean_absolute_error(y_test_inv, predictions_test)\n",
    "            nse = nash_sutcliffe_efficiency(y_test_inv, predictions_test)\n",
    "        else:\n",
    "            rmse = np.nan\n",
    "            mae = np.nan\n",
    "            nse = np.nan\n",
    "        last_sequence = scaled_data[-lookback:]\n",
    "        forecasts = []\n",
    "        current_seq = last_sequence.flatten()\n",
    "        for _ in range(forecast_periods):\n",
    "            input_seq = current_seq[-lookback:].reshape(1, lookback, 1)\n",
    "            pred = model.predict(input_seq, verbose=0)[0, 0]\n",
    "            forecasts.append(pred)\n",
    "            current_seq = np.append(current_seq, pred)\n",
    "        forecasts = scaler.inverse_transform(np.array(forecasts).reshape(-1, 1)).flatten()\n",
    "        forecast_lower = forecasts - 1.96 * rmse if not np.isnan(rmse) else forecasts\n",
    "        forecast_upper = forecasts + 1.96 * rmse if not np.isnan(rmse) else forecasts\n",
    "        K.clear_session()\n",
    "        del model\n",
    "        return {\n",
    "            'forecast': forecasts,\n",
    "            'forecast_lower': forecast_lower,\n",
    "            'forecast_upper': forecast_upper,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'nse': nse\n",
    "        }\n",
    "    except:\n",
    "        K.clear_session()\n",
    "        return None\n",
    "\n",
    "def fit_bilstm_model(series, forecast_periods=60, lookback=12):\n",
    "    try:\n",
    "        series = series.dropna().astype(float)\n",
    "        if len(series) < lookback + 48:\n",
    "            return None\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled_data = scaler.fit_transform(series.values.reshape(-1, 1))\n",
    "        X, y = [], []\n",
    "        for i in range(lookback, len(scaled_data)):\n",
    "            X.append(scaled_data[i-lookback:i, 0])\n",
    "            y.append(scaled_data[i, 0])\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "        if len(X) < 30:\n",
    "            return None\n",
    "        train_size = int(len(X) * 0.8)\n",
    "        X_train, X_test = X[:train_size], X[train_size:]\n",
    "        y_train, y_test = y[:train_size], y[train_size:]\n",
    "        model = Sequential([\n",
    "            Bidirectional(LSTM(64, return_sequences=True), input_shape=(lookback, 1)),\n",
    "            Dropout(0.3),\n",
    "            Bidirectional(LSTM(32)),\n",
    "            Dropout(0.3),\n",
    "            Dense(16, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            batch_size=LSTM_BATCH_SIZE,\n",
    "            epochs=LSTM_EPOCHS,\n",
    "            validation_split=0.15,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=0\n",
    "        )\n",
    "        if len(X_test) > 0:\n",
    "            pred_test = model.predict(X_test, verbose=0)\n",
    "            pred_test = scaler.inverse_transform(pred_test)\n",
    "            y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "            rmse = np.sqrt(mean_squared_error(y_test_inv, pred_test))\n",
    "            mae = mean_absolute_error(y_test_inv, pred_test)\n",
    "            nse = nash_sutcliffe_efficiency(y_test_inv, pred_test)\n",
    "        else:\n",
    "            rmse = np.nan\n",
    "            mae = np.nan\n",
    "            nse = np.nan\n",
    "        last_seq = scaled_data[-lookback:]\n",
    "        forecasts = []\n",
    "        current_seq = last_seq.flatten()\n",
    "        for _ in range(forecast_periods):\n",
    "            input_seq = current_seq[-lookback:].reshape(1, lookback, 1)\n",
    "            pred = model.predict(input_seq, verbose=0)[0, 0]\n",
    "            forecasts.append(pred)\n",
    "            current_seq = np.append(current_seq, pred)\n",
    "        forecasts = scaler.inverse_transform(np.array(forecasts).reshape(-1, 1)).flatten()\n",
    "        forecast_lower = forecasts - 1.96 * rmse if not np.isnan(rmse) else forecasts\n",
    "        forecast_upper = forecasts + 1.96 * rmse if not np.isnan(rmse) else forecasts\n",
    "        K.clear_session()\n",
    "        del model\n",
    "        return {\n",
    "            'forecast': forecasts,\n",
    "            'forecast_lower': forecast_lower,\n",
    "            'forecast_upper': forecast_upper,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'nse': nse\n",
    "        }\n",
    "    except:\n",
    "        K.clear_session()\n",
    "        return None\n",
    "\n",
    "def calculate_ensemble_prediction(predictions_dict):\n",
    "    try:\n",
    "        valid_predictions = {}\n",
    "        valid_uncertainties = {}\n",
    "        for model_name, pred_data in predictions_dict.items():\n",
    "            if pred_data is not None and 'forecast' in pred_data:\n",
    "                forecast = pred_data['forecast']\n",
    "                final_value = get_last_value(forecast)\n",
    "                if not np.isnan(final_value):\n",
    "                    valid_predictions[model_name] = final_value\n",
    "                    if 'forecast_lower' in pred_data and 'forecast_upper' in pred_data:\n",
    "                        lower = get_last_value(pred_data['forecast_lower'])\n",
    "                        upper = get_last_value(pred_data['forecast_upper'])\n",
    "                        if not np.isnan(lower) and not np.isnan(upper):\n",
    "                            valid_uncertainties[model_name] = (upper - lower) / 2\n",
    "        if len(valid_predictions) == 0:\n",
    "            return None\n",
    "        values = list(valid_predictions.values())\n",
    "        ensemble_mean = np.mean(values)\n",
    "        ensemble_std = np.std(values) if len(values) > 1 else 0\n",
    "        if valid_uncertainties:\n",
    "            combined_uncertainty = np.sqrt(np.sum([u**2 for u in valid_uncertainties.values()])) / len(valid_uncertainties)\n",
    "            total_uncertainty = np.sqrt(ensemble_std**2 + combined_uncertainty**2)\n",
    "        else:\n",
    "            total_uncertainty = ensemble_std\n",
    "        return {\n",
    "            'ensemble_mean': ensemble_mean,\n",
    "            'ensemble_std': ensemble_std,\n",
    "            'ensemble_lower': ensemble_mean - 1.96 * total_uncertainty,\n",
    "            'ensemble_upper': ensemble_mean + 1.96 * total_uncertainty,\n",
    "            'n_models': len(valid_predictions),\n",
    "            'model_predictions': valid_predictions\n",
    "        }\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def process_single_well(well_dict, df_piez_subset, piez_codigo_col_local):\n",
    "    try:\n",
    "        station_code = well_dict['Station_Code']\n",
    "        \n",
    "        well_data = df_piez_subset[df_piez_subset[piez_codigo_col_local] == station_code].copy()\n",
    "        well_data = well_data.sort_values('Date')\n",
    "        \n",
    "        if len(well_data) < 10:\n",
    "            return None\n",
    "        \n",
    "        monthly_data = well_data.groupby('YearMonth').agg({\n",
    "            'WaterLevel_m': 'mean',\n",
    "            'Date': 'first'\n",
    "        }).reset_index()\n",
    "        monthly_data['Date'] = monthly_data['YearMonth'].dt.to_timestamp()\n",
    "        monthly_data = monthly_data.sort_values('Date')\n",
    "        monthly_data = monthly_data.set_index('Date')\n",
    "        \n",
    "        result = {\n",
    "            'Station_Code': station_code,\n",
    "            'Station_Name': well_dict['Station_Name'],\n",
    "            'Latitude': well_dict['Latitude'],\n",
    "            'Longitude': well_dict['Longitude'],\n",
    "            'N_Records': int(well_dict['N_Records']),\n",
    "            'N_Monthly_Records': len(monthly_data),\n",
    "            'Year_Start': int(well_dict['Year_Start']),\n",
    "            'Year_End': int(well_dict['Year_End']),\n",
    "            'Years_Span': int(well_dict['Years_Span']),\n",
    "            'Records_Per_Year': float(well_dict['Records_Per_Year']),\n",
    "            'WL_Mean': float(well_dict['WL_Mean']),\n",
    "            'WL_Std': float(well_dict['WL_Std']) if pd.notna(well_dict['WL_Std']) else np.nan,\n",
    "            'WL_Min': float(well_dict['WL_Min']),\n",
    "            'WL_Max': float(well_dict['WL_Max']),\n",
    "            'WL_Current': float(monthly_data['WaterLevel_m'].iloc[-1]) if len(monthly_data) > 0 else np.nan,\n",
    "            'WL_First': float(monthly_data['WaterLevel_m'].iloc[0]) if len(monthly_data) > 0 else np.nan,\n",
    "            'Total_Change_m': float(monthly_data['WaterLevel_m'].iloc[-1] - monthly_data['WaterLevel_m'].iloc[0]) if len(monthly_data) > 1 else np.nan\n",
    "        }\n",
    "        \n",
    "        linear_result = calculate_linear_trend(well_data['Date'], well_data['WaterLevel_m'])\n",
    "        if linear_result:\n",
    "            result.update({\n",
    "                'Linear_Slope_m_yr': linear_result['slope_m_per_year'],\n",
    "                'Linear_R2': linear_result['r_squared'],\n",
    "                'Linear_PValue': linear_result['p_value'],\n",
    "                'Linear_StdErr': linear_result['std_error'],\n",
    "                'Linear_CI_Lower': linear_result['slope_ci_lower'],\n",
    "                'Linear_CI_Upper': linear_result['slope_ci_upper'],\n",
    "                'Linear_Significant': linear_result['significant'],\n",
    "                'Linear_Trend': linear_result['trend_direction']\n",
    "            })\n",
    "        else:\n",
    "            result.update({\n",
    "                'Linear_Slope_m_yr': np.nan, 'Linear_R2': np.nan, 'Linear_PValue': np.nan,\n",
    "                'Linear_StdErr': np.nan, 'Linear_CI_Lower': np.nan, 'Linear_CI_Upper': np.nan,\n",
    "                'Linear_Significant': False, 'Linear_Trend': 'Unknown'\n",
    "            })\n",
    "        \n",
    "        mk_result = mann_kendall_test(monthly_data['WaterLevel_m'])\n",
    "        if mk_result:\n",
    "            result.update({\n",
    "                'MK_Trend': mk_result['trend'],\n",
    "                'MK_PValue': mk_result['p_value'],\n",
    "                'MK_ZScore': mk_result['z_score'],\n",
    "                'MK_Tau': mk_result['tau'],\n",
    "                'MK_Significant': mk_result['significant'],\n",
    "                'MK_Direction': mk_result['trend_direction']\n",
    "            })\n",
    "        else:\n",
    "            result.update({\n",
    "                'MK_Trend': 'N/A', 'MK_PValue': np.nan, 'MK_ZScore': np.nan,\n",
    "                'MK_Tau': np.nan, 'MK_Significant': False, 'MK_Direction': 'Unknown'\n",
    "            })\n",
    "        \n",
    "        sens_result = sens_slope_estimator(well_data['Date'], well_data['WaterLevel_m'])\n",
    "        if sens_result:\n",
    "            result.update({\n",
    "                'Sens_Slope_m_yr': sens_result['slope_m_per_year'],\n",
    "                'Sens_Trend': sens_result['trend_direction'],\n",
    "                'Sens_Predicted_Change': sens_result['predicted_change']\n",
    "            })\n",
    "        else:\n",
    "            result.update({\n",
    "                'Sens_Slope_m_yr': np.nan, 'Sens_Trend': 'Unknown', 'Sens_Predicted_Change': np.nan\n",
    "            })\n",
    "        \n",
    "        if len(monthly_data) >= 24:\n",
    "            sk_result = seasonal_kendall_test(monthly_data['WaterLevel_m'])\n",
    "            if sk_result:\n",
    "                result.update({\n",
    "                    'SK_Trend': sk_result['trend'],\n",
    "                    'SK_PValue': sk_result['p_value'],\n",
    "                    'SK_Significant': sk_result['significant']\n",
    "                })\n",
    "            else:\n",
    "                result.update({'SK_Trend': 'N/A', 'SK_PValue': np.nan, 'SK_Significant': False})\n",
    "        else:\n",
    "            result.update({'SK_Trend': 'N/A', 'SK_PValue': np.nan, 'SK_Significant': False})\n",
    "        \n",
    "        trends = []\n",
    "        if result.get('Linear_Trend') in ['Decreasing', 'Increasing']:\n",
    "            trends.append(result['Linear_Trend'])\n",
    "        if result.get('MK_Direction') in ['Decreasing', 'Increasing']:\n",
    "            trends.append(result['MK_Direction'])\n",
    "        if result.get('Sens_Trend') in ['Decreasing', 'Increasing']:\n",
    "            trends.append(result['Sens_Trend'])\n",
    "        \n",
    "        if trends:\n",
    "            result['Consensus_Trend'] = max(set(trends), key=trends.count)\n",
    "        else:\n",
    "            result['Consensus_Trend'] = 'Unknown'\n",
    "        \n",
    "        last_date = monthly_data.index.max()\n",
    "        target_date = pd.Timestamp(f'{PREDICTION_YEAR}-12-31')\n",
    "        months_to_forecast = max(1, (target_date.year - last_date.year) * 12 + (target_date.month - last_date.month))\n",
    "        \n",
    "        predictions = {}\n",
    "        \n",
    "        arima_result = fit_arima_model(monthly_data['WaterLevel_m'], months_to_forecast)\n",
    "        if arima_result:\n",
    "            predictions['ARIMA'] = arima_result\n",
    "            result.update({\n",
    "                'ARIMA_Pred_2030': float(arima_result['forecast'][-1]),\n",
    "                'ARIMA_Lower_2030': float(arima_result['forecast_lower'][-1]),\n",
    "                'ARIMA_Upper_2030': float(arima_result['forecast_upper'][-1]),\n",
    "                'ARIMA_Order': str(arima_result['order']),\n",
    "                'ARIMA_AIC': float(arima_result['aic']),\n",
    "                'ARIMA_RMSE': float(arima_result['rmse']),\n",
    "                'ARIMA_MAE': float(arima_result['mae']),\n",
    "                'ARIMA_NSE': float(arima_result['nse'])\n",
    "            })\n",
    "        else:\n",
    "            result.update({\n",
    "                'ARIMA_Pred_2030': np.nan, 'ARIMA_Lower_2030': np.nan, 'ARIMA_Upper_2030': np.nan,\n",
    "                'ARIMA_Order': 'Failed', 'ARIMA_AIC': np.nan, 'ARIMA_RMSE': np.nan,\n",
    "                'ARIMA_MAE': np.nan, 'ARIMA_NSE': np.nan\n",
    "            })\n",
    "        \n",
    "        hw_result = fit_holtwinters_model(monthly_data['WaterLevel_m'], months_to_forecast)\n",
    "        if hw_result:\n",
    "            predictions['HoltWinters'] = hw_result\n",
    "            result.update({\n",
    "                'HW_Pred_2030': float(hw_result['forecast'][-1]),\n",
    "                'HW_Lower_2030': float(hw_result['forecast_lower'][-1]),\n",
    "                'HW_Upper_2030': float(hw_result['forecast_upper'][-1]),\n",
    "                'HW_RMSE': float(hw_result['rmse']),\n",
    "                'HW_MAE': float(hw_result['mae']),\n",
    "                'HW_NSE': float(hw_result['nse'])\n",
    "            })\n",
    "        else:\n",
    "            result.update({\n",
    "                'HW_Pred_2030': np.nan, 'HW_Lower_2030': np.nan, 'HW_Upper_2030': np.nan,\n",
    "                'HW_RMSE': np.nan, 'HW_MAE': np.nan, 'HW_NSE': np.nan\n",
    "            })\n",
    "        \n",
    "        if PROPHET_AVAILABLE and len(monthly_data) >= 24:\n",
    "            prophet_result = fit_prophet_model(monthly_data['WaterLevel_m'], months_to_forecast)\n",
    "            if prophet_result:\n",
    "                predictions['Prophet'] = prophet_result\n",
    "                result.update({\n",
    "                    'Prophet_Pred_2030': float(prophet_result['forecast'][-1]),\n",
    "                    'Prophet_Lower_2030': float(prophet_result['forecast_lower'][-1]),\n",
    "                    'Prophet_Upper_2030': float(prophet_result['forecast_upper'][-1]),\n",
    "                    'Prophet_RMSE': float(prophet_result['rmse']),\n",
    "                    'Prophet_MAE': float(prophet_result['mae']),\n",
    "                    'Prophet_NSE': float(prophet_result['nse'])\n",
    "                })\n",
    "            else:\n",
    "                result.update({\n",
    "                    'Prophet_Pred_2030': np.nan, 'Prophet_Lower_2030': np.nan, 'Prophet_Upper_2030': np.nan,\n",
    "                    'Prophet_RMSE': np.nan, 'Prophet_MAE': np.nan, 'Prophet_NSE': np.nan\n",
    "                })\n",
    "        else:\n",
    "            result.update({\n",
    "                'Prophet_Pred_2030': np.nan, 'Prophet_Lower_2030': np.nan, 'Prophet_Upper_2030': np.nan,\n",
    "                'Prophet_RMSE': np.nan, 'Prophet_MAE': np.nan, 'Prophet_NSE': np.nan\n",
    "            })\n",
    "        \n",
    "        if len(monthly_data) >= 48:\n",
    "            lstm_result = fit_lstm_model(monthly_data['WaterLevel_m'], months_to_forecast, LSTM_LOOKBACK)\n",
    "            if lstm_result:\n",
    "                predictions['LSTM'] = lstm_result\n",
    "                result.update({\n",
    "                    'LSTM_Pred_2030': float(lstm_result['forecast'][-1]),\n",
    "                    'LSTM_Lower_2030': float(lstm_result['forecast_lower'][-1]),\n",
    "                    'LSTM_Upper_2030': float(lstm_result['forecast_upper'][-1]),\n",
    "                    'LSTM_RMSE': float(lstm_result['rmse']) if not np.isnan(lstm_result['rmse']) else np.nan,\n",
    "                    'LSTM_MAE': float(lstm_result['mae']) if not np.isnan(lstm_result['mae']) else np.nan,\n",
    "                    'LSTM_NSE': float(lstm_result['nse']) if not np.isnan(lstm_result['nse']) else np.nan\n",
    "                })\n",
    "            else:\n",
    "                result.update({\n",
    "                    'LSTM_Pred_2030': np.nan, 'LSTM_Lower_2030': np.nan, 'LSTM_Upper_2030': np.nan,\n",
    "                    'LSTM_RMSE': np.nan, 'LSTM_MAE': np.nan, 'LSTM_NSE': np.nan\n",
    "                })\n",
    "        else:\n",
    "            result.update({\n",
    "                'LSTM_Pred_2030': np.nan, 'LSTM_Lower_2030': np.nan, 'LSTM_Upper_2030': np.nan,\n",
    "                'LSTM_RMSE': np.nan, 'LSTM_MAE': np.nan, 'LSTM_NSE': np.nan\n",
    "            })\n",
    "        \n",
    "        if len(monthly_data) >= 60:\n",
    "            bilstm_result = fit_bilstm_model(monthly_data['WaterLevel_m'], months_to_forecast, LSTM_LOOKBACK)\n",
    "            if bilstm_result:\n",
    "                predictions['BiLSTM'] = bilstm_result\n",
    "                result.update({\n",
    "                    'BiLSTM_Pred_2030': float(bilstm_result['forecast'][-1]),\n",
    "                    'BiLSTM_Lower_2030': float(bilstm_result['forecast_lower'][-1]),\n",
    "                    'BiLSTM_Upper_2030': float(bilstm_result['forecast_upper'][-1]),\n",
    "                    'BiLSTM_RMSE': float(bilstm_result['rmse']) if not np.isnan(bilstm_result['rmse']) else np.nan,\n",
    "                    'BiLSTM_MAE': float(bilstm_result['mae']) if not np.isnan(bilstm_result['mae']) else np.nan,\n",
    "                    'BiLSTM_NSE': float(bilstm_result['nse']) if not np.isnan(bilstm_result['nse']) else np.nan\n",
    "                })\n",
    "            else:\n",
    "                result.update({\n",
    "                    'BiLSTM_Pred_2030': np.nan, 'BiLSTM_Lower_2030': np.nan, 'BiLSTM_Upper_2030': np.nan,\n",
    "                    'BiLSTM_RMSE': np.nan, 'BiLSTM_MAE': np.nan, 'BiLSTM_NSE': np.nan\n",
    "                })\n",
    "        else:\n",
    "            result.update({\n",
    "                'BiLSTM_Pred_2030': np.nan, 'BiLSTM_Lower_2030': np.nan, 'BiLSTM_Upper_2030': np.nan,\n",
    "                'BiLSTM_RMSE': np.nan, 'BiLSTM_MAE': np.nan, 'BiLSTM_NSE': np.nan\n",
    "            })\n",
    "        \n",
    "        ensemble = calculate_ensemble_prediction(predictions)\n",
    "        if ensemble:\n",
    "            result.update({\n",
    "                'Ensemble_Pred_2030': float(ensemble['ensemble_mean']),\n",
    "                'Ensemble_Lower_2030': float(ensemble['ensemble_lower']),\n",
    "                'Ensemble_Upper_2030': float(ensemble['ensemble_upper']),\n",
    "                'Ensemble_Std': float(ensemble['ensemble_std']),\n",
    "                'Ensemble_N_Models': int(ensemble['n_models'])\n",
    "            })\n",
    "        else:\n",
    "            result.update({\n",
    "                'Ensemble_Pred_2030': np.nan, 'Ensemble_Lower_2030': np.nan,\n",
    "                'Ensemble_Upper_2030': np.nan, 'Ensemble_Std': np.nan, 'Ensemble_N_Models': 0\n",
    "            })\n",
    "        \n",
    "        current_level = result['WL_Current']\n",
    "        for model in ['ARIMA', 'HW', 'Prophet', 'LSTM', 'BiLSTM', 'Ensemble']:\n",
    "            pred_col = f'{model}_Pred_2030'\n",
    "            change_col = f'{model}_Change_2030'\n",
    "            if pred_col in result and not np.isnan(result.get(pred_col, np.nan)):\n",
    "                result[change_col] = float(result[pred_col] - current_level)\n",
    "            else:\n",
    "                result[change_col] = np.nan\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing well: {e}\")\n",
    "        return None\n",
    "\n",
    "write_output(\"=\"*80)\n",
    "write_output(\"GROUNDWATER LEVEL TREND ANALYSIS AND PREDICTION\")\n",
    "write_output(\"=\"*80)\n",
    "write_output(f\"Analysis started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "processed_wells, well_results = load_progress()\n",
    "if len(processed_wells) > 0:\n",
    "    write_output(f\"\\nRESUMING FROM PREVIOUS RUN: {len(processed_wells)} wells already processed\")\n",
    "\n",
    "write_output(\"\\n\" + \"=\"*80)\n",
    "write_output(\"SECTION 1: DATA LOADING AND PREPROCESSING\")\n",
    "write_output(\"=\"*80)\n",
    "\n",
    "write_output(\"\\nLoading piezometric data...\")\n",
    "try:\n",
    "    gdf_piezometric = gpd.read_file(path_piezometric)\n",
    "    write_output(f\"   Loaded {len(gdf_piezometric):,} piezometric records\")\n",
    "    write_output(f\"   Detected CRS: {gdf_piezometric.crs}\")\n",
    "except Exception as e:\n",
    "    write_output(f\"   Error loading piezometric data: {e}\")\n",
    "    raise\n",
    "\n",
    "write_output(\"\\n\" + \"-\"*60)\n",
    "write_output(\"DATA PREPROCESSING\")\n",
    "write_output(\"-\"*60)\n",
    "\n",
    "df_piez = gdf_piezometric.copy()\n",
    "df_piez['Date'] = df_piez[piez_fecha_col].apply(parse_date)\n",
    "df_piez['WaterLevel_m'] = pd.to_numeric(df_piez[piez_nivel_col], errors='coerce')\n",
    "\n",
    "if piez_lat_col in df_piez.columns and piez_lon_col in df_piez.columns:\n",
    "    df_piez['Latitude'] = pd.to_numeric(df_piez[piez_lat_col], errors='coerce')\n",
    "    df_piez['Longitude'] = pd.to_numeric(df_piez[piez_lon_col], errors='coerce')\n",
    "else:\n",
    "    df_piez['Longitude'] = df_piez.geometry.x\n",
    "    df_piez['Latitude'] = df_piez.geometry.y\n",
    "\n",
    "if piez_nombre_col in df_piez.columns:\n",
    "    df_piez['Station_Name'] = df_piez[piez_nombre_col]\n",
    "else:\n",
    "    df_piez['Station_Name'] = df_piez[piez_codigo_col]\n",
    "\n",
    "write_output(\"Cleaning data...\")\n",
    "initial_count = len(df_piez)\n",
    "\n",
    "df_piez_clean = df_piez.dropna(subset=['Date', 'WaterLevel_m']).copy()\n",
    "write_output(f\"After removing null dates/levels: {len(df_piez_clean):,}\")\n",
    "\n",
    "df_piez_clean = df_piez_clean[\n",
    "    (df_piez_clean['Latitude'] >= -56) & (df_piez_clean['Latitude'] <= -17) &\n",
    "    (df_piez_clean['Longitude'] >= -76) & (df_piez_clean['Longitude'] <= -66)\n",
    "]\n",
    "write_output(f\"After geographic filtering: {len(df_piez_clean):,}\")\n",
    "\n",
    "df_piez_clean = df_piez_clean[\n",
    "    (df_piez_clean['WaterLevel_m'] > 0) &\n",
    "    (df_piez_clean['WaterLevel_m'] < 500)\n",
    "]\n",
    "write_output(f\"After water level filtering: {len(df_piez_clean):,}\")\n",
    "\n",
    "df_piez_clean['Year'] = df_piez_clean['Date'].dt.year\n",
    "df_piez_clean['Month'] = df_piez_clean['Date'].dt.month\n",
    "df_piez_clean['YearMonth'] = df_piez_clean['Date'].dt.to_period('M')\n",
    "df_piez_clean['DayOfYear'] = df_piez_clean['Date'].dt.dayofyear\n",
    "\n",
    "write_output(f\"\\nTotal cleaned records: {len(df_piez_clean):,} ({len(df_piez_clean)/initial_count*100:.1f}% retained)\")\n",
    "write_output(f\"Date range: {df_piez_clean['Date'].min().strftime('%Y-%m-%d')} to {df_piez_clean['Date'].max().strftime('%Y-%m-%d')}\")\n",
    "\n",
    "write_output(\"\\n\" + \"-\"*60)\n",
    "write_output(\"ID NORMALIZATION\")\n",
    "write_output(\"-\"*60)\n",
    "\n",
    "id_lengths = df_piez_clean[piez_codigo_col].astype(str).str.strip().str.len().value_counts()\n",
    "target_length = id_lengths.idxmax()\n",
    "write_output(f\"ID length distribution: {id_lengths.to_dict()}\")\n",
    "write_output(f\"Using target length: {target_length}\")\n",
    "\n",
    "df_piez_clean['ID_Normalized'] = df_piez_clean[piez_codigo_col].apply(\n",
    "    lambda x: normalize_id_with_padding(x, target_length)\n",
    ")\n",
    "\n",
    "unique_before = df_piez_clean[piez_codigo_col].nunique()\n",
    "unique_after = df_piez_clean['ID_Normalized'].nunique()\n",
    "write_output(f\"Unique IDs before normalization: {unique_before:,}\")\n",
    "write_output(f\"Unique IDs after normalization: {unique_after:,}\")\n",
    "\n",
    "piez_codigo_col = 'ID_Normalized'\n",
    "\n",
    "write_output(\"ID normalization complete\")\n",
    "\n",
    "write_output(\"\\n\" + \"-\"*60)\n",
    "write_output(\"FILTERING WELLS WITH RECENT DATA\")\n",
    "write_output(\"-\"*60)\n",
    "\n",
    "write_output(\"Calculating well statistics...\")\n",
    "\n",
    "well_stats = df_piez_clean.groupby(piez_codigo_col).agg({\n",
    "    'Date': ['min', 'max', 'count'],\n",
    "    'Year': ['min', 'max', 'nunique'],\n",
    "    'Latitude': 'first',\n",
    "    'Longitude': 'first',\n",
    "    'Station_Name': 'first',\n",
    "    'WaterLevel_m': ['mean', 'std', 'min', 'max', 'median']\n",
    "}).reset_index()\n",
    "\n",
    "well_stats.columns = [\n",
    "    'Station_Code', 'Date_Start', 'Date_End', 'N_Records',\n",
    "    'Year_Start', 'Year_End', 'N_Years_With_Data',\n",
    "    'Latitude', 'Longitude', 'Station_Name',\n",
    "    'WL_Mean', 'WL_Std', 'WL_Min', 'WL_Max', 'WL_Median'\n",
    "]\n",
    "\n",
    "well_stats['Years_Span'] = well_stats['Year_End'] - well_stats['Year_Start']\n",
    "well_stats['Records_Per_Year'] = well_stats['N_Records'] / (well_stats['Years_Span'] + 1)\n",
    "well_stats['Data_Completeness'] = well_stats['N_Years_With_Data'] / (well_stats['Years_Span'] + 1)\n",
    "\n",
    "write_output(f\"\\nTotal unique wells in database: {len(well_stats):,}\")\n",
    "\n",
    "write_output(f\"\\nDistribution of wells by last year of data:\")\n",
    "year_dist = well_stats.groupby('Year_End').size()\n",
    "for year in sorted(year_dist.index)[-10:]:\n",
    "    write_output(f\"   {year}: {year_dist[year]:,} wells\")\n",
    "\n",
    "write_output(f\"\\nApplying filters:\")\n",
    "write_output(f\"   - Data extending to >= {MIN_RECENT_YEAR}\")\n",
    "write_output(f\"   - Minimum {MIN_RECORDS} records\")\n",
    "write_output(f\"   - Minimum {MIN_YEARS_SPAN} years span\")\n",
    "\n",
    "wells_filtered = well_stats[\n",
    "    (well_stats['Year_End'] >= MIN_RECENT_YEAR) &\n",
    "    (well_stats['N_Records'] >= MIN_RECORDS) &\n",
    "    (well_stats['Years_Span'] >= MIN_YEARS_SPAN)\n",
    "].copy()\n",
    "\n",
    "write_output(f\"\\nWells meeting all criteria: {len(wells_filtered):,}\")\n",
    "\n",
    "if len(wells_filtered) < 10:\n",
    "    write_output(\"\\nWARNING: Too few wells meet criteria. Relaxing constraints...\")\n",
    "    relaxation_levels = [\n",
    "        (2018, 20, 4),\n",
    "        (2016, 15, 3),\n",
    "        (2015, 12, 3),\n",
    "        (2010, 10, 2)\n",
    "    ]\n",
    "    for min_year, min_rec, min_span in relaxation_levels:\n",
    "        wells_filtered = well_stats[\n",
    "            (well_stats['Year_End'] >= min_year) &\n",
    "            (well_stats['N_Records'] >= min_rec) &\n",
    "            (well_stats['Years_Span'] >= min_span)\n",
    "        ].copy()\n",
    "        if len(wells_filtered) >= 10:\n",
    "            write_output(f\"Using relaxed criteria: year>={min_year}, records>={min_rec}, span>={min_span}\")\n",
    "            write_output(f\"Wells meeting relaxed criteria: {len(wells_filtered):,}\")\n",
    "            MIN_RECENT_YEAR = min_year\n",
    "            MIN_RECORDS = min_rec\n",
    "            MIN_YEARS_SPAN = min_span\n",
    "            break\n",
    "\n",
    "if len(wells_filtered) == 0:\n",
    "    write_output(\"ERROR: No wells meet any criteria. Please check data quality.\")\n",
    "    wells_filtered = well_stats[well_stats['N_Records'] >= 5].copy()\n",
    "    write_output(f\"Using all wells with >= 5 records: {len(wells_filtered):,}\")\n",
    "\n",
    "write_output(\"\\n\" + \"=\"*80)\n",
    "write_output(\"SECTION 2: INDIVIDUAL WELL ANALYSIS\")\n",
    "write_output(\"=\"*80)\n",
    "\n",
    "total_wells = len(wells_filtered)\n",
    "successful_analyses = len([r for r in well_results if 'Station_Code' in r])\n",
    "failed_analyses = 0\n",
    "skipped_wells = 0\n",
    "\n",
    "write_output(f\"\\nAnalyzing {total_wells} wells with multiple models...\")\n",
    "write_output(f\"Previously processed: {len(processed_wells)} wells\")\n",
    "\n",
    "wells_to_process = []\n",
    "for _, well in wells_filtered.iterrows():\n",
    "    station_code = well['Station_Code']\n",
    "    if station_code not in processed_wells:\n",
    "        wells_to_process.append(well.to_dict())\n",
    "    else:\n",
    "        skipped_wells += 1\n",
    "\n",
    "write_output(f\"Wells to process: {len(wells_to_process)}\")\n",
    "write_output(f\"Wells skipped (already processed): {skipped_wells}\")\n",
    "\n",
    "if len(wells_to_process) > 0:\n",
    "    df_piez_subset = df_piez_clean[[piez_codigo_col, 'Date', 'WaterLevel_m', 'YearMonth']].copy()\n",
    "    \n",
    "    write_output(\"\\nProcessing wells...\")\n",
    "    \n",
    "    for idx, well in enumerate(wells_to_process):\n",
    "        station_code = well['Station_Code']\n",
    "        \n",
    "        if (idx + 1) % 10 == 0 or idx == 0:\n",
    "            pct = (idx + 1) / len(wells_to_process) * 100\n",
    "            write_output(f\"Progress: {idx + 1}/{len(wells_to_process)} ({pct:.1f}%) - Current: {station_code}\")\n",
    "        \n",
    "        try:\n",
    "            result = process_single_well(well, df_piez_subset, piez_codigo_col)\n",
    "            \n",
    "            if result is not None:\n",
    "                well_results.append(result)\n",
    "                processed_wells.add(station_code)\n",
    "                successful_analyses += 1\n",
    "            else:\n",
    "                failed_analyses += 1\n",
    "                processed_wells.add(station_code)\n",
    "        \n",
    "        except Exception as e:\n",
    "            write_output(f\"Error processing {station_code}: {e}\")\n",
    "            failed_analyses += 1\n",
    "            processed_wells.add(station_code)\n",
    "        \n",
    "        if (idx + 1) % 10 == 0:\n",
    "            save_progress(processed_wells, well_results, idx + 1, len(wells_to_process))\n",
    "            gc.collect()\n",
    "            K.clear_session()\n",
    "\n",
    "save_progress(processed_wells, well_results, len(wells_to_process), len(wells_to_process))\n",
    "\n",
    "df_well_results = pd.DataFrame(well_results)\n",
    "\n",
    "write_output(f\"\\nAnalysis complete!\")\n",
    "write_output(f\"Successfully analyzed: {successful_analyses:,} wells\")\n",
    "write_output(f\"Failed analyses: {failed_analyses:,} wells\")\n",
    "write_output(f\"Skipped (already processed): {skipped_wells:,} wells\")\n",
    "\n",
    "write_output(f\"\\nModel availability summary:\")\n",
    "write_output(f\"   - Linear trend: {df_well_results['Linear_Slope_m_yr'].notna().sum():,} wells\")\n",
    "write_output(f\"   - Mann-Kendall: {(df_well_results['MK_Trend'] != 'N/A').sum():,} wells\")\n",
    "write_output(f\"   - Sen's Slope: {df_well_results['Sens_Slope_m_yr'].notna().sum():,} wells\")\n",
    "write_output(f\"   - ARIMA: {df_well_results['ARIMA_Pred_2030'].notna().sum():,} wells\")\n",
    "write_output(f\"   - Holt-Winters: {df_well_results['HW_Pred_2030'].notna().sum():,} wells\")\n",
    "write_output(f\"   - Prophet: {df_well_results['Prophet_Pred_2030'].notna().sum():,} wells\")\n",
    "write_output(f\"   - LSTM: {df_well_results['LSTM_Pred_2030'].notna().sum():,} wells\")\n",
    "write_output(f\"   - BiLSTM: {df_well_results['BiLSTM_Pred_2030'].notna().sum():,} wells\")\n",
    "write_output(f\"   - Ensemble: {df_well_results['Ensemble_Pred_2030'].notna().sum():,} wells\")\n",
    "\n",
    "write_output(\"\\n\" + \"=\"*80)\n",
    "write_output(\"SECTION 3: SUMMARY STATISTICS\")\n",
    "write_output(\"=\"*80)\n",
    "\n",
    "total_wells_analyzed = len(df_well_results)\n",
    "wells_with_decreasing_linear = len(df_well_results[df_well_results['Linear_Trend'] == 'Decreasing'])\n",
    "wells_with_decreasing_mk = len(df_well_results[df_well_results['MK_Direction'] == 'Decreasing'])\n",
    "wells_with_decreasing_consensus = len(df_well_results[df_well_results['Consensus_Trend'] == 'Decreasing'])\n",
    "wells_with_significant_linear = len(df_well_results[df_well_results['Linear_Significant'] == True])\n",
    "wells_with_significant_mk = len(df_well_results[df_well_results['MK_Significant'] == True])\n",
    "\n",
    "write_output(f\"\\nOVERALL STATISTICS:\")\n",
    "write_output(f\"Total wells analyzed: {total_wells_analyzed:,}\")\n",
    "write_output(f\"\\nTrend Detection Results:\")\n",
    "write_output(f\"Linear Regression - Decreasing: {wells_with_decreasing_linear:,} ({wells_with_decreasing_linear/total_wells_analyzed*100:.1f}%)\")\n",
    "write_output(f\"Linear Regression - Significant: {wells_with_significant_linear:,} ({wells_with_significant_linear/total_wells_analyzed*100:.1f}%)\")\n",
    "write_output(f\"Mann-Kendall - Decreasing: {wells_with_decreasing_mk:,} ({wells_with_decreasing_mk/total_wells_analyzed*100:.1f}%)\")\n",
    "write_output(f\"Mann-Kendall - Significant: {wells_with_significant_mk:,} ({wells_with_significant_mk/total_wells_analyzed*100:.1f}%)\")\n",
    "write_output(f\"Consensus - Decreasing: {wells_with_decreasing_consensus:,} ({wells_with_decreasing_consensus/total_wells_analyzed*100:.1f}%)\")\n",
    "\n",
    "valid_linear_slopes = df_well_results['Linear_Slope_m_yr'].dropna()\n",
    "valid_sens_slopes = df_well_results['Sens_Slope_m_yr'].dropna()\n",
    "\n",
    "write_output(f\"\\nTREND MAGNITUDE STATISTICS:\")\n",
    "write_output(f\"Linear OLS Trend (n={len(valid_linear_slopes)}):\")\n",
    "write_output(f\"   Mean: {valid_linear_slopes.mean():.4f} m/year\")\n",
    "write_output(f\"   Median: {valid_linear_slopes.median():.4f} m/year\")\n",
    "write_output(f\"   Std Dev: {valid_linear_slopes.std():.4f} m/year\")\n",
    "\n",
    "write_output(f\"\\nSen's Slope (robust, n={len(valid_sens_slopes)}):\")\n",
    "write_output(f\"   Mean: {valid_sens_slopes.mean():.4f} m/year\")\n",
    "write_output(f\"   Median: {valid_sens_slopes.median():.4f} m/year\")\n",
    "\n",
    "write_output(f\"\\nMODEL PERFORMANCE SUMMARY (NSE):\")\n",
    "for model in ['ARIMA', 'HW', 'Prophet', 'LSTM', 'BiLSTM']:\n",
    "    nse_col = f'{model}_NSE'\n",
    "    if nse_col in df_well_results.columns:\n",
    "        valid_nse = df_well_results[nse_col].dropna()\n",
    "        if len(valid_nse) > 0:\n",
    "            write_output(f\"   {model}: Mean NSE = {valid_nse.mean():.3f}, Median NSE = {valid_nse.median():.3f} (n={len(valid_nse)})\")\n",
    "\n",
    "write_output(f\"\\nPREDICTION SUMMARY (to {PREDICTION_YEAR}):\")\n",
    "for model_name in ['ARIMA', 'HW', 'Prophet', 'LSTM', 'BiLSTM', 'Ensemble']:\n",
    "    change_col = f'{model_name}_Change_2030'\n",
    "    if change_col in df_well_results.columns:\n",
    "        valid_preds = df_well_results[change_col].dropna()\n",
    "        if len(valid_preds) > 0:\n",
    "            write_output(f\"\\n{model_name} (n={len(valid_preds)}):\")\n",
    "            write_output(f\"   Mean change: {valid_preds.mean():+.2f} m\")\n",
    "            write_output(f\"   Median change: {valid_preds.median():+.2f} m\")\n",
    "\n",
    "write_output(\"\\n\" + \"=\"*80)\n",
    "write_output(\"SECTION 4: SAVING RESULTS\")\n",
    "write_output(\"=\"*80)\n",
    "\n",
    "excel_path = os.path.join(output_folder, 'Excel', 'Groundwater_Trend_Analysis_Complete.xlsx')\n",
    "\n",
    "write_output(f\"\\nSaving Excel file: {os.path.basename(excel_path)}\")\n",
    "\n",
    "with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "    summary_data = {\n",
    "        'Parameter': [\n",
    "            'Analysis Date',\n",
    "            'Total Wells Analyzed',\n",
    "            'Wells with Decreasing Trend (Linear)',\n",
    "            'Wells with Decreasing Trend (Mann-Kendall)',\n",
    "            'Wells with Decreasing Trend (Consensus)',\n",
    "            'Percentage Decreasing (Consensus)',\n",
    "            'Wells with Significant Linear Trend',\n",
    "            'Wells with Significant MK Trend',\n",
    "            'Mean Linear Slope (m/year)',\n",
    "            'Median Linear Slope (m/year)',\n",
    "            'Mean Sens Slope (m/year)',\n",
    "            'Mean ARIMA NSE',\n",
    "            'Mean LSTM NSE',\n",
    "            'Mean Ensemble Predicted Change (m)',\n",
    "            'Minimum Recent Year Required',\n",
    "            'Minimum Records Required',\n",
    "            'Minimum Years Span Required',\n",
    "            'Prediction Target Year',\n",
    "            'Significance Level',\n",
    "            'Confidence Level'\n",
    "        ],\n",
    "        'Value': [\n",
    "            datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            total_wells_analyzed,\n",
    "            wells_with_decreasing_linear,\n",
    "            wells_with_decreasing_mk,\n",
    "            wells_with_decreasing_consensus,\n",
    "            f\"{wells_with_decreasing_consensus/total_wells_analyzed*100:.2f}%\",\n",
    "            wells_with_significant_linear,\n",
    "            wells_with_significant_mk,\n",
    "            f\"{valid_linear_slopes.mean():.4f}\",\n",
    "            f\"{valid_linear_slopes.median():.4f}\",\n",
    "            f\"{valid_sens_slopes.mean():.4f}\",\n",
    "            f\"{df_well_results['ARIMA_NSE'].mean():.3f}\" if df_well_results['ARIMA_NSE'].notna().any() else \"N/A\",\n",
    "            f\"{df_well_results['LSTM_NSE'].mean():.3f}\" if df_well_results['LSTM_NSE'].notna().any() else \"N/A\",\n",
    "            f\"{df_well_results['Ensemble_Change_2030'].mean():.2f}\" if df_well_results['Ensemble_Change_2030'].notna().any() else \"N/A\",\n",
    "            MIN_RECENT_YEAR,\n",
    "            MIN_RECORDS,\n",
    "            MIN_YEARS_SPAN,\n",
    "            PREDICTION_YEAR,\n",
    "            SIGNIFICANCE_LEVEL,\n",
    "            CONFIDENCE_LEVEL\n",
    "        ]\n",
    "    }\n",
    "    pd.DataFrame(summary_data).to_excel(writer, sheet_name='Summary', index=False)\n",
    "    df_well_results.to_excel(writer, sheet_name='All_Wells_Details', index=False)\n",
    "\n",
    "write_output(\"Excel file saved successfully\")\n",
    "\n",
    "csv_path = os.path.join(output_folder, 'Excel', 'Groundwater_Trend_Analysis_Complete.csv')\n",
    "df_well_results.to_csv(csv_path, index=False)\n",
    "write_output(f\"CSV file saved: {os.path.basename(csv_path)}\")\n",
    "\n",
    "txt_path = os.path.join(output_folder, 'Text_Output', 'Groundwater_Trend_Analysis_Complete_Report.txt')\n",
    "try:\n",
    "    import shutil\n",
    "    shutil.copy(log_file_path, txt_path)\n",
    "    write_output(f\"Text report saved: {os.path.basename(txt_path)}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if os.path.exists(progress_file):\n",
    "    try:\n",
    "        os.remove(progress_file)\n",
    "        write_output(\"Progress tracker file cleaned up\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "write_output(\"\\n\" + \"=\"*80)\n",
    "write_output(\"ANALYSIS COMPLETED SUCCESSFULLY\")\n",
    "write_output(\"=\"*80)\n",
    "write_output(f\"Output Directory: {output_folder}\")\n",
    "write_output(f\"Analysis completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "write_output(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-water]",
   "language": "python",
   "name": "conda-env-.conda-water-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
