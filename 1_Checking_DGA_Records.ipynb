{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a878a48-98be-471a-9a9b-f9798425410f",
   "metadata": {},
   "source": [
    "# Procesando los datos de la DGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d7d303-1154-41e7-8eb5-200dde3aea53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from pyproj import Transformer\n",
    "from shapely.geometry import Point\n",
    "from shapely.prepared import prep\n",
    "\n",
    "INPUT_XLSX = r\"\\assessment_of_wells_chile\\data\\DGA\\DGA_consolidado_nacional\\DerechosConcedidos.xlsx\"\n",
    "\n",
    "OUTPUT_CSV_FILTERED = r\"\\assessment_of_wells_chile\\data\\DGA\\DGA_consolidado_nacional\\DerechosConcedidos_edit_Subterraneo.csv\"\n",
    "OUTPUT_XLSX_FILTERED = OUTPUT_CSV_FILTERED.replace(\".csv\", \".xlsx\")\n",
    "\n",
    "GDB_PATH = r\"\\assessment_of_wells_chile\\arcgis\\assessment_of_wells_chile\\Default.gdb\"\n",
    "LAYER_NAME = \"CHL_Country\"\n",
    "\n",
    "OUTPUTS_FOLDER = r\"\\assessment_of_wells_chile\\data\\DGA\\DGA_consolidado_nacional\\outputs\"\n",
    "os.makedirs(OUTPUTS_FOLDER, exist_ok=True)\n",
    "\n",
    "def normalize_columns(cols: pd.Index) -> pd.Index:\n",
    "    s = cols.astype(str)\n",
    "    s = s.str.replace(\"\\n\", \" \", regex=False).str.replace(\"\\t\", \" \", regex=False)\n",
    "    s = s.str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "    return s\n",
    "\n",
    "def strip_accents(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    return \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "\n",
    "def norm_text(s) -> str:\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    s = str(s).strip()\n",
    "    s = strip_accents(s).casefold()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def clean_illegal_characters(val):\n",
    "    if pd.isna(val) or not isinstance(val, str):\n",
    "        return val\n",
    "    val = re.sub(r\"[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\x9f]\", \"\", val)\n",
    "    return val.replace(\"\\r\\n\", \" \").replace(\"\\r\", \" \").strip()\n",
    "\n",
    "def clean_number(val):\n",
    "    if pd.isna(val):\n",
    "        return np.nan\n",
    "    s = str(val).strip().replace(\"\\t\", \"\").replace(\" \", \"\")\n",
    "    if s == \"\":\n",
    "        return np.nan\n",
    "\n",
    "    if re.search(r\"[a-zA-Z]\", s) and \"E\" not in s.upper():\n",
    "        return np.nan\n",
    "\n",
    "    if \".\" in s and \",\" in s:\n",
    "        s = s.replace(\".\", \"\").replace(\",\", \".\")\n",
    "    elif \",\" in s:\n",
    "        s = s.replace(\",\", \".\")\n",
    "\n",
    "    try:\n",
    "        return float(s)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "def parse_dms_compact(val):\n",
    "    if pd.isna(val):\n",
    "        return np.nan\n",
    "    try:\n",
    "        v = float(val)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "    if v == 0:\n",
    "        return np.nan\n",
    "\n",
    "    if abs(v) < 90:\n",
    "        return -abs(v)\n",
    "\n",
    "    s_val = \"{:.6f}\".format(v).split(\".\")[0].zfill(6)\n",
    "\n",
    "    try:\n",
    "        deg = float(s_val[:-4])\n",
    "        minu = float(s_val[-4:-2])\n",
    "        sec = float(s_val[-2:])\n",
    "\n",
    "        if not (0 <= minu < 60) or not (0 <= sec < 60):\n",
    "            return np.nan\n",
    "\n",
    "        decimal = deg + (minu / 60.0) + (sec / 3600.0)\n",
    "        return -decimal\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def is_plausible_chile_bbox(lat, lon) -> bool:\n",
    "    if pd.isna(lat) or pd.isna(lon):\n",
    "        return False\n",
    "    try:\n",
    "        lat = float(lat)\n",
    "        lon = float(lon)\n",
    "    except Exception:\n",
    "        return False\n",
    "    return (-56.8 <= lat <= -16.0) and (-76.8 <= lon <= -66.0)\n",
    "\n",
    "def pick_col(df: pd.DataFrame, candidates, required=True):\n",
    "    for c in df.columns:\n",
    "        for cand in candidates:\n",
    "            if callable(cand):\n",
    "                try:\n",
    "                    if cand(c):\n",
    "                        return c\n",
    "                except Exception:\n",
    "                    continue\n",
    "            else:\n",
    "                if c == cand:\n",
    "                    return c\n",
    "    if required:\n",
    "        raise KeyError(f\"No se encontr√≥ columna. Candidatos: {candidates}\")\n",
    "    return None\n",
    "\n",
    "def save_df_both(df: pd.DataFrame, base_path_no_ext: str, index=False):\n",
    "    xlsx_path = base_path_no_ext + \".xlsx\"\n",
    "    csv_path = base_path_no_ext + \".csv\"\n",
    "    df.to_excel(xlsx_path, index=index)\n",
    "    df.to_csv(csv_path, index=index, encoding=\"utf-8-sig\")\n",
    "    return xlsx_path, csv_path\n",
    "\n",
    "def save_multisheet_with_csvs(sheets: dict, xlsx_path: str, csv_folder: str):\n",
    "    os.makedirs(csv_folder, exist_ok=True)\n",
    "    with pd.ExcelWriter(xlsx_path, engine=\"openpyxl\") as writer:\n",
    "        for name, df_ in sheets.items():\n",
    "            sheet = str(name)[:31]\n",
    "            df_.to_excel(writer, sheet_name=sheet, index=False)\n",
    "\n",
    "            csv_path = os.path.join(csv_folder, f\"{name}.csv\")\n",
    "            xlsx_sheet_path = os.path.join(csv_folder, f\"{name}.xlsx\")\n",
    "\n",
    "            df_.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "            df_.to_excel(xlsx_sheet_path, index=False)\n",
    "\n",
    "_transformers_cache = {}\n",
    "\n",
    "def parse_datum_label(datum_val):\n",
    "    s = norm_text(datum_val)\n",
    "    if not s:\n",
    "        return None\n",
    "    if \"1984\" in s or \"wgs\" in s:\n",
    "        return \"1984\"\n",
    "    if \"1956\" in s or \"psad\" in s:\n",
    "        return \"1956\"\n",
    "    if \"1969\" in s or \"sad\" in s:\n",
    "        return \"1969\"\n",
    "    return None\n",
    "\n",
    "def get_transformer_utm_to_wgs84(datum_val, huso_val):\n",
    "    d = parse_datum_label(datum_val)\n",
    "    if d is None:\n",
    "        return None\n",
    "    try:\n",
    "        z = int(round(float(huso_val)))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    epsg = None\n",
    "    if d == \"1984\":\n",
    "        epsg = {17: 32717, 18: 32718, 19: 32719, 20: 32720}.get(z)\n",
    "    elif d == \"1956\":\n",
    "        epsg = {17: 24877, 18: 24878, 19: 24879, 20: None}.get(z)\n",
    "    elif d == \"1969\":\n",
    "        epsg = {17: 29187, 18: 29188, 19: 29189, 20: None}.get(z)\n",
    "\n",
    "    if epsg is None:\n",
    "        return None\n",
    "\n",
    "    if epsg not in _transformers_cache:\n",
    "        _transformers_cache[epsg] = Transformer.from_crs(f\"EPSG:{epsg}\", \"EPSG:4326\", always_xy=True)\n",
    "    return _transformers_cache[epsg]\n",
    "\n",
    "def get_transformer_geog_to_wgs84(datum_val):\n",
    "    d = parse_datum_label(datum_val)\n",
    "    if d is None or d == \"1984\":\n",
    "        return None\n",
    "\n",
    "    src = None\n",
    "    if d == \"1956\":\n",
    "        src = \"EPSG:4248\"\n",
    "    elif d == \"1969\":\n",
    "        src = \"EPSG:4618\"\n",
    "\n",
    "    if src is None:\n",
    "        return None\n",
    "\n",
    "    key = (src, \"EPSG:4326\")\n",
    "    if key not in _transformers_cache:\n",
    "        _transformers_cache[key] = Transformer.from_crs(src, \"EPSG:4326\", always_xy=True)\n",
    "    return _transformers_cache[key]\n",
    "\n",
    "def choose_datum_cols(df, utm_n_col, utm_e_col, lat_col, lon_col):\n",
    "    datum_cols = [c for c in df.columns if norm_text(c).startswith(\"datum\")]\n",
    "    if not datum_cols:\n",
    "        return None, None\n",
    "    if len(datum_cols) == 1:\n",
    "        return datum_cols[0], datum_cols[0]\n",
    "\n",
    "    utm_mask = pd.Series(False, index=df.index)\n",
    "    ll_mask = pd.Series(False, index=df.index)\n",
    "\n",
    "    if utm_n_col and utm_e_col:\n",
    "        utm_mask = df[utm_n_col].notna() & df[utm_e_col].notna()\n",
    "    if lat_col and lon_col:\n",
    "        ll_mask = df[lat_col].notna() & df[lon_col].notna()\n",
    "\n",
    "    scores_utm = {}\n",
    "    scores_ll = {}\n",
    "    for c in datum_cols:\n",
    "        scores_utm[c] = int((df[c].notna() & utm_mask).sum())\n",
    "        scores_ll[c] = int((df[c].notna() & ll_mask).sum())\n",
    "\n",
    "    datum_utm = max(scores_utm, key=scores_utm.get)\n",
    "    datum_ll = max(scores_ll, key=scores_ll.get)\n",
    "    return datum_utm, datum_ll\n",
    "\n",
    "def utm_inputs_valid(norte, este, huso, datum) -> bool:\n",
    "    if pd.isna(norte) or pd.isna(este) or pd.isna(huso) or pd.isna(datum):\n",
    "        return False\n",
    "    try:\n",
    "        n = float(norte)\n",
    "        e = float(este)\n",
    "        h = int(round(float(huso)))\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "    if h < 17 or h > 20:\n",
    "        return False\n",
    "\n",
    "    if not (100000 <= e <= 900000):\n",
    "        return False\n",
    "    if not (1000000 <= n <= 10000000):\n",
    "        return False\n",
    "\n",
    "    return get_transformer_utm_to_wgs84(datum, huso) is not None\n",
    "\n",
    "def latlon_inputs_valid(lat_raw, lon_raw) -> bool:\n",
    "    if pd.isna(lat_raw) or pd.isna(lon_raw):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def main():\n",
    "    df = pd.read_excel(INPUT_XLSX, header=6, engine=\"openpyxl\")\n",
    "    df.columns = normalize_columns(df.columns)\n",
    "\n",
    "    df[\"_row_id\"] = np.arange(len(df), dtype=int)\n",
    "    df[\"_excel_row\"] = df[\"_row_id\"] + 8\n",
    "\n",
    "    col_nat = pick_col(\n",
    "        df,\n",
    "        candidates=[\n",
    "            \"Naturaleza del Agua\",\n",
    "            lambda c: (\"naturaleza\" in norm_text(c) and \"agua\" in norm_text(c)),\n",
    "        ],\n",
    "        required=True,\n",
    "    )\n",
    "\n",
    "    mask_sub = df[col_nat].apply(norm_text).eq(\"subterranea\")\n",
    "    df_sub = df.loc[mask_sub].copy()\n",
    "\n",
    "    col_expediente = pick_col(\n",
    "        df_sub,\n",
    "        candidates=[\n",
    "            \"C√≥digo de Expediente\",\n",
    "            lambda c: (\"codigo\" in norm_text(c) and \"expediente\" in norm_text(c)),\n",
    "        ],\n",
    "        required=True,\n",
    "    )\n",
    "\n",
    "    col_utm_n = pick_col(\n",
    "        df_sub,\n",
    "        [lambda c: (\"utm\" in norm_text(c) and \"norte\" in norm_text(c) and \"capt\" in norm_text(c))],\n",
    "        required=False,\n",
    "    )\n",
    "    col_utm_e = pick_col(\n",
    "        df_sub,\n",
    "        [lambda c: (\"utm\" in norm_text(c) and \"este\" in norm_text(c) and \"capt\" in norm_text(c))],\n",
    "        required=False,\n",
    "    )\n",
    "    col_huso = pick_col(df_sub, [lambda c: norm_text(c) == \"huso\"], required=False)\n",
    "\n",
    "    col_lat_cap = pick_col(\n",
    "        df_sub, [lambda c: (\"latitud\" in norm_text(c) and \"capt\" in norm_text(c))], required=False\n",
    "    )\n",
    "    col_lon_cap = pick_col(\n",
    "        df_sub, [lambda c: (\"longitud\" in norm_text(c) and \"capt\" in norm_text(c))], required=False\n",
    "    )\n",
    "\n",
    "    datum_utm_col, datum_ll_col = choose_datum_cols(df_sub, col_utm_n, col_utm_e, col_lat_cap, col_lon_cap)\n",
    "\n",
    "    months = [\n",
    "        \"Enero\", \"Febrero\", \"Marzo\", \"Abril\", \"Mayo\", \"Junio\",\n",
    "        \"Julio\", \"Agosto\", \"Septiembre\", \"Octubre\", \"Noviembre\", \"Diciembre\"\n",
    "    ]\n",
    "    month_cols = [c for c in df_sub.columns if c in months]\n",
    "\n",
    "    col_caudal_anual_prom = pick_col(\n",
    "        df_sub,\n",
    "        candidates=[\n",
    "            \"Caudal Anual Prom\",\n",
    "            lambda c: (\"caudal\" in norm_text(c) and \"anual\" in norm_text(c) and \"prom\" in norm_text(c)),\n",
    "        ],\n",
    "        required=False,\n",
    "    )\n",
    "\n",
    "    numeric_cols = []\n",
    "    numeric_cols += month_cols\n",
    "    if col_caudal_anual_prom:\n",
    "        numeric_cols.append(col_caudal_anual_prom)\n",
    "    for c in [col_utm_n, col_utm_e, col_huso, col_lat_cap, col_lon_cap]:\n",
    "        if c:\n",
    "            numeric_cols.append(c)\n",
    "\n",
    "    numeric_cols = sorted(set(numeric_cols))\n",
    "    for c in numeric_cols:\n",
    "        df_sub[c] = df_sub[c].apply(clean_number)\n",
    "\n",
    "    for c in df_sub.select_dtypes(include=[\"object\"]).columns:\n",
    "        df_sub[c] = df_sub[c].apply(clean_illegal_characters)\n",
    "\n",
    "    df_sub[\"lat_wgs84\"] = np.nan\n",
    "    df_sub[\"lon_wgs84\"] = np.nan\n",
    "    df_sub[\"Metodo_Coordenadas\"] = \"\"\n",
    "    df_sub[\"Coord_Valida_Chile_BBox\"] = False\n",
    "    df_sub[\"Huso_Usado_En_Calculo\"] = np.nan\n",
    "\n",
    "    def calculate_wgs84(row):\n",
    "        if col_utm_n and col_utm_e and col_huso and datum_utm_col:\n",
    "            norte = row[col_utm_n]\n",
    "            este = row[col_utm_e]\n",
    "            huso = row[col_huso]\n",
    "            datum_utm = row[datum_utm_col]\n",
    "\n",
    "            if utm_inputs_valid(norte, este, huso, datum_utm):\n",
    "                tr = get_transformer_utm_to_wgs84(datum_utm, huso)\n",
    "                try:\n",
    "                    lon, lat = tr.transform(float(este), float(norte))\n",
    "\n",
    "                    if is_plausible_chile_bbox(lat, lon):\n",
    "                        return lat, lon, \"UTM\", True, float(round(float(huso), 0))\n",
    "                    if is_plausible_chile_bbox(lon, lat):\n",
    "                        return lon, lat, \"UTM\", True, float(round(float(huso), 0))\n",
    "\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "        if col_lat_cap and col_lon_cap:\n",
    "            lat_raw = row[col_lat_cap]\n",
    "            lon_raw = row[col_lon_cap]\n",
    "            datum_ll = row[datum_ll_col] if datum_ll_col else np.nan\n",
    "\n",
    "            if latlon_inputs_valid(lat_raw, lon_raw):\n",
    "                lat = parse_dms_compact(lat_raw)\n",
    "                lon = parse_dms_compact(lon_raw)\n",
    "\n",
    "                if is_plausible_chile_bbox(lat, lon):\n",
    "                    trg = get_transformer_geog_to_wgs84(datum_ll)\n",
    "                    if trg is not None:\n",
    "                        try:\n",
    "                            lon2, lat2 = trg.transform(float(lon), float(lat))\n",
    "                            if is_plausible_chile_bbox(lat2, lon2):\n",
    "                                return lat2, lon2, \"LatLon\", True, np.nan\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                    return lat, lon, \"LatLon\", True, np.nan\n",
    "\n",
    "        return np.nan, np.nan, \"\", False, np.nan\n",
    "\n",
    "    latlons = df_sub.apply(calculate_wgs84, axis=1, result_type=\"expand\")\n",
    "    latlons.columns = [\"lat_wgs84\", \"lon_wgs84\", \"Metodo_Coordenadas\", \"Coord_Valida_Chile_BBox\", \"Huso_Usado_En_Calculo\"]\n",
    "    df_sub[latlons.columns] = latlons\n",
    "\n",
    "    df_sub.to_csv(OUTPUT_CSV_FILTERED, index=False, encoding=\"utf-8-sig\")\n",
    "    df_sub.to_excel(OUTPUT_XLSX_FILTERED, index=False)\n",
    "\n",
    "    gdf_chile = gpd.read_file(GDB_PATH, layer=LAYER_NAME)\n",
    "    if str(gdf_chile.crs).upper() != \"EPSG:4326\":\n",
    "        gdf_chile = gdf_chile.to_crs(\"EPSG:4326\")\n",
    "\n",
    "    try:\n",
    "        gdf_chile[\"geometry\"] = gdf_chile.geometry.make_valid()\n",
    "    except Exception:\n",
    "        gdf_chile[\"geometry\"] = gdf_chile.geometry.buffer(0)\n",
    "\n",
    "    chile_geom = gdf_chile.geometry.unary_union\n",
    "    chile_prepared = prep(chile_geom)\n",
    "\n",
    "    def point_in_chile(lon, lat) -> bool:\n",
    "        if pd.isna(lon) or pd.isna(lat):\n",
    "            return False\n",
    "        try:\n",
    "            p = Point(float(lon), float(lat))\n",
    "        except Exception:\n",
    "            return False\n",
    "        return bool(chile_prepared.covers(p))\n",
    "\n",
    "    df_con_coords = df_sub.dropna(subset=[\"lat_wgs84\", \"lon_wgs84\"]).copy()\n",
    "    df_sin_coords = df_sub[df_sub[[\"lat_wgs84\", \"lon_wgs84\"]].isna().any(axis=1)].copy()\n",
    "\n",
    "    df_zonefix_diag = []\n",
    "\n",
    "    def try_fix_zone(norte, este, huso, datum):\n",
    "        if pd.isna(norte) or pd.isna(este) or pd.isna(huso) or pd.isna(datum):\n",
    "            return None\n",
    "        try:\n",
    "            n = float(norte)\n",
    "            e = float(este)\n",
    "            h0 = int(round(float(huso)))\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "        candidates = [h0, h0 - 1, h0 + 1]\n",
    "        candidates = [h for h in candidates if 17 <= h <= 20]\n",
    "\n",
    "        for h in candidates:\n",
    "            tr = get_transformer_utm_to_wgs84(datum, h)\n",
    "            if tr is None:\n",
    "                continue\n",
    "            try:\n",
    "                lon, lat = tr.transform(e, n)\n",
    "            except Exception:\n",
    "                continue\n",
    "            if point_in_chile(lon, lat):\n",
    "                return lat, lon, h\n",
    "        return None\n",
    "\n",
    "    if len(df_con_coords) > 0:\n",
    "        inside_mask_initial = df_con_coords.apply(lambda r: point_in_chile(r[\"lon_wgs84\"], r[\"lat_wgs84\"]), axis=1)\n",
    "    else:\n",
    "        inside_mask_initial = pd.Series([], dtype=bool)\n",
    "\n",
    "    df_inside_initial = df_con_coords.loc[inside_mask_initial].copy()\n",
    "    df_outside_geo_initial = df_con_coords.loc[~inside_mask_initial].copy()\n",
    "\n",
    "    if len(df_outside_geo_initial) > 0 and col_utm_n and col_utm_e and col_huso and datum_utm_col:\n",
    "        mask_utm_outside = df_outside_geo_initial[\"Metodo_Coordenadas\"].eq(\"UTM\")\n",
    "        df_candidates = df_outside_geo_initial.loc[mask_utm_outside].copy()\n",
    "\n",
    "        for idx, row in df_candidates.iterrows():\n",
    "            res = try_fix_zone(\n",
    "                norte=row[col_utm_n],\n",
    "                este=row[col_utm_e],\n",
    "                huso=row[col_huso],\n",
    "                datum=row[datum_utm_col],\n",
    "            )\n",
    "            df_zonefix_diag.append({\n",
    "                \"_row_id\": row.get(\"_row_id\", np.nan),\n",
    "                \"_excel_row\": row.get(\"_excel_row\", np.nan),\n",
    "                \"Codigo_Expediente\": row.get(col_expediente, \"\"),\n",
    "                \"huso_original\": row.get(col_huso, np.nan),\n",
    "                \"datum_utm\": row.get(datum_utm_col, np.nan),\n",
    "                \"lat_pre\": row.get(\"lat_wgs84\", np.nan),\n",
    "                \"lon_pre\": row.get(\"lon_wgs84\", np.nan),\n",
    "                \"fixed\": res is not None,\n",
    "                \"huso_fixed\": (res[2] if res else np.nan),\n",
    "                \"lat_post\": (res[0] if res else np.nan),\n",
    "                \"lon_post\": (res[1] if res else np.nan),\n",
    "            })\n",
    "\n",
    "            if res is None:\n",
    "                continue\n",
    "\n",
    "            lat_new, lon_new, huso_new = res\n",
    "\n",
    "            df_sub.at[idx, \"lat_wgs84\"] = lat_new\n",
    "            df_sub.at[idx, \"lon_wgs84\"] = lon_new\n",
    "            df_sub.at[idx, \"Metodo_Coordenadas\"] = f\"UTM_zone_fixed_to_{huso_new}\"\n",
    "            df_sub.at[idx, \"Huso_Usado_En_Calculo\"] = float(huso_new)\n",
    "\n",
    "    if df_zonefix_diag:\n",
    "        df_zonefix_diag = pd.DataFrame(df_zonefix_diag)\n",
    "        diag_base = os.path.join(OUTPUTS_FOLDER, \"Diagnostico_Rescate_Huso_UTM\")\n",
    "        save_df_both(df_zonefix_diag, diag_base, index=False)\n",
    "\n",
    "    df_con_coords = df_sub.dropna(subset=[\"lat_wgs84\", \"lon_wgs84\"]).copy()\n",
    "    df_sin_coords = df_sub[df_sub[[\"lat_wgs84\", \"lon_wgs84\"]].isna().any(axis=1)].copy()\n",
    "\n",
    "    if len(df_con_coords) > 0:\n",
    "        inside_mask = df_con_coords.apply(lambda r: point_in_chile(r[\"lon_wgs84\"], r[\"lat_wgs84\"]), axis=1)\n",
    "    else:\n",
    "        inside_mask = pd.Series([], dtype=bool)\n",
    "\n",
    "    df_inside = df_con_coords.loc[inside_mask].copy()\n",
    "    df_outside_geo = df_con_coords.loc[~inside_mask].copy()\n",
    "\n",
    "    df_inside[\"Estado_Ubicacion\"] = \"Validado en Chile\"\n",
    "    df_outside_geo[\"Estado_Ubicacion\"] = \"Fuera del limite\"\n",
    "    df_sin_coords[\"Estado_Ubicacion\"] = \"Sin coordenadas\"\n",
    "\n",
    "    df_outside_final = pd.concat([df_outside_geo, df_sin_coords], ignore_index=True)\n",
    "\n",
    "    def norm_code_series(s: pd.Series) -> pd.Series:\n",
    "        return s.fillna(\"\").astype(str).str.strip()\n",
    "\n",
    "    df_inside[\"_exp_norm\"] = norm_code_series(df_inside[col_expediente]) if len(df_inside) else \"\"\n",
    "    df_outside_final[\"_exp_norm\"] = norm_code_series(df_outside_final[col_expediente]) if len(df_outside_final) else \"\"\n",
    "    df_sub[\"_exp_norm\"] = norm_code_series(df_sub[col_expediente])\n",
    "\n",
    "    inside_codes = set(pd.Series(df_inside[\"_exp_norm\"]).replace(\"\", np.nan).dropna().unique()) if len(df_inside) else set()\n",
    "\n",
    "    mask_remove_outside = df_outside_final[\"_exp_norm\"].isin(inside_codes) & df_outside_final[\"_exp_norm\"].ne(\"\")\n",
    "    df_outside_removed = df_outside_final.loc[mask_remove_outside].copy()\n",
    "    df_outside_removed[\"Motivo_Eliminacion\"] = \"Expediente existe en grupo DENTRO\"\n",
    "\n",
    "    df_outside_final_clean = df_outside_final.loc[~mask_remove_outside].copy()\n",
    "\n",
    "    dup_mask_all = df_sub[\"_exp_norm\"].ne(\"\") & df_sub[\"_exp_norm\"].duplicated(keep=False)\n",
    "    df_duplicates_all = df_sub.loc[dup_mask_all].copy()\n",
    "\n",
    "    codes_removed = sorted(set(df_outside_removed[\"_exp_norm\"].replace(\"\", np.nan).dropna().unique()))\n",
    "    df_dup_pairs = pd.concat(\n",
    "        [\n",
    "            df_inside[df_inside[\"_exp_norm\"].isin(codes_removed)].assign(Grupo=\"DENTRO\"),\n",
    "            df_outside_removed.assign(Grupo=\"FUERA_ELIMINADO\"),\n",
    "        ],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "\n",
    "    dup_summary = pd.DataFrame(\n",
    "        {\n",
    "            \"metric\": [\n",
    "                \"n_subterranea_total\",\n",
    "                \"n_inside\",\n",
    "                \"n_outside_final_antes\",\n",
    "                \"n_outside_eliminados_por_duplicado_con_inside\",\n",
    "                \"n_outside_final_despues\",\n",
    "                \"n_codigos_con_eliminacion\",\n",
    "                \"n_registros_con_codigo_duplicado_en_subterranea\",\n",
    "            ],\n",
    "            \"value\": [\n",
    "                int(len(df_sub)),\n",
    "                int(len(df_inside)),\n",
    "                int(len(df_outside_final)),\n",
    "                int(len(df_outside_removed)),\n",
    "                int(len(df_outside_final_clean)),\n",
    "                int(len(codes_removed)),\n",
    "                int(len(df_duplicates_all)),\n",
    "            ],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    dup_xlsx = os.path.join(OUTPUTS_FOLDER, \"Duplicados_Codigo_de_Expediente.xlsx\")\n",
    "    dup_csv_folder = os.path.join(OUTPUTS_FOLDER, \"Duplicados_Codigo_de_Expediente_CSVs\")\n",
    "    save_multisheet_with_csvs(\n",
    "        sheets={\n",
    "            \"Resumen\": dup_summary,\n",
    "            \"Fuera_Eliminados\": df_outside_removed.drop(columns=[\"_exp_norm\"], errors=\"ignore\"),\n",
    "            \"Pares_Dentro_vs_Fuera\": df_dup_pairs.drop(columns=[\"_exp_norm\"], errors=\"ignore\"),\n",
    "            \"Duplicados_Todos_Subterranea\": df_duplicates_all.drop(columns=[\"_exp_norm\"], errors=\"ignore\"),\n",
    "        },\n",
    "        xlsx_path=dup_xlsx,\n",
    "        csv_folder=dup_csv_folder,\n",
    "    )\n",
    "\n",
    "    for _df in [df_inside, df_outside_final_clean]:\n",
    "        if \"_exp_norm\" in _df.columns:\n",
    "            _df.drop(columns=[\"_exp_norm\"], inplace=True)\n",
    "\n",
    "    inside_base = os.path.join(OUTPUTS_FOLDER, \"Registros_En_Chile\")\n",
    "    outside_base = os.path.join(OUTPUTS_FOLDER, \"Registros_Fuera_Chile\")\n",
    "\n",
    "    save_df_both(df_inside, inside_base, index=False)\n",
    "    save_df_both(df_outside_final_clean, outside_base, index=False)\n",
    "\n",
    "    region_col = pick_col(df_sub, [\"Regi√≥n\", \"Region\", lambda c: norm_text(c) == \"region\"], required=False)\n",
    "    prov_col = pick_col(df_sub, [\"Provincia\", lambda c: norm_text(c) == \"provincia\"], required=False)\n",
    "    com_col = pick_col(df_sub, [\"Comuna\", lambda c: norm_text(c) == \"comuna\"], required=False)\n",
    "\n",
    "    df_outside_only_geo_clean = df_outside_final_clean[\n",
    "        df_outside_final_clean[\"Estado_Ubicacion\"].eq(\"Fuera del limite\")\n",
    "    ].copy()\n",
    "\n",
    "    if len(df_outside_only_geo_clean) > 0 and all([region_col, prov_col, com_col]):\n",
    "        dens = (\n",
    "            df_outside_only_geo_clean\n",
    "            .groupby([region_col, prov_col, com_col], dropna=False)\n",
    "            .size()\n",
    "            .reset_index(name=\"n_puntos_fuera\")\n",
    "            .sort_values(\"n_puntos_fuera\", ascending=False)\n",
    "        )\n",
    "    else:\n",
    "        dens = pd.DataFrame(columns=[region_col or \"Regi√≥n\", prov_col or \"Provincia\", com_col or \"Comuna\", \"n_puntos_fuera\"])\n",
    "\n",
    "    dens_base = os.path.join(OUTPUTS_FOLDER, \"Analisis_Densidad_Fuera_Chile\")\n",
    "    save_df_both(dens, dens_base, index=False)\n",
    "\n",
    "    neg_xlsx = os.path.join(OUTPUTS_FOLDER, \"Analisis_Caudal_Anual_Prom_Negativo.xlsx\")\n",
    "    neg_csv_folder = os.path.join(OUTPUTS_FOLDER, \"Analisis_Caudal_Anual_Prom_Negativo_CSVs\")\n",
    "\n",
    "    if col_caudal_anual_prom and col_caudal_anual_prom in df_sub.columns:\n",
    "        map_inside = dict(zip(df_inside.get(\"_row_id\", []), df_inside.get(\"Estado_Ubicacion\", [])))\n",
    "        map_out = dict(zip(df_outside_final_clean.get(\"_row_id\", []), df_outside_final_clean.get(\"Estado_Ubicacion\", [])))\n",
    "\n",
    "        def get_state(rid):\n",
    "            if rid in map_inside:\n",
    "                return map_inside[rid]\n",
    "            if rid in map_out:\n",
    "                return map_out[rid]\n",
    "            return \"No clasificado\"\n",
    "\n",
    "        df_sub_state = df_sub.copy()\n",
    "        df_sub_state[\"Estado_Ubicacion\"] = df_sub_state[\"_row_id\"].apply(get_state)\n",
    "\n",
    "        df_neg = df_sub_state[\n",
    "            df_sub_state[col_caudal_anual_prom].notna() & (df_sub_state[col_caudal_anual_prom] < 0)\n",
    "        ].copy()\n",
    "\n",
    "        resumen_general = pd.DataFrame(\n",
    "            {\n",
    "                \"metric\": [\n",
    "                    \"registros_total_subterranea\",\n",
    "                    \"registros_con_caudal_anual_prom_no_nulo\",\n",
    "                    \"registros_caudal_anual_prom_negativo\",\n",
    "                ],\n",
    "                \"value\": [\n",
    "                    int(len(df_sub_state)),\n",
    "                    int(df_sub_state[col_caudal_anual_prom].notna().sum()),\n",
    "                    int(len(df_neg)),\n",
    "                ],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        resumen_por_estado = (\n",
    "            df_neg.groupby(\"Estado_Ubicacion\", dropna=False)\n",
    "            .size()\n",
    "            .reset_index(name=\"n_negativos\")\n",
    "            .sort_values(\"n_negativos\", ascending=False)\n",
    "        )\n",
    "\n",
    "        if all([region_col, prov_col, com_col]):\n",
    "            resumen_por_loc = (\n",
    "                df_neg.groupby([region_col, prov_col, com_col, \"Estado_Ubicacion\"], dropna=False)\n",
    "                .size()\n",
    "                .reset_index(name=\"n_negativos\")\n",
    "                .sort_values(\"n_negativos\", ascending=False)\n",
    "            )\n",
    "        else:\n",
    "            resumen_por_loc = pd.DataFrame()\n",
    "\n",
    "        save_multisheet_with_csvs(\n",
    "            sheets={\n",
    "                \"Resumen_General\": resumen_general,\n",
    "                \"Resumen_por_Estado\": resumen_por_estado,\n",
    "                \"Resumen_por_Region_Prov_Com\": resumen_por_loc,\n",
    "                \"Registros_Negativos\": df_neg,\n",
    "            },\n",
    "            xlsx_path=neg_xlsx,\n",
    "            csv_folder=neg_csv_folder,\n",
    "        )\n",
    "    else:\n",
    "        save_multisheet_with_csvs(\n",
    "            sheets={\"Error\": pd.DataFrame({\"error\": [\"No se encontr√≥ la columna 'Caudal Anual Prom' (o equivalente).\"]})},\n",
    "            xlsx_path=neg_xlsx,\n",
    "            csv_folder=neg_csv_folder,\n",
    "        )\n",
    "\n",
    "    print(\"OK. Archivos principales:\")\n",
    "    print(\" - Filtrado Subterranea:\", OUTPUT_XLSX_FILTERED, \"y\", OUTPUT_CSV_FILTERED)\n",
    "    print(\" - Outputs:\", OUTPUTS_FOLDER)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087e06d9-2b80-4103-a952-3a733292a198",
   "metadata": {},
   "source": [
    "# Downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9dacac-0a25-43a9-ae23-71513b025150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "from queue import Queue\n",
    "import sys\n",
    "import re\n",
    "\n",
    "TEST_MODE = False\n",
    "SHOW_BROWSER = False\n",
    "\n",
    "BRAVE_PATH = r\"\\AppData\\Local\\BraveSoftware\\Brave-Browser\\Application\\brave.exe\"\n",
    "\n",
    "RUTA_BASE = r\"\\assessment_of_wells_chile\\data\\DGA\"\n",
    "\n",
    "ARCHIVO_EXCEL_1 = r\"\\assessment_of_wells_chile\\data\\DGA\\DGA_consolidado_nacional\\outputs\\Analisis_Caudal_Anual_Prom_Negativo_CSVs\\Registros_Negativos.xlsx\"\n",
    "ARCHIVO_EXCEL_2 = r\"\\assessment_of_wells_chile\\data\\DGA\\DGA_consolidado_nacional\\outputs\\Registros_Fuera_Chile.xlsx\"\n",
    "\n",
    "RUTAS = {\n",
    "    'txt_salida': os.path.join(RUTA_BASE, \"output_revisados\", \"txt\"),\n",
    "    'excel_salida': os.path.join(RUTA_BASE, \"output_revisados\", \"excel\"),\n",
    "    'log_exitos': os.path.join(RUTA_BASE, \"output_revisados\", \"log_exitos.json\"),\n",
    "    'log_errores': os.path.join(RUTA_BASE, \"output_revisados\", \"log_errores.json\"),\n",
    "}\n",
    "\n",
    "for key in ['txt_salida', 'excel_salida']:\n",
    "    os.makedirs(RUTAS[key], exist_ok=True)\n",
    "os.makedirs(os.path.join(RUTA_BASE, \"output_revisados\"), exist_ok=True)\n",
    "\n",
    "NUM_WORKERS = 15\n",
    "BATCH_SAVE_SIZE = 50\n",
    "\n",
    "lock_log_exitos = threading.Lock()\n",
    "lock_log_errores = threading.Lock()\n",
    "lock_print = threading.Lock()\n",
    "\n",
    "def cargar_log(ruta_log):\n",
    "    if os.path.exists(ruta_log):\n",
    "        try:\n",
    "            with open(ruta_log, 'r', encoding='utf-8') as f:\n",
    "                return json.load(f)\n",
    "        except:\n",
    "            return {}\n",
    "    return {}\n",
    "\n",
    "def guardar_log(ruta_log, datos):\n",
    "    with open(ruta_log, 'w', encoding='utf-8') as f:\n",
    "        json.dump(datos, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "def registrar_exito(codigo, datos):\n",
    "    with lock_log_exitos:\n",
    "        log = cargar_log(RUTAS['log_exitos'])\n",
    "        log[codigo] = {'fecha': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'estado': 'exitoso'}\n",
    "        guardar_log(RUTAS['log_exitos'], log)\n",
    "\n",
    "def registrar_error(codigo, msg):\n",
    "    with lock_log_errores:\n",
    "        log = cargar_log(RUTAS['log_errores'])\n",
    "        log[codigo] = {'fecha': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'error': str(msg)}\n",
    "        guardar_log(RUTAS['log_errores'], log)\n",
    "\n",
    "def print_safe(msg):\n",
    "    with lock_print:\n",
    "        print(msg)\n",
    "\n",
    "def encontrar_columna_expediente(df):\n",
    "    patrones = ['c√≥digo de expediente', 'codigo de expediente', 'expediente', 'id', 'c√≥digo', 'codigo']\n",
    "    columnas_lower = {col.lower().strip(): col for col in df.columns}\n",
    "    \n",
    "    for patron in patrones:\n",
    "        if patron in columnas_lower: return columnas_lower[patron]\n",
    "        for col in columnas_lower:\n",
    "            if patron in col: return columnas_lower[col]\n",
    "            \n",
    "    return df.columns[0] if len(df.columns) > 0 else None\n",
    "\n",
    "def leer_codigos_excel(ruta_archivo, nombre_archivo=\"\"):\n",
    "    codigos = []\n",
    "    if not os.path.exists(ruta_archivo):\n",
    "        print(f\"   ‚ùå Archivo no existe: {ruta_archivo}\")\n",
    "        return codigos\n",
    "    \n",
    "    try:\n",
    "        print(f\"   üìÑ Leyendo: {nombre_archivo}\")\n",
    "        df = pd.read_excel(ruta_archivo)\n",
    "        col_exp = encontrar_columna_expediente(df)\n",
    "        if col_exp:\n",
    "            codigos_raw = df[col_exp].dropna().astype(str).str.strip().tolist()\n",
    "            codigos = [c for c in codigos_raw if c and c.lower() != 'nan' and len(c) > 2]\n",
    "            print(f\"      ‚úÖ Registros encontrados: {len(codigos)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"      ‚ùå Error leyendo: {e}\")\n",
    "    return codigos\n",
    "\n",
    "def crear_driver(headless=True):\n",
    "    options = Options()\n",
    "    options.binary_location = BRAVE_PATH\n",
    "    \n",
    "    if headless and not SHOW_BROWSER:\n",
    "        options.add_argument('--headless=new')\n",
    "    \n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--window-size=1920,1080')\n",
    "    options.add_argument('--log-level=3')\n",
    "    options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "    options.add_argument('--disable-infobars')\n",
    "    options.add_argument('--disable-extensions')\n",
    "    options.add_experimental_option('excludeSwitches', ['enable-automation'])\n",
    "    options.add_experimental_option('useAutomationExtension', False)\n",
    "    \n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "    return driver\n",
    "\n",
    "class DriverPool:\n",
    "    def __init__(self, size):\n",
    "        self.drivers = Queue()\n",
    "        print(f\"ü¶Å Iniciando {size} navegador(es) Brave...\")\n",
    "        for i in range(size):\n",
    "            try:\n",
    "                driver = crear_driver(headless=not SHOW_BROWSER)\n",
    "                self.drivers.put(driver)\n",
    "                print(f\"   ‚úÖ Brave #{i+1} listo\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Error iniciando Brave #{i+1}: {e}\")\n",
    "    \n",
    "    def get_driver(self): return self.drivers.get()\n",
    "    def return_driver(self, driver): self.drivers.put(driver)\n",
    "    def close_all(self):\n",
    "        print(\"üîí Cerrando navegadores...\")\n",
    "        while not self.drivers.empty():\n",
    "            try: self.drivers.get().quit()\n",
    "            except: pass\n",
    "\n",
    "def extraer_ubicacion_detalle_solicitud(driver, verbose=False):\n",
    "    ubicacion = {\n",
    "        'Latitud': None, 'Longitud': None, 'Datum': None,\n",
    "        'UTM norte': None, 'UTM este': None, 'Huso': None,\n",
    "        'Datum UTM': None, 'Unidad UTM': None\n",
    "    }\n",
    "    try:\n",
    "        tab_encontrado = False\n",
    "        try:\n",
    "            tabs = driver.find_elements(By.XPATH, \"//*[contains(text(), 'Detalle de la solicitud')]\")\n",
    "            for tab in tabs:\n",
    "                if tab.is_displayed():\n",
    "                    driver.execute_script(\"arguments[0].click();\", tab)\n",
    "                    tab_encontrado = True\n",
    "                    time.sleep(2)\n",
    "                    break\n",
    "        except: pass\n",
    "        \n",
    "        if not tab_encontrado:\n",
    "            try:\n",
    "                mat_tabs = driver.find_elements(By.CSS_SELECTOR, \".mat-tab-label, [role='tab']\")\n",
    "                for tab in mat_tabs:\n",
    "                    if 'solicitud' in tab.text.lower():\n",
    "                        driver.execute_script(\"arguments[0].click();\", tab)\n",
    "                        tab_encontrado = True\n",
    "                        time.sleep(2)\n",
    "                        break\n",
    "            except: pass\n",
    "        \n",
    "        if not tab_encontrado: return ubicacion\n",
    "        \n",
    "        puntos_encontrado = False\n",
    "        try:\n",
    "            elems = driver.find_elements(By.XPATH, \"//*[contains(text(), 'Puntos de captaci√≥n')]\")\n",
    "            for elem in elems:\n",
    "                if elem.is_displayed():\n",
    "                    driver.execute_script(\"arguments[0].click();\", elem)\n",
    "                    puntos_encontrado = True\n",
    "                    time.sleep(2)\n",
    "                    break\n",
    "        except: pass\n",
    "        \n",
    "        if not puntos_encontrado:\n",
    "            try:\n",
    "                btns = driver.find_elements(By.CSS_SELECTOR, \"button, mat-expansion-panel-header\")\n",
    "                for btn in btns:\n",
    "                    if 'capta' in btn.text.lower():\n",
    "                        driver.execute_script(\"arguments[0].click();\", btn)\n",
    "                        puntos_encontrado = True\n",
    "                        time.sleep(2)\n",
    "                        break\n",
    "            except: pass\n",
    "            \n",
    "        time.sleep(1)\n",
    "        tablas = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "        for tabla in tablas:\n",
    "            try:\n",
    "                if not tabla.is_displayed(): continue\n",
    "                headers = [th.text.strip().lower() for th in tabla.find_elements(By.TAG_NAME, \"th\")]\n",
    "                if any(h in ' '.join(headers) for h in ['utm', 'latitud', 'huso']):\n",
    "                    filas = tabla.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    for fila in filas:\n",
    "                        celdas = fila.find_elements(By.TAG_NAME, \"td\")\n",
    "                        if len(celdas) >= 6:\n",
    "                            vals = [c.text.strip() for c in celdas]\n",
    "                            headers_limpios = [h for h in headers if h]\n",
    "                            for idx, h in enumerate(headers_limpios):\n",
    "                                if idx < len(vals):\n",
    "                                    v = vals[idx]\n",
    "                                    if v and v != '-':\n",
    "                                        if 'latitud' in h: ubicacion['Latitud'] = v\n",
    "                                        elif 'longitud' in h: ubicacion['Longitud'] = v\n",
    "                                        elif 'datum' in h and 'utm' not in h: ubicacion['Datum'] = v\n",
    "                                        elif 'norte' in h: ubicacion['UTM norte'] = v\n",
    "                                        elif 'este' in h: ubicacion['UTM este'] = v\n",
    "                                        elif 'huso' in h: ubicacion['Huso'] = v\n",
    "                                        elif 'datum utm' in h: ubicacion['Datum UTM'] = v\n",
    "                                        elif 'unidad' in h: ubicacion['Unidad UTM'] = v\n",
    "                            \n",
    "                            if not any(ubicacion.values()):\n",
    "                                orden = ['Latitud', 'Longitud', 'Datum', 'UTM norte', 'UTM este', 'Huso', 'Datum UTM', 'Unidad UTM']\n",
    "                                for i, k in enumerate(orden):\n",
    "                                    if i < len(vals) and vals[i] != '-': ubicacion[k] = vals[i]\n",
    "                            break\n",
    "                    break\n",
    "            except: continue\n",
    "            \n",
    "        if not any(ubicacion.values()):\n",
    "            body = driver.find_element(By.TAG_NAME, \"body\").text\n",
    "            patrones = {\n",
    "                'UTM norte': r'UTM\\s*norte[:\\s]+([0-9.,]+)', 'UTM este': r'UTM\\s*este[:\\s]+([0-9.,]+)',\n",
    "                'Huso': r'Huso[:\\s]+(\\d+)', 'Latitud': r'Latitud[:\\s]+([-0-9.,]+)', 'Longitud': r'Longitud[:\\s]+([-0-9.,]+)'\n",
    "            }\n",
    "            for k, p in patrones.items():\n",
    "                m = re.search(p, body, re.IGNORECASE)\n",
    "                if m and not ubicacion.get(k): ubicacion[k] = m.group(1)\n",
    "\n",
    "    except: pass\n",
    "    return ubicacion\n",
    "\n",
    "def extraer_datos_dga(url, driver, verbose=False):\n",
    "    raw_id = url.split('/')[-1]\n",
    "    if raw_id.endswith('-1'):\n",
    "        expediente_limpio = raw_id[:-2]\n",
    "    else:\n",
    "        expediente_limpio = raw_id\n",
    "\n",
    "    resultado = {\n",
    "        'expediente': expediente_limpio,\n",
    "        'datos_solicitud': {}, 'caudal_concedido': {}, 'caudal_mensual': {}, 'ubicacion': {}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        resultado['ubicacion'] = extraer_ubicacion_detalle_solicitud(driver, verbose)\n",
    "        \n",
    "        try:\n",
    "            btns = driver.find_elements(By.XPATH, \"//*[contains(text(), 'Detalle de la resoluci√≥n')]\")\n",
    "            for btn in btns:\n",
    "                if btn.is_displayed():\n",
    "                    driver.execute_script(\"arguments[0].click();\", btn)\n",
    "                    time.sleep(2)\n",
    "                    break\n",
    "        except: pass\n",
    "\n",
    "        try:\n",
    "            btns_c = driver.find_elements(By.XPATH, \"//*[contains(text(), 'Caudal concedido')]\")\n",
    "            for btn in btns_c:\n",
    "                if btn.is_displayed():\n",
    "                    driver.execute_script(\"arguments[0].click();\", btn)\n",
    "                    time.sleep(1)\n",
    "                    break\n",
    "            \n",
    "            body = driver.find_element(By.TAG_NAME, \"body\").text\n",
    "            lines = body.split('\\n')\n",
    "            campos = {'Ejercicio del derecho': None, 'Tipo de distribuci√≥n': None, 'Caudal': None, 'Volumen': None}\n",
    "            \n",
    "            for i, l in enumerate(lines):\n",
    "                lc = l.strip()\n",
    "                if lc in campos:\n",
    "                    for j in range(i+1, min(i+5, len(lines))):\n",
    "                        val = lines[j].strip()\n",
    "                        if val and '---' not in val and val not in campos:\n",
    "                            campos[lc] = val\n",
    "                            break\n",
    "            resultado['caudal_concedido'] = campos\n",
    "\n",
    "            meses = ['Enero', 'Febrero', 'Marzo', 'Abril']\n",
    "            for t in driver.find_elements(By.TAG_NAME, \"table\"):\n",
    "                if not t.is_displayed(): continue\n",
    "                hs = [x.text.strip() for x in t.find_elements(By.TAG_NAME, \"th\")]\n",
    "                if any(m in hs for m in meses):\n",
    "                    rows = t.find_elements(By.TAG_NAME, \"tr\")\n",
    "                    for r in rows:\n",
    "                        cs = [x.text.strip() for x in r.find_elements(By.TAG_NAME, \"td\")]\n",
    "                        if len(cs) >= 12:\n",
    "                            m_names = ['Enero', 'Febrero', 'Marzo', 'Abril', 'Mayo', 'Junio', 'Julio', 'Agosto', 'Septiembre', 'Octubre', 'Noviembre', 'Diciembre', 'Unidad']\n",
    "                            for k, m_name in enumerate(m_names):\n",
    "                                if k < len(cs): resultado['caudal_mensual'][m_name] = cs[k]\n",
    "                            break\n",
    "        except: pass\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    return resultado\n",
    "\n",
    "def guardar_txt_formato_especifico(datos, ruta_carpeta):\n",
    "    nombre = f\"{datos['expediente']}.txt\"\n",
    "    ruta = os.path.join(ruta_carpeta, nombre)\n",
    "    cc = datos.get('caudal_concedido', {})\n",
    "    cm = datos.get('caudal_mensual', {})\n",
    "    ub = datos.get('ubicacion', {})\n",
    "    \n",
    "    with open(ruta, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=\"*60 + f\"\\nEXPEDIENTE: {datos['expediente']}\\n\" + \"=\"*60 + \"\\n\\n\")\n",
    "        f.write(\"PUNTOS DE CAPTACI√ìN (Ubicaci√≥n):\\n\" + \"-\"*40 + \"\\n\")\n",
    "        for k, v in ub.items(): f.write(f\"{k}: {v if v else '-'}\\n\")\n",
    "        f.write(\"\\nCAUDAL CONCEDIDO:\\n\" + \"-\"*40 + \"\\n\")\n",
    "        f.write(f\"Ejercicio del derecho: {cc.get('Ejercicio del derecho', 'Sin informaci√≥n')}\\n\")\n",
    "        f.write(f\"Tipo de distribuci√≥n: {cc.get('Tipo de distribuci√≥n', 'Sin informaci√≥n')}\\n\")\n",
    "        f.write(f\"Caudal: {cc.get('Caudal', 'Sin informaci√≥n')}\\n\")\n",
    "        f.write(f\"Volumen: {cc.get('Volumen', 'Sin informaci√≥n')}\\n\\n\")\n",
    "        f.write(\"CAUDAL MENSUAL:\\n\" + \"-\"*40 + \"\\n\")\n",
    "        ms = ['Enero', 'Febrero', 'Marzo', 'Abril', 'Mayo', 'Junio', 'Julio', 'Agosto', 'Septiembre', 'Octubre', 'Noviembre', 'Diciembre', 'Unidad']\n",
    "        f.write(\"\\t\".join(ms) + \"\\n\")\n",
    "        f.write(\"\\t\".join([cm.get(m, '0') for m in ms]) + \"\\n\")\n",
    "    return ruta\n",
    "\n",
    "def generar_fila_excel(datos):\n",
    "    cc = datos.get('caudal_concedido', {})\n",
    "    cm = datos.get('caudal_mensual', {})\n",
    "    ub = datos.get('ubicacion', {})\n",
    "    fila = {\n",
    "        'C√≥digo de Expediente': datos['expediente'],\n",
    "        'Latitud': ub.get('Latitud', ''), 'Longitud': ub.get('Longitud', ''),\n",
    "        'Datum': ub.get('Datum', ''), 'UTM_Norte': ub.get('UTM norte', ''),\n",
    "        'UTM_Este': ub.get('UTM este', ''), 'Huso': ub.get('Huso', ''),\n",
    "        'Datum_UTM': ub.get('Datum UTM', ''), 'Unidad_UTM': ub.get('Unidad UTM', ''),\n",
    "        'Ejercicio del derecho': cc.get('Ejercicio del derecho', ''),\n",
    "        'Tipo de distribuci√≥n': cc.get('Tipo de distribuci√≥n', ''),\n",
    "        'Caudal Aprobado': cc.get('Caudal', ''), 'Volumen Aprobado': cc.get('Volumen', '')\n",
    "    }\n",
    "    for m in ['Enero', 'Febrero', 'Marzo', 'Abril', 'Mayo', 'Junio', 'Julio', 'Agosto', 'Septiembre', 'Octubre', 'Noviembre', 'Diciembre', 'Unidad']:\n",
    "        fila[f'Caudal_{m}'] = cm.get(m, '')\n",
    "    return fila\n",
    "\n",
    "def procesar_codigo(codigo, driver_pool, verbose=False):\n",
    "    driver = driver_pool.get_driver()\n",
    "    codigo = str(codigo).strip()\n",
    "    url_code = codigo if codigo.endswith('-1') else f\"{codigo}-1\"\n",
    "    url = f\"https://consulta-expedientes.mop.gob.cl/detalle-expediente/{url_code}\"\n",
    "    \n",
    "    try:\n",
    "        datos = extraer_datos_dga(url, driver, verbose)\n",
    "        guardar_txt_formato_especifico(datos, RUTAS['txt_salida'])\n",
    "        fila = generar_fila_excel(datos)\n",
    "        registrar_exito(codigo, datos)\n",
    "        return {'status': 'ok', 'codigo': codigo, 'fila': fila}\n",
    "    except Exception as e:\n",
    "        registrar_error(codigo, str(e))\n",
    "        return {'status': 'error', 'codigo': codigo, 'msg': str(e)}\n",
    "    finally:\n",
    "        driver_pool.return_driver(driver)\n",
    "\n",
    "def main():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üöÄ DESCARGA MASIVA DGA (CORREGIDA)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if not os.path.exists(BRAVE_PATH):\n",
    "        print(f\"‚ùå Brave no encontrado en: {BRAVE_PATH}\")\n",
    "        return\n",
    "\n",
    "    codigos_procesar = set()\n",
    "    codigos_procesar.update(leer_codigos_excel(ARCHIVO_EXCEL_1, \"Registros Negativos\"))\n",
    "    codigos_procesar.update(leer_codigos_excel(ARCHIVO_EXCEL_2, \"Registros Fuera Chile\"))\n",
    "    \n",
    "    lista_codigos = sorted(list(codigos_procesar))\n",
    "    if not lista_codigos:\n",
    "        print(\"‚ùå No hay c√≥digos para procesar.\")\n",
    "        return\n",
    "\n",
    "    log = cargar_log(RUTAS['log_exitos'])\n",
    "    procesados = set(log.keys())\n",
    "    procesados.update([k.replace('-1', '') for k in log.keys()])\n",
    "    \n",
    "    pendientes = [c for c in lista_codigos if c not in procesados and f\"{c}-1\" not in log]\n",
    "    \n",
    "    print(f\"\\nüìã Total √∫nicos: {len(lista_codigos)}\")\n",
    "    print(f\"‚úÖ Ya procesados: {len(lista_codigos) - len(pendientes)}\")\n",
    "    print(f\"üöÄ Pendientes:    {len(pendientes)}\")\n",
    "    \n",
    "    if not pendientes:\n",
    "        print(\"\\n‚ú® ¬°Todo actualizado!\")\n",
    "        return\n",
    "\n",
    "    driver_pool = DriverPool(NUM_WORKERS)\n",
    "    resultados = []\n",
    "    \n",
    "    try:\n",
    "        with ThreadPoolExecutor(max_workers=NUM_WORKERS) as executor:\n",
    "            futures = {executor.submit(procesar_codigo, c, driver_pool): c for c in pendientes}\n",
    "            \n",
    "            for i, fut in enumerate(as_completed(futures)):\n",
    "                res = fut.result()\n",
    "                code = futures[fut]\n",
    "                \n",
    "                if res['status'] == 'ok':\n",
    "                    resultados.append(res['fila'])\n",
    "                    print_safe(f\"[{i+1}/{len(pendientes)}] ‚úÖ {code}\")\n",
    "                else:\n",
    "                    print_safe(f\"[{i+1}/{len(pendientes)}] ‚ùå {code} | {res.get('msg')[:50]}\")\n",
    "                \n",
    "                if len(resultados) % BATCH_SAVE_SIZE == 0:\n",
    "                    pd.DataFrame(resultados).to_excel(os.path.join(RUTAS['excel_salida'], \"parcial.xlsx\"), index=False)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n‚ö†Ô∏è Interrumpido por usuario\")\n",
    "    finally:\n",
    "        driver_pool.close_all()\n",
    "        \n",
    "    if resultados:\n",
    "        ts = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        ruta_fin = os.path.join(RUTAS['excel_salida'], f\"Consolidado_{ts}.xlsx\")\n",
    "        pd.DataFrame(resultados).to_excel(ruta_fin, index=False)\n",
    "        print(f\"\\nüíæ Guardado final: {ruta_fin}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c050221f-393d-4d53-8970-0b5ea340ad79",
   "metadata": {},
   "source": [
    "# Fixing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0762c36a-07f9-4ff3-87e9-ef60172fd9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "input_file = r\"\\assessment_of_wells_chile\\data\\DGA\\output_revisados\\excel\\Consolidado_20251216_222918.xlsx\"\n",
    "output_file = r\"\\assessment_of_wells_chile\\data\\DGA\\output_revisados\\excel\\Consolidado_txt_individual.xlsx\"\n",
    "\n",
    "print(f\"üìñ Leyendo archivo: {input_file} ...\")\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "def limpiar_formato_chileno_seguro(valor):\n",
    "    if pd.isna(valor) or str(valor).strip() == \"\":\n",
    "        return np.nan\n",
    "    \n",
    "    if isinstance(valor, (int, float)):\n",
    "        return float(valor)\n",
    "\n",
    "    valor_str = str(valor).strip()\n",
    "    \n",
    "    valor_clean = re.sub(r'[^\\d,.-]', '', valor_str)\n",
    "    \n",
    "    if not valor_clean:\n",
    "        return np.nan\n",
    "    \n",
    "    try:\n",
    "        valor_clean = valor_clean.replace('.', '')\n",
    "        \n",
    "        valor_clean = valor_clean.replace(',', '.')\n",
    "        \n",
    "        return float(valor_clean)\n",
    "        \n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "cols_a_convertir = [\n",
    "    'Latitud', \n",
    "    'Longitud', \n",
    "    'Datum', \n",
    "    'UTM_Norte', \n",
    "    'UTM_Este', \n",
    "    'Huso', \n",
    "    'Datum_UTM',\n",
    "    'Caudal Aprobado', \n",
    "    'Volumen Aprobado'\n",
    "]\n",
    "\n",
    "meses = ['Enero', 'Febrero', 'Marzo', 'Abril', 'Mayo', 'Junio', \n",
    "         'Julio', 'Agosto', 'Septiembre', 'Octubre', 'Noviembre', 'Diciembre']\n",
    "for mes in meses:\n",
    "    col_name = f'Caudal_{mes}'\n",
    "    if col_name in df.columns:\n",
    "        cols_a_convertir.append(col_name)\n",
    "\n",
    "print(\"üîÑ Aplicando correcci√≥n de formato Chileno (segura)...\")\n",
    "\n",
    "for col in cols_a_convertir:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].apply(limpiar_formato_chileno_seguro)\n",
    "\n",
    "print(f\"üíæ Guardando archivo corregido en: {output_file}\")\n",
    "df.to_excel(output_file, index=False)\n",
    "print(\"‚úÖ ¬°Proceso completado! Los n√∫meros enteros (19, 188200) se han mantenido intactos.\")\n",
    "\n",
    "check_cols = [c for c in ['UTM_Norte', 'Huso', 'Caudal_Enero'] if c in df.columns]\n",
    "df[check_cols].head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-water]",
   "language": "python",
   "name": "conda-env-.conda-water-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
