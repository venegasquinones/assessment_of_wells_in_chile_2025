{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d37e37-ba1e-425a-86b1-4d28483d24d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import warnings\n",
    "import os\n",
    "import gc\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import json\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "try:\n",
    "    from prophet import Prophet\n",
    "    PROPHET_AVAILABLE = True\n",
    "except ImportError:\n",
    "    try:\n",
    "        from fbprophet import Prophet\n",
    "        PROPHET_AVAILABLE = True\n",
    "    except ImportError:\n",
    "        PROPHET_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import pymannkendall as mk\n",
    "    MK_AVAILABLE = True\n",
    "except ImportError:\n",
    "    MK_AVAILABLE = False\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "MIN_RECENT_YEAR = 2020\n",
    "MIN_RECORDS = 24\n",
    "MIN_YEARS_SPAN = 5\n",
    "PREDICTION_YEAR = 2030\n",
    "\n",
    "ARIMA_MAX_ORDER = 3\n",
    "LSTM_LOOKBACK = 12\n",
    "LSTM_EPOCHS = 150\n",
    "LSTM_BATCH_SIZE = 16\n",
    "\n",
    "SIGNIFICANCE_LEVEL = 0.05\n",
    "CONFIDENCE_LEVEL = 0.95\n",
    "\n",
    "CRS_WORKING = \"EPSG:4326\"\n",
    "\n",
    "output_folder = r\"\\assessment_of_wells_chile\\data\\DGA\\Trend_Analysis_Predictions_Complete_v2\"\n",
    "\n",
    "for folder in ['Excel', 'Figures', 'Text_Output', 'Models', 'Individual_Wells']:\n",
    "    path = os.path.join(output_folder, folder)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "log_file_path = os.path.join(output_folder, 'Text_Output', 'process_log_realtime.txt')\n",
    "checkpoint_file = os.path.join(output_folder, 'checkpoint_well_results.csv')\n",
    "progress_file = os.path.join(output_folder, 'progress_tracker.json')\n",
    "\n",
    "def load_progress():\n",
    "    processed_wells = set()\n",
    "    well_results = []\n",
    "    if os.path.exists(progress_file):\n",
    "        try:\n",
    "            with open(progress_file, 'r') as f:\n",
    "                progress_data = json.load(f)\n",
    "                processed_wells = set(progress_data.get('processed_wells', []))\n",
    "        except:\n",
    "            processed_wells = set()\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        try:\n",
    "            df_checkpoint = pd.read_csv(checkpoint_file)\n",
    "            well_results = df_checkpoint.to_dict('records')\n",
    "            for result in well_results:\n",
    "                processed_wells.add(result['Station_Code'])\n",
    "        except:\n",
    "            well_results = []\n",
    "    return processed_wells, well_results\n",
    "\n",
    "def save_progress(processed_wells, well_results, current_idx, total_wells):\n",
    "    try:\n",
    "        progress_data = {\n",
    "            'processed_wells': list(processed_wells),\n",
    "            'last_index': current_idx,\n",
    "            'total_wells': total_wells,\n",
    "            'last_update': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "        with open(progress_file, 'w') as f:\n",
    "            json.dump(progress_data, f)\n",
    "        if well_results:\n",
    "            pd.DataFrame(well_results).to_csv(checkpoint_file, index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving progress: {e}\")\n",
    "\n",
    "def write_output(text):\n",
    "    print(text)\n",
    "    try:\n",
    "        with open(log_file_path, 'a', encoding='utf-8') as f:\n",
    "            f.write(text + \"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing to log file: {e}\")\n",
    "\n",
    "if not os.path.exists(log_file_path):\n",
    "    with open(log_file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"STARTING PROCESS AT: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "else:\n",
    "    with open(log_file_path, 'a', encoding='utf-8') as f:\n",
    "        f.write(f\"\\nRESUMING PROCESS AT: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "\n",
    "path_piezometric = r\"\\assessment_of_wells_chile\\data\\DGA_aguas_subterraneas_12_2025\\niveles_estaticos_pozos_historico\\output\\niveles_estaticos_todos.shp\"\n",
    "gdb_path = r\"\\assessment_of_wells_chile\\arcgis\\assessment_of_wells_chile\\Default.gdb\"\n",
    "shp_cuencas = r\"\\assessment_of_wells_chile\\data\\Basins\\Cuencas_BNA\\Cuencas_BNA.shp\"\n",
    "shp_shac = r\"\\assessment_of_wells_chile\\data\\Aquifers\\INV_ACUIFEROS_SHAC_202302\\INV_ACUIFEROS_SHAC.shp\"\n",
    "\n",
    "LAYER_CONFIG = {\n",
    "    'municipalities': {'path': gdb_path, 'layer': 'CHL_Municipalities', 'native_crs': 'EPSG:3857'},\n",
    "    'regions': {'path': gdb_path, 'layer': 'CHL_Regions', 'native_crs': 'EPSG:3857'},\n",
    "    'cuencas': {'path': shp_cuencas, 'layer': None, 'native_crs': 'EPSG:32719'},\n",
    "    'shac': {'path': shp_shac, 'layer': None, 'native_crs': 'EPSG:32719'},\n",
    "    'piezometric': {'path': path_piezometric, 'layer': None, 'native_crs': 'EPSG:4326'}\n",
    "}\n",
    "\n",
    "piez_codigo_col = 'COD_EST'\n",
    "piez_nombre_col = 'NOM_EST'\n",
    "piez_fecha_col = 'FECHA_US'\n",
    "piez_nivel_col = 'NIVEL'\n",
    "piez_lat_col = 'LAT_WGS84'\n",
    "piez_lon_col = 'LON_WGS84'\n",
    "\n",
    "cuenca_col = 'NOM_CUEN'\n",
    "shac_col = 'SHAC'\n",
    "comuna_col = 'NAME'\n",
    "region_col = 'NAME'\n",
    "\n",
    "def normalize_id_with_padding(id_value, target_length=8):\n",
    "    id_str = str(id_value).strip()\n",
    "    if '.' in id_str:\n",
    "        id_str = id_str.split('.')[0]\n",
    "    try:\n",
    "        if 'e' in id_str.lower() or 'E' in id_str:\n",
    "            id_str = str(int(float(id_str)))\n",
    "    except:\n",
    "        pass\n",
    "    if id_str.isdigit():\n",
    "        id_str = id_str.zfill(target_length)\n",
    "    return id_str\n",
    "\n",
    "def nash_sutcliffe_efficiency(observed, predicted):\n",
    "    observed = np.array(observed).flatten()\n",
    "    predicted = np.array(predicted).flatten()\n",
    "    mask = ~(np.isnan(observed) | np.isnan(predicted))\n",
    "    observed = observed[mask]\n",
    "    predicted = predicted[mask]\n",
    "    if len(observed) < 2:\n",
    "        return np.nan\n",
    "    numerator = np.sum((observed - predicted) ** 2)\n",
    "    denominator = np.sum((observed - np.mean(observed)) ** 2)\n",
    "    if denominator == 0:\n",
    "        return np.nan\n",
    "    return 1 - (numerator / denominator)\n",
    "\n",
    "def parse_date(date_val):\n",
    "    if pd.isna(date_val):\n",
    "        return pd.NaT\n",
    "    date_str = str(date_val).strip()\n",
    "    formats = ['%d/%m/%Y', '%m/%d/%Y', '%Y-%m-%d', '%d-%m-%Y', '%Y/%m/%d',\n",
    "               '%d/%m/%y', '%m/%d/%y', '%Y%m%d']\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            return pd.to_datetime(date_str, format=fmt)\n",
    "        except:\n",
    "            continue\n",
    "    try:\n",
    "        return pd.to_datetime(date_str, dayfirst=True)\n",
    "    except:\n",
    "        try:\n",
    "            return pd.to_datetime(date_str)\n",
    "        except:\n",
    "            return pd.NaT\n",
    "\n",
    "def calculate_linear_trend(dates, values):\n",
    "    try:\n",
    "        if len(dates) < 5 or len(values) < 5:\n",
    "            return None\n",
    "        years = (dates - dates.min()).dt.days / 365.25\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(years.values, values.values)\n",
    "        n = len(years)\n",
    "        t_critical = stats.t.ppf((1 + CONFIDENCE_LEVEL) / 2, n - 2)\n",
    "        slope_ci_lower = slope - t_critical * std_err\n",
    "        slope_ci_upper = slope + t_critical * std_err\n",
    "        years_to_prediction = PREDICTION_YEAR - dates.max().year\n",
    "        predicted_change = slope * years_to_prediction\n",
    "        return {\n",
    "            'method': 'Linear_OLS',\n",
    "            'slope_m_per_year': slope,\n",
    "            'intercept': intercept,\n",
    "            'r_squared': r_value**2,\n",
    "            'p_value': p_value,\n",
    "            'std_error': std_err,\n",
    "            'slope_ci_lower': slope_ci_lower,\n",
    "            'slope_ci_upper': slope_ci_upper,\n",
    "            'significant': p_value < SIGNIFICANCE_LEVEL,\n",
    "            'trend_direction': 'Decreasing' if slope > 0 else 'Increasing',\n",
    "            'predicted_change': predicted_change\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def mann_kendall_test(values):\n",
    "    try:\n",
    "        if not MK_AVAILABLE:\n",
    "            return None\n",
    "        values_clean = values.dropna()\n",
    "        if len(values_clean) < 10:\n",
    "            return None\n",
    "        result = mk.original_test(values_clean)\n",
    "        return {\n",
    "            'method': 'Mann_Kendall',\n",
    "            'trend': result.trend,\n",
    "            'h': result.h,\n",
    "            'p_value': result.p,\n",
    "            'z_score': result.z,\n",
    "            'tau': result.Tau,\n",
    "            's': result.s,\n",
    "            'significant': result.h,\n",
    "            'trend_direction': 'Decreasing' if result.slope > 0 else 'Increasing' if result.slope < 0 else 'No Trend'\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def sens_slope_estimator(dates, values):\n",
    "    try:\n",
    "        if not MK_AVAILABLE:\n",
    "            values_clean = values.dropna()\n",
    "            dates_clean = dates[values.notna()]\n",
    "            if len(values_clean) < 5:\n",
    "                return None\n",
    "            years = (dates_clean - dates_clean.min()).dt.days / 365.25\n",
    "            slopes = []\n",
    "            for i in range(len(years)):\n",
    "                for j in range(i + 1, len(years)):\n",
    "                    if years.iloc[j] != years.iloc[i]:\n",
    "                        slope = (values_clean.iloc[j] - values_clean.iloc[i]) / (years.iloc[j] - years.iloc[i])\n",
    "                        slopes.append(slope)\n",
    "            if len(slopes) == 0:\n",
    "                return None\n",
    "            sen_slope = np.median(slopes)\n",
    "        else:\n",
    "            result = mk.original_test(values.dropna())\n",
    "            sen_slope = result.slope\n",
    "        years_to_prediction = PREDICTION_YEAR - dates.max().year\n",
    "        predicted_change = sen_slope * years_to_prediction\n",
    "        return {\n",
    "            'method': 'Sens_Slope',\n",
    "            'slope_m_per_year': sen_slope,\n",
    "            'trend_direction': 'Decreasing' if sen_slope > 0 else 'Increasing',\n",
    "            'predicted_change': predicted_change\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def seasonal_kendall_test(values, period=12):\n",
    "    try:\n",
    "        if not MK_AVAILABLE:\n",
    "            return None\n",
    "        values_clean = values.dropna()\n",
    "        if len(values_clean) < period * 2:\n",
    "            return None\n",
    "        result = mk.seasonal_test(values_clean, period=period)\n",
    "        return {\n",
    "            'method': 'Seasonal_Kendall',\n",
    "            'trend': result.trend,\n",
    "            'h': result.h,\n",
    "            'p_value': result.p,\n",
    "            'z_score': result.z,\n",
    "            'significant': result.h,\n",
    "            'slope': result.slope,\n",
    "            'trend_direction': 'Decreasing' if result.slope > 0 else 'Increasing' if result.slope < 0 else 'No Trend'\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def fit_arima_model(series, forecast_periods=60):\n",
    "    try:\n",
    "        series = series.dropna().astype(float)\n",
    "        if len(series) < 24:\n",
    "            return None\n",
    "        try:\n",
    "            adf_result = adfuller(series, autolag='AIC')\n",
    "            d = 0 if adf_result[1] < 0.05 else 1\n",
    "        except:\n",
    "            d = 1\n",
    "        best_aic = np.inf\n",
    "        best_model = None\n",
    "        best_order = None\n",
    "        orders_to_try = [\n",
    "            (2, d, 2), (1, d, 1), (2, d, 1), (1, d, 2),\n",
    "            (0, d, 1), (1, d, 0), (0, d, 2), (2, d, 0),\n",
    "            (3, d, 1), (1, d, 3), (3, d, 2), (2, d, 3)\n",
    "        ]\n",
    "        for order in orders_to_try:\n",
    "            try:\n",
    "                model = ARIMA(series, order=order)\n",
    "                fitted = model.fit()\n",
    "                if fitted.aic < best_aic:\n",
    "                    best_aic = fitted.aic\n",
    "                    best_model = fitted\n",
    "                    best_order = order\n",
    "            except:\n",
    "                continue\n",
    "        if best_model is None:\n",
    "            return None\n",
    "        forecast_result = best_model.get_forecast(steps=forecast_periods)\n",
    "        forecast = forecast_result.predicted_mean\n",
    "        conf_int = forecast_result.conf_int(alpha=1-CONFIDENCE_LEVEL)\n",
    "        in_sample_pred = best_model.fittedvalues\n",
    "        rmse = np.sqrt(mean_squared_error(series[1:], in_sample_pred[1:]))\n",
    "        mae = mean_absolute_error(series[1:], in_sample_pred[1:])\n",
    "        nse = nash_sutcliffe_efficiency(series[1:], in_sample_pred[1:])\n",
    "        return {\n",
    "            'model': best_model,\n",
    "            'forecast': forecast,\n",
    "            'forecast_lower': conf_int.iloc[:, 0],\n",
    "            'forecast_upper': conf_int.iloc[:, 1],\n",
    "            'aic': best_aic,\n",
    "            'bic': best_model.bic,\n",
    "            'order': best_order,\n",
    "            'residuals': best_model.resid,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'nse': nse\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def fit_holtwinters_model(series, forecast_periods=60, seasonal_periods=12):\n",
    "    try:\n",
    "        series = series.dropna().astype(float)\n",
    "        if len(series) < seasonal_periods * 2:\n",
    "            try:\n",
    "                model = ExponentialSmoothing(\n",
    "                    series,\n",
    "                    trend='add',\n",
    "                    seasonal=None\n",
    "                )\n",
    "                fitted = model.fit()\n",
    "            except:\n",
    "                return None\n",
    "        else:\n",
    "            try:\n",
    "                model = ExponentialSmoothing(\n",
    "                    series,\n",
    "                    trend='add',\n",
    "                    seasonal='add',\n",
    "                    seasonal_periods=seasonal_periods\n",
    "                )\n",
    "                fitted = model.fit()\n",
    "            except:\n",
    "                try:\n",
    "                    model = ExponentialSmoothing(\n",
    "                        series,\n",
    "                        trend='add',\n",
    "                        seasonal=None\n",
    "                    )\n",
    "                    fitted = model.fit()\n",
    "                except:\n",
    "                    return None\n",
    "        forecast = fitted.forecast(steps=forecast_periods)\n",
    "        residual_std = fitted.resid.std()\n",
    "        forecast_lower = forecast - 1.96 * residual_std\n",
    "        forecast_upper = forecast + 1.96 * residual_std\n",
    "        in_sample_pred = fitted.fittedvalues\n",
    "        rmse = np.sqrt(mean_squared_error(series, in_sample_pred))\n",
    "        mae = mean_absolute_error(series, in_sample_pred)\n",
    "        nse = nash_sutcliffe_efficiency(series, in_sample_pred)\n",
    "        return {\n",
    "            'model': fitted,\n",
    "            'forecast': forecast,\n",
    "            'forecast_lower': forecast_lower,\n",
    "            'forecast_upper': forecast_upper,\n",
    "            'aic': fitted.aic if hasattr(fitted, 'aic') else np.nan,\n",
    "            'residual_std': residual_std,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'nse': nse\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def fit_prophet_model(df_series, forecast_periods=60):\n",
    "    try:\n",
    "        if not PROPHET_AVAILABLE:\n",
    "            return None\n",
    "        df_prophet = df_series.reset_index()\n",
    "        df_prophet.columns = ['ds', 'y']\n",
    "        df_prophet['ds'] = pd.to_datetime(df_prophet['ds'])\n",
    "        df_prophet = df_prophet.dropna()\n",
    "        if len(df_prophet) < 24:\n",
    "            return None\n",
    "        model = Prophet(\n",
    "            yearly_seasonality=True,\n",
    "            weekly_seasonality=False,\n",
    "            daily_seasonality=False,\n",
    "            seasonality_mode='multiplicative',\n",
    "            interval_width=CONFIDENCE_LEVEL\n",
    "        )\n",
    "        import logging\n",
    "        logging.getLogger('prophet').setLevel(logging.WARNING)\n",
    "        logging.getLogger('cmdstanpy').setLevel(logging.WARNING)\n",
    "        model.fit(df_prophet)\n",
    "        last_date = df_prophet['ds'].max()\n",
    "        future_dates = pd.date_range(start=last_date, periods=forecast_periods+1, freq='M')[1:]\n",
    "        future = pd.DataFrame({'ds': future_dates})\n",
    "        forecast = model.predict(future)\n",
    "        in_sample = model.predict(df_prophet[['ds']])\n",
    "        rmse = np.sqrt(mean_squared_error(df_prophet['y'], in_sample['yhat']))\n",
    "        mae = mean_absolute_error(df_prophet['y'], in_sample['yhat'])\n",
    "        nse = nash_sutcliffe_efficiency(df_prophet['y'], in_sample['yhat'])\n",
    "        return {\n",
    "            'model': model,\n",
    "            'forecast': forecast['yhat'].values,\n",
    "            'forecast_lower': forecast['yhat_lower'].values,\n",
    "            'forecast_upper': forecast['yhat_upper'].values,\n",
    "            'trend': forecast['trend'].values,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'nse': nse\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def fit_lstm_model(series, forecast_periods=60, lookback=12):\n",
    "    try:\n",
    "        series = series.dropna().astype(float)\n",
    "        if len(series) < lookback + 36:\n",
    "            return None\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled_data = scaler.fit_transform(series.values.reshape(-1, 1))\n",
    "        X, y = [], []\n",
    "        for i in range(lookback, len(scaled_data)):\n",
    "            X.append(scaled_data[i-lookback:i, 0])\n",
    "            y.append(scaled_data[i, 0])\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "        if len(X) < 20:\n",
    "            return None\n",
    "        train_size = int(len(X) * 0.8)\n",
    "        X_train, X_test = X[:train_size], X[train_size:]\n",
    "        y_train, y_test = y[:train_size], y[train_size:]\n",
    "        model = Sequential([\n",
    "            LSTM(64, return_sequences=True, input_shape=(lookback, 1)),\n",
    "            Dropout(0.2),\n",
    "            LSTM(32, return_sequences=False),\n",
    "            Dropout(0.2),\n",
    "            Dense(16, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.001),\n",
    "            loss='mean_squared_error'\n",
    "        )\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            batch_size=LSTM_BATCH_SIZE,\n",
    "            epochs=LSTM_EPOCHS,\n",
    "            validation_split=0.15,\n",
    "            callbacks=[early_stop, reduce_lr],\n",
    "            verbose=0\n",
    "        )\n",
    "        if len(X_test) > 0:\n",
    "            predictions_test = model.predict(X_test, verbose=0)\n",
    "            predictions_test = scaler.inverse_transform(predictions_test)\n",
    "            y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "            rmse = np.sqrt(mean_squared_error(y_test_inv, predictions_test))\n",
    "            mae = mean_absolute_error(y_test_inv, predictions_test)\n",
    "            nse = nash_sutcliffe_efficiency(y_test_inv, predictions_test)\n",
    "        else:\n",
    "            rmse = np.nan\n",
    "            mae = np.nan\n",
    "            nse = np.nan\n",
    "        last_sequence = scaled_data[-lookback:]\n",
    "        forecasts = []\n",
    "        current_seq = last_sequence.flatten()\n",
    "        for _ in range(forecast_periods):\n",
    "            input_seq = current_seq[-lookback:].reshape(1, lookback, 1)\n",
    "            pred = model.predict(input_seq, verbose=0)[0, 0]\n",
    "            forecasts.append(pred)\n",
    "            current_seq = np.append(current_seq, pred)\n",
    "        forecasts = scaler.inverse_transform(np.array(forecasts).reshape(-1, 1)).flatten()\n",
    "        forecast_lower = forecasts - 1.96 * rmse if not np.isnan(rmse) else forecasts\n",
    "        forecast_upper = forecasts + 1.96 * rmse if not np.isnan(rmse) else forecasts\n",
    "        return {\n",
    "            'model': model,\n",
    "            'forecast': forecasts,\n",
    "            'forecast_lower': forecast_lower,\n",
    "            'forecast_upper': forecast_upper,\n",
    "            'scaler': scaler,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'nse': nse\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def fit_bilstm_model(series, forecast_periods=60, lookback=12):\n",
    "    try:\n",
    "        series = series.dropna().astype(float)\n",
    "        if len(series) < lookback + 48:\n",
    "            return None\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaled_data = scaler.fit_transform(series.values.reshape(-1, 1))\n",
    "        X, y = [], []\n",
    "        for i in range(lookback, len(scaled_data)):\n",
    "            X.append(scaled_data[i-lookback:i, 0])\n",
    "            y.append(scaled_data[i, 0])\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "        if len(X) < 30:\n",
    "            return None\n",
    "        train_size = int(len(X) * 0.8)\n",
    "        X_train, X_test = X[:train_size], X[train_size:]\n",
    "        y_train, y_test = y[:train_size], y[train_size:]\n",
    "        model = Sequential([\n",
    "            Bidirectional(LSTM(64, return_sequences=True), input_shape=(lookback, 1)),\n",
    "            Dropout(0.3),\n",
    "            Bidirectional(LSTM(32)),\n",
    "            Dropout(0.3),\n",
    "            Dense(16, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            batch_size=LSTM_BATCH_SIZE,\n",
    "            epochs=LSTM_EPOCHS,\n",
    "            validation_split=0.15,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=0\n",
    "        )\n",
    "        if len(X_test) > 0:\n",
    "            pred_test = model.predict(X_test, verbose=0)\n",
    "            pred_test = scaler.inverse_transform(pred_test)\n",
    "            y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "            rmse = np.sqrt(mean_squared_error(y_test_inv, pred_test))\n",
    "            mae = mean_absolute_error(y_test_inv, pred_test)\n",
    "            nse = nash_sutcliffe_efficiency(y_test_inv, pred_test)\n",
    "        else:\n",
    "            rmse = np.nan\n",
    "            mae = np.nan\n",
    "            nse = np.nan\n",
    "        last_seq = scaled_data[-lookback:]\n",
    "        forecasts = []\n",
    "        current_seq = last_seq.flatten()\n",
    "        for _ in range(forecast_periods):\n",
    "            input_seq = current_seq[-lookback:].reshape(1, lookback, 1)\n",
    "            pred = model.predict(input_seq, verbose=0)[0, 0]\n",
    "            forecasts.append(pred)\n",
    "            current_seq = np.append(current_seq, pred)\n",
    "        forecasts = scaler.inverse_transform(np.array(forecasts).reshape(-1, 1)).flatten()\n",
    "        forecast_lower = forecasts - 1.96 * rmse if not np.isnan(rmse) else forecasts\n",
    "        forecast_upper = forecasts + 1.96 * rmse if not np.isnan(rmse) else forecasts\n",
    "        return {\n",
    "            'model': model,\n",
    "            'forecast': forecasts,\n",
    "            'forecast_lower': forecast_lower,\n",
    "            'forecast_upper': forecast_upper,\n",
    "            'scaler': scaler,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'nse': nse\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def calculate_ensemble_prediction(predictions_dict):\n",
    "    try:\n",
    "        valid_predictions = {}\n",
    "        valid_uncertainties = {}\n",
    "        for model_name, pred_data in predictions_dict.items():\n",
    "            if pred_data is not None and 'forecast' in pred_data:\n",
    "                forecast = pred_data['forecast']\n",
    "                if isinstance(forecast, (pd.Series, np.ndarray)):\n",
    "                    final_value = forecast[-1] if len(forecast) > 0 else np.nan\n",
    "                else:\n",
    "                    final_value = forecast\n",
    "                if not np.isnan(final_value):\n",
    "                    valid_predictions[model_name] = final_value\n",
    "                    if 'forecast_lower' in pred_data and 'forecast_upper' in pred_data:\n",
    "                        lower = pred_data['forecast_lower'][-1] if hasattr(pred_data['forecast_lower'], '__len__') else pred_data['forecast_lower']\n",
    "                        upper = pred_data['forecast_upper'][-1] if hasattr(pred_data['forecast_upper'], '__len__') else pred_data['forecast_upper']\n",
    "                        valid_uncertainties[model_name] = (upper - lower) / 2\n",
    "        if len(valid_predictions) == 0:\n",
    "            return None\n",
    "        values = list(valid_predictions.values())\n",
    "        ensemble_mean = np.mean(values)\n",
    "        ensemble_std = np.std(values) if len(values) > 1 else 0\n",
    "        if valid_uncertainties:\n",
    "            combined_uncertainty = np.sqrt(np.sum([u**2 for u in valid_uncertainties.values()])) / len(valid_uncertainties)\n",
    "            total_uncertainty = np.sqrt(ensemble_std**2 + combined_uncertainty**2)\n",
    "        else:\n",
    "            total_uncertainty = ensemble_std\n",
    "        return {\n",
    "            'ensemble_mean': ensemble_mean,\n",
    "            'ensemble_std': ensemble_std,\n",
    "            'ensemble_lower': ensemble_mean - 1.96 * total_uncertainty,\n",
    "            'ensemble_upper': ensemble_mean + 1.96 * total_uncertainty,\n",
    "            'n_models': len(valid_predictions),\n",
    "            'model_predictions': valid_predictions\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def load_layer_with_crs(path, layer_name=None, expected_crs=None, target_crs=CRS_WORKING):\n",
    "    if layer_name:\n",
    "        gdf = gpd.read_file(path, layer=layer_name)\n",
    "        source_name = f\"{path} (layer: {layer_name})\"\n",
    "    else:\n",
    "        gdf = gpd.read_file(path)\n",
    "        source_name = path\n",
    "    if gdf.crs is None:\n",
    "        if expected_crs:\n",
    "            write_output(f\"   Setting CRS for {os.path.basename(source_name)}: {expected_crs}\")\n",
    "            gdf = gdf.set_crs(expected_crs)\n",
    "        else:\n",
    "            write_output(f\"   WARNING: No CRS detected for {os.path.basename(source_name)}\")\n",
    "            return gdf\n",
    "    else:\n",
    "        detected_crs = str(gdf.crs)\n",
    "        if expected_crs and detected_crs != expected_crs:\n",
    "            write_output(f\"   NOTE: {os.path.basename(source_name)} has CRS {detected_crs}, expected {expected_crs}\")\n",
    "    original_crs = gdf.crs\n",
    "    if gdf.crs != target_crs:\n",
    "        gdf = gdf.to_crs(target_crs)\n",
    "        write_output(f\"   Reprojected {os.path.basename(source_name)}: {original_crs} -> {target_crs}\")\n",
    "    else:\n",
    "        write_output(f\"   {os.path.basename(source_name)} already in {target_crs}\")\n",
    "    return gdf\n",
    "\n",
    "write_output(\"=\"*80)\n",
    "write_output(\"GROUNDWATER LEVEL TREND ANALYSIS AND PREDICTION\")\n",
    "write_output(\"=\"*80)\n",
    "write_output(f\"Analysis started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "processed_wells, well_results = load_progress()\n",
    "if len(processed_wells) > 0:\n",
    "    write_output(f\"\\nRESUMING FROM PREVIOUS RUN: {len(processed_wells)} wells already processed\")\n",
    "\n",
    "write_output(\"\\n\" + \"=\"*80)\n",
    "write_output(\"SECTION 1: DATA LOADING AND PREPROCESSING\")\n",
    "write_output(\"=\"*80)\n",
    "\n",
    "write_output(f\"\\nWorking CRS for all spatial operations: {CRS_WORKING}\")\n",
    "\n",
    "write_output(\"\\nLoading piezometric data...\")\n",
    "try:\n",
    "    gdf_piezometric = gpd.read_file(path_piezometric)\n",
    "    write_output(f\"   Loaded {len(gdf_piezometric):,} piezometric records\")\n",
    "    write_output(f\"   Detected CRS: {gdf_piezometric.crs}\")\n",
    "    if gdf_piezometric.crs is None:\n",
    "        sample_x = gdf_piezometric.geometry.iloc[0].x if len(gdf_piezometric) > 0 else 0\n",
    "        if -180 <= sample_x <= 180:\n",
    "            write_output(\"   No CRS detected, assuming EPSG:4326 (WGS84) based on coordinate values\")\n",
    "            gdf_piezometric = gdf_piezometric.set_crs(\"EPSG:4326\")\n",
    "        else:\n",
    "            write_output(\"   No CRS detected, assuming EPSG:32719 (UTM 19S) based on coordinate values\")\n",
    "            gdf_piezometric = gdf_piezometric.set_crs(\"EPSG:32719\")\n",
    "    if gdf_piezometric.crs != CRS_WORKING:\n",
    "        original_crs = gdf_piezometric.crs\n",
    "        gdf_piezometric = gdf_piezometric.to_crs(CRS_WORKING)\n",
    "        write_output(f\"   Reprojected piezometric data: {original_crs} -> {CRS_WORKING}\")\n",
    "except Exception as e:\n",
    "    write_output(f\"   Error loading piezometric data: {e}\")\n",
    "    raise\n",
    "\n",
    "write_output(\"\\nLoading spatial reference layers...\")\n",
    "\n",
    "try:\n",
    "    gdf_cuencas = load_layer_with_crs(\n",
    "        shp_cuencas,\n",
    "        layer_name=None,\n",
    "        expected_crs='EPSG:32719',\n",
    "        target_crs=CRS_WORKING\n",
    "    )\n",
    "    write_output(f\"   Cuencas: {len(gdf_cuencas)} features\")\n",
    "    gdf_shac = load_layer_with_crs(\n",
    "        shp_shac,\n",
    "        layer_name=None,\n",
    "        expected_crs='EPSG:32719',\n",
    "        target_crs=CRS_WORKING\n",
    "    )\n",
    "    write_output(f\"   SHAC: {len(gdf_shac)} features\")\n",
    "    gdf_comunas = load_layer_with_crs(\n",
    "        gdb_path,\n",
    "        layer_name='CHL_Municipalities',\n",
    "        expected_crs='EPSG:3857',\n",
    "        target_crs=CRS_WORKING\n",
    "    )\n",
    "    write_output(f\"   Municipalities: {len(gdf_comunas)} features\")\n",
    "    gdf_regiones = load_layer_with_crs(\n",
    "        gdb_path,\n",
    "        layer_name='CHL_Regions',\n",
    "        expected_crs='EPSG:3857',\n",
    "        target_crs=CRS_WORKING\n",
    "    )\n",
    "    write_output(f\"   Regions: {len(gdf_regiones)} features\")\n",
    "    write_output(\"\\nAll spatial layers loaded successfully\")\n",
    "except Exception as e:\n",
    "    write_output(f\"Error loading spatial layers: {e}\")\n",
    "    raise\n",
    "\n",
    "write_output(\"\\nCRS Verification:\")\n",
    "all_layers = {\n",
    "    'Piezometric': gdf_piezometric,\n",
    "    'Cuencas': gdf_cuencas,\n",
    "    'SHAC': gdf_shac,\n",
    "    'Municipalities': gdf_comunas,\n",
    "    'Regions': gdf_regiones\n",
    "}\n",
    "\n",
    "crs_check_passed = True\n",
    "for name, gdf in all_layers.items():\n",
    "    crs_status = \"OK\" if str(gdf.crs) == CRS_WORKING else \"X\"\n",
    "    write_output(f\"   {crs_status} {name}: {gdf.crs}\")\n",
    "    if str(gdf.crs) != CRS_WORKING:\n",
    "        crs_check_passed = False\n",
    "\n",
    "if crs_check_passed:\n",
    "    write_output(\"\\n   All layers are in the same CRS - spatial joins will be accurate\")\n",
    "else:\n",
    "    write_output(\"\\n   WARNING: CRS mismatch detected - spatial joins may be inaccurate!\")\n",
    "\n",
    "write_output(\"\\n\" + \"-\"*60)\n",
    "write_output(\"DATA PREPROCESSING\")\n",
    "write_output(\"-\"*60)\n",
    "\n",
    "df_piez = gdf_piezometric.copy()\n",
    "df_piez['Date'] = df_piez[piez_fecha_col].apply(parse_date)\n",
    "df_piez['WaterLevel_m'] = pd.to_numeric(df_piez[piez_nivel_col], errors='coerce')\n",
    "\n",
    "if piez_lat_col in df_piez.columns and piez_lon_col in df_piez.columns:\n",
    "    df_piez['Latitude'] = pd.to_numeric(df_piez[piez_lat_col], errors='coerce')\n",
    "    df_piez['Longitude'] = pd.to_numeric(df_piez[piez_lon_col], errors='coerce')\n",
    "else:\n",
    "    df_piez['Longitude'] = df_piez.geometry.x\n",
    "    df_piez['Latitude'] = df_piez.geometry.y\n",
    "\n",
    "if piez_nombre_col in df_piez.columns:\n",
    "    df_piez['Station_Name'] = df_piez[piez_nombre_col]\n",
    "else:\n",
    "    df_piez['Station_Name'] = df_piez[piez_codigo_col]\n",
    "\n",
    "write_output(\"Cleaning data...\")\n",
    "initial_count = len(df_piez)\n",
    "\n",
    "df_piez_clean = df_piez.dropna(subset=['Date', 'WaterLevel_m']).copy()\n",
    "write_output(f\"After removing null dates/levels: {len(df_piez_clean):,}\")\n",
    "\n",
    "df_piez_clean = df_piez_clean[\n",
    "    (df_piez_clean['Latitude'] >= -56) & (df_piez_clean['Latitude'] <= -17) &\n",
    "    (df_piez_clean['Longitude'] >= -76) & (df_piez_clean['Longitude'] <= -66)\n",
    "]\n",
    "write_output(f\"After geographic filtering: {len(df_piez_clean):,}\")\n",
    "\n",
    "df_piez_clean = df_piez_clean[\n",
    "    (df_piez_clean['WaterLevel_m'] > 0) &\n",
    "    (df_piez_clean['WaterLevel_m'] < 500)\n",
    "]\n",
    "write_output(f\"After water level filtering: {len(df_piez_clean):,}\")\n",
    "\n",
    "df_piez_clean['Year'] = df_piez_clean['Date'].dt.year\n",
    "df_piez_clean['Month'] = df_piez_clean['Date'].dt.month\n",
    "df_piez_clean['YearMonth'] = df_piez_clean['Date'].dt.to_period('M')\n",
    "df_piez_clean['DayOfYear'] = df_piez_clean['Date'].dt.dayofyear\n",
    "\n",
    "write_output(f\"\\nTotal cleaned records: {len(df_piez_clean):,} ({len(df_piez_clean)/initial_count*100:.1f}% retained)\")\n",
    "write_output(f\"Date range: {df_piez_clean['Date'].min().strftime('%Y-%m-%d')} to {df_piez_clean['Date'].max().strftime('%Y-%m-%d')}\")\n",
    "\n",
    "write_output(\"\\n\" + \"-\"*60)\n",
    "write_output(\"ID NORMALIZATION\")\n",
    "write_output(\"-\"*60)\n",
    "\n",
    "id_lengths = df_piez_clean[piez_codigo_col].astype(str).str.strip().str.len().value_counts()\n",
    "target_length = id_lengths.idxmax()\n",
    "write_output(f\"ID length distribution: {id_lengths.to_dict()}\")\n",
    "write_output(f\"Using target length: {target_length}\")\n",
    "\n",
    "df_piez_clean['ID_Normalized'] = df_piez_clean[piez_codigo_col].apply(\n",
    "    lambda x: normalize_id_with_padding(x, target_length)\n",
    ")\n",
    "\n",
    "unique_before = df_piez_clean[piez_codigo_col].nunique()\n",
    "unique_after = df_piez_clean['ID_Normalized'].nunique()\n",
    "write_output(f\"Unique IDs before normalization: {unique_before:,}\")\n",
    "write_output(f\"Unique IDs after normalization: {unique_after:,}\")\n",
    "\n",
    "if unique_before != unique_after:\n",
    "    write_output(f\"WARNING: {unique_before - unique_after} IDs merged during normalization!\")\n",
    "\n",
    "piez_codigo_col = 'ID_Normalized'\n",
    "\n",
    "write_output(\"ID normalization complete\")\n",
    "\n",
    "write_output(\"\\n\" + \"-\"*60)\n",
    "write_output(\"FILTERING WELLS WITH RECENT DATA\")\n",
    "write_output(\"-\"*60)\n",
    "\n",
    "write_output(\"Calculating well statistics...\")\n",
    "\n",
    "well_stats = df_piez_clean.groupby(piez_codigo_col).agg({\n",
    "    'Date': ['min', 'max', 'count'],\n",
    "    'Year': ['min', 'max', 'nunique'],\n",
    "    'Latitude': 'first',\n",
    "    'Longitude': 'first',\n",
    "    'Station_Name': 'first',\n",
    "    'WaterLevel_m': ['mean', 'std', 'min', 'max', 'median']\n",
    "}).reset_index()\n",
    "\n",
    "well_stats.columns = [\n",
    "    'Station_Code', 'Date_Start', 'Date_End', 'N_Records',\n",
    "    'Year_Start', 'Year_End', 'N_Years_With_Data',\n",
    "    'Latitude', 'Longitude', 'Station_Name',\n",
    "    'WL_Mean', 'WL_Std', 'WL_Min', 'WL_Max', 'WL_Median'\n",
    "]\n",
    "\n",
    "well_stats['Years_Span'] = well_stats['Year_End'] - well_stats['Year_Start']\n",
    "well_stats['Records_Per_Year'] = well_stats['N_Records'] / (well_stats['Years_Span'] + 1)\n",
    "well_stats['Data_Completeness'] = well_stats['N_Years_With_Data'] / (well_stats['Years_Span'] + 1)\n",
    "\n",
    "write_output(f\"\\nTotal unique wells in database: {len(well_stats):,}\")\n",
    "\n",
    "write_output(f\"\\nDistribution of wells by last year of data:\")\n",
    "year_dist = well_stats.groupby('Year_End').size()\n",
    "for year in sorted(year_dist.index)[-10:]:\n",
    "    write_output(f\"   {year}: {year_dist[year]:,} wells\")\n",
    "\n",
    "write_output(f\"\\nApplying filters:\")\n",
    "write_output(f\"   - Data extending to >= {MIN_RECENT_YEAR}\")\n",
    "write_output(f\"   - Minimum {MIN_RECORDS} records\")\n",
    "write_output(f\"   - Minimum {MIN_YEARS_SPAN} years span\")\n",
    "\n",
    "wells_filtered = well_stats[\n",
    "    (well_stats['Year_End'] >= MIN_RECENT_YEAR) &\n",
    "    (well_stats['N_Records'] >= MIN_RECORDS) &\n",
    "    (well_stats['Years_Span'] >= MIN_YEARS_SPAN)\n",
    "].copy()\n",
    "\n",
    "write_output(f\"\\nWells meeting all criteria: {len(wells_filtered):,}\")\n",
    "\n",
    "if len(wells_filtered) < 10:\n",
    "    write_output(\"\\nWARNING: Too few wells meet criteria. Relaxing constraints...\")\n",
    "    relaxation_levels = [\n",
    "        (2018, 20, 4),\n",
    "        (2016, 15, 3),\n",
    "        (2015, 12, 3),\n",
    "        (2010, 10, 2)\n",
    "    ]\n",
    "    for min_year, min_rec, min_span in relaxation_levels:\n",
    "        wells_filtered = well_stats[\n",
    "            (well_stats['Year_End'] >= min_year) &\n",
    "            (well_stats['N_Records'] >= min_rec) &\n",
    "            (well_stats['Years_Span'] >= min_span)\n",
    "        ].copy()\n",
    "        if len(wells_filtered) >= 10:\n",
    "            write_output(f\"Using relaxed criteria: year>={min_year}, records>={min_rec}, span>={min_span}\")\n",
    "            write_output(f\"Wells meeting relaxed criteria: {len(wells_filtered):,}\")\n",
    "            MIN_RECENT_YEAR = min_year\n",
    "            MIN_RECORDS = min_rec\n",
    "            MIN_YEARS_SPAN = min_span\n",
    "            break\n",
    "\n",
    "if len(wells_filtered) == 0:\n",
    "    write_output(\"ERROR: No wells meet any criteria. Please check data quality.\")\n",
    "    wells_filtered = well_stats[well_stats['N_Records'] >= 5].copy()\n",
    "    write_output(f\"Using all wells with >= 5 records: {len(wells_filtered):,}\")\n",
    "\n",
    "write_output(\"\\n\" + \"-\"*60)\n",
    "write_output(\"SPATIAL ASSIGNMENT OF WELLS\")\n",
    "write_output(\"-\"*60)\n",
    "\n",
    "gdf_wells = gpd.GeoDataFrame(\n",
    "    wells_filtered,\n",
    "    geometry=gpd.points_from_xy(wells_filtered['Longitude'], wells_filtered['Latitude']),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "gdf_wells = gdf_wells.to_crs(CRS_WORKING)\n",
    "\n",
    "def safe_spatial_join(gdf_points, gdf_polygons, col_name, join_col):\n",
    "    try:\n",
    "        result = gpd.sjoin(\n",
    "            gdf_points,\n",
    "            gdf_polygons[[join_col, 'geometry']],\n",
    "            how='left',\n",
    "            predicate='within'\n",
    "        )\n",
    "        if join_col in result.columns:\n",
    "            return result[join_col].values\n",
    "        elif f'{join_col}_right' in result.columns:\n",
    "            return result[f'{join_col}_right'].values\n",
    "        else:\n",
    "            return pd.Series([None] * len(gdf_points))\n",
    "    except Exception as e:\n",
    "        write_output(f\"Spatial join error for {col_name}: {e}\")\n",
    "        return pd.Series([None] * len(gdf_points))\n",
    "\n",
    "write_output(\"Performing spatial joins...\")\n",
    "\n",
    "wells_filtered['SHAC'] = safe_spatial_join(gdf_wells, gdf_shac, 'SHAC', shac_col)\n",
    "wells_filtered['Cuenca'] = safe_spatial_join(gdf_wells, gdf_cuencas, 'Cuenca', cuenca_col)\n",
    "wells_filtered['Comuna'] = safe_spatial_join(gdf_wells, gdf_comunas, 'Comuna', comuna_col)\n",
    "wells_filtered['Region'] = safe_spatial_join(gdf_wells, gdf_regiones, 'Region', region_col)\n",
    "\n",
    "write_output(f\"\\nSpatial assignment results:\")\n",
    "write_output(f\"   - SHAC assigned: {wells_filtered['SHAC'].notna().sum():,} wells ({wells_filtered['SHAC'].nunique()} unique)\")\n",
    "write_output(f\"   - Cuenca assigned: {wells_filtered['Cuenca'].notna().sum():,} wells ({wells_filtered['Cuenca'].nunique()} unique)\")\n",
    "write_output(f\"   - Comuna assigned: {wells_filtered['Comuna'].notna().sum():,} wells ({wells_filtered['Comuna'].nunique()} unique)\")\n",
    "write_output(f\"   - Region assigned: {wells_filtered['Region'].notna().sum():,} wells ({wells_filtered['Region'].nunique()} unique)\")\n",
    "\n",
    "write_output(\"\\n\" + \"=\"*80)\n",
    "write_output(\"SECTION 2: INDIVIDUAL WELL ANALYSIS\")\n",
    "write_output(\"=\"*80)\n",
    "\n",
    "total_wells = len(wells_filtered)\n",
    "successful_analyses = len([r for r in well_results if 'Station_Code' in r])\n",
    "failed_analyses = 0\n",
    "skipped_wells = 0\n",
    "\n",
    "write_output(f\"\\nAnalyzing {total_wells} wells with multiple models...\")\n",
    "write_output(f\"Previously processed: {len(processed_wells)} wells\")\n",
    "\n",
    "progress_intervals = [int(total_wells * p) for p in [0.1, 0.25, 0.5, 0.75, 0.9, 1.0]]\n",
    "\n",
    "for idx, (_, well) in enumerate(wells_filtered.iterrows()):\n",
    "    station_code = well['Station_Code']\n",
    "    \n",
    "    if station_code in processed_wells:\n",
    "        skipped_wells += 1\n",
    "        continue\n",
    "    \n",
    "    if idx + 1 in progress_intervals or idx == 0 or (idx + 1) % 10 == 0:\n",
    "        pct = (idx + 1) / total_wells * 100\n",
    "        write_output(f\"Progress: {idx + 1}/{total_wells} ({pct:.0f}%) - Current: {station_code} - Processed: {successful_analyses} - Skipped: {skipped_wells}\")\n",
    "    \n",
    "    try:\n",
    "        well_data = df_piez_clean[df_piez_clean[piez_codigo_col] == station_code].copy()\n",
    "        well_data = well_data.sort_values('Date')\n",
    "        \n",
    "        if len(well_data) < 10:\n",
    "            failed_analyses += 1\n",
    "            processed_wells.add(station_code)\n",
    "            write_output(f\"Skipping {station_code}: Insufficient data ({len(well_data)} records)\")\n",
    "            continue\n",
    "        \n",
    "        monthly_data = well_data.groupby('YearMonth').agg({\n",
    "            'WaterLevel_m': 'mean',\n",
    "            'Date': 'first'\n",
    "        }).reset_index()\n",
    "        monthly_data['Date'] = monthly_data['YearMonth'].dt.to_timestamp()\n",
    "        monthly_data = monthly_data.sort_values('Date')\n",
    "        monthly_data = monthly_data.set_index('Date')\n",
    "        \n",
    "        result = {\n",
    "            'Station_Code': station_code,\n",
    "            'Station_Name': well['Station_Name'],\n",
    "            'SHAC': well['SHAC'],\n",
    "            'Cuenca': well['Cuenca'],\n",
    "            'Comuna': well['Comuna'],\n",
    "            'Region': well['Region'],\n",
    "            'Latitude': well['Latitude'],\n",
    "            'Longitude': well['Longitude'],\n",
    "            'N_Records': well['N_Records'],\n",
    "            'N_Monthly_Records': len(monthly_data),\n",
    "            'Year_Start': well['Year_Start'],\n",
    "            'Year_End': well['Year_End'],\n",
    "            'Years_Span': well['Years_Span'],\n",
    "            'Records_Per_Year': well['Records_Per_Year'],\n",
    "            'WL_Mean': well['WL_Mean'],\n",
    "            'WL_Std': well['WL_Std'],\n",
    "            'WL_Min': well['WL_Min'],\n",
    "            'WL_Max': well['WL_Max'],\n",
    "            'WL_Current': monthly_data['WaterLevel_m'].iloc[-1] if len(monthly_data) > 0 else np.nan,\n",
    "            'WL_First': monthly_data['WaterLevel_m'].iloc[0] if len(monthly_data) > 0 else np.nan,\n",
    "            'Total_Change_m': monthly_data['WaterLevel_m'].iloc[-1] - monthly_data['WaterLevel_m'].iloc[0] if len(monthly_data) > 1 else np.nan\n",
    "        }\n",
    "        \n",
    "        linear_result = calculate_linear_trend(well_data['Date'], well_data['WaterLevel_m'])\n",
    "        if linear_result:\n",
    "            result.update({\n",
    "                'Linear_Slope_m_yr': linear_result['slope_m_per_year'],\n",
    "                'Linear_R2': linear_result['r_squared'],\n",
    "                'Linear_PValue': linear_result['p_value'],\n",
    "                'Linear_StdErr': linear_result['std_error'],\n",
    "                'Linear_CI_Lower': linear_result['slope_ci_lower'],\n",
    "                'Linear_CI_Upper': linear_result['slope_ci_upper'],\n",
    "                'Linear_Significant': linear_result['significant'],\n",
    "                'Linear_Trend': linear_result['trend_direction']\n",
    "            })\n",
    "        else:\n",
    "            result.update({\n",
    "                'Linear_Slope_m_yr': np.nan, 'Linear_R2': np.nan, 'Linear_PValue': np.nan,\n",
    "                'Linear_StdErr': np.nan, 'Linear_CI_Lower': np.nan, 'Linear_CI_Upper': np.nan,\n",
    "                'Linear_Significant': False, 'Linear_Trend': 'Unknown'\n",
    "            })\n",
    "        \n",
    "        mk_result = mann_kendall_test(monthly_data['WaterLevel_m'])\n",
    "        if mk_result:\n",
    "            result.update({\n",
    "                'MK_Trend': mk_result['trend'],\n",
    "                'MK_PValue': mk_result['p_value'],\n",
    "                'MK_ZScore': mk_result['z_score'],\n",
    "                'MK_Tau': mk_result['tau'],\n",
    "                'MK_Significant': mk_result['significant'],\n",
    "                'MK_Direction': mk_result['trend_direction']\n",
    "            })\n",
    "        else:\n",
    "            result.update({\n",
    "                'MK_Trend': 'N/A', 'MK_PValue': np.nan, 'MK_ZScore': np.nan,\n",
    "                'MK_Tau': np.nan, 'MK_Significant': False, 'MK_Direction': 'Unknown'\n",
    "            })\n",
    "        \n",
    "        sens_result = sens_slope_estimator(well_data['Date'], well_data['WaterLevel_m'])\n",
    "        if sens_result:\n",
    "            result.update({\n",
    "                'Sens_Slope_m_yr': sens_result['slope_m_per_year'],\n",
    "                'Sens_Trend': sens_result['trend_direction'],\n",
    "                'Sens_Predicted_Change': sens_result['predicted_change']\n",
    "            })\n",
    "        else:\n",
    "            result.update({\n",
    "                'Sens_Slope_m_yr': np.nan, 'Sens_Trend': 'Unknown', 'Sens_Predicted_Change': np.nan\n",
    "            })\n",
    "        \n",
    "        if len(monthly_data) >= 24:\n",
    "            sk_result = seasonal_kendall_test(monthly_data['WaterLevel_m'])\n",
    "            if sk_result:\n",
    "                result.update({\n",
    "                    'SK_Trend': sk_result['trend'],\n",
    "                    'SK_PValue': sk_result['p_value'],\n",
    "                    'SK_Significant': sk_result['significant']\n",
    "                })\n",
    "            else:\n",
    "                result.update({'SK_Trend': 'N/A', 'SK_PValue': np.nan, 'SK_Significant': False})\n",
    "        else:\n",
    "            result.update({'SK_Trend': 'N/A', 'SK_PValue': np.nan, 'SK_Significant': False})\n",
    "        \n",
    "        trends = []\n",
    "        if result.get('Linear_Trend') in ['Decreasing', 'Increasing']:\n",
    "            trends.append(result['Linear_Trend'])\n",
    "        if result.get('MK_Direction') in ['Decreasing', 'Increasing']:\n",
    "            trends.append(result['MK_Direction'])\n",
    "        if result.get('Sens_Trend') in ['Decreasing', 'Increasing']:\n",
    "            trends.append(result['Sens_Trend'])\n",
    "        \n",
    "        if trends:\n",
    "            result['Consensus_Trend'] = max(set(trends), key=trends.count)\n",
    "        else:\n",
    "            result['Consensus_Trend'] = 'Unknown'\n",
    "        \n",
    "        last_date = monthly_data.index.max()\n",
    "        target_date = pd.Timestamp(f'{PREDICTION_YEAR}-12-31')\n",
    "        months_to_forecast = max(1, (target_date.year - last_date.year) * 12 + (target_date.month - last_date.month))\n",
    "        \n",
    "        predictions = {}\n",
    "        \n",
    "        try:\n",
    "            arima_result = fit_arima_model(monthly_data['WaterLevel_m'], months_to_forecast)\n",
    "            if arima_result:\n",
    "                predictions['ARIMA'] = arima_result\n",
    "                result.update({\n",
    "                    'ARIMA_Pred_2030': arima_result['forecast'].iloc[-1] if len(arima_result['forecast']) > 0 else np.nan,\n",
    "                    'ARIMA_Lower_2030': arima_result['forecast_lower'].iloc[-1] if len(arima_result['forecast_lower']) > 0 else np.nan,\n",
    "                    'ARIMA_Upper_2030': arima_result['forecast_upper'].iloc[-1] if len(arima_result['forecast_upper']) > 0 else np.nan,\n",
    "                    'ARIMA_Order': str(arima_result['order']),\n",
    "                    'ARIMA_AIC': arima_result['aic'],\n",
    "                    'ARIMA_RMSE': arima_result['rmse'],\n",
    "                    'ARIMA_MAE': arima_result['mae'],\n",
    "                    'ARIMA_NSE': arima_result['nse']\n",
    "                })\n",
    "            else:\n",
    "                result.update({\n",
    "                    'ARIMA_Pred_2030': np.nan, 'ARIMA_Lower_2030': np.nan, 'ARIMA_Upper_2030': np.nan,\n",
    "                    'ARIMA_Order': 'Failed', 'ARIMA_AIC': np.nan, 'ARIMA_RMSE': np.nan,\n",
    "                    'ARIMA_MAE': np.nan, 'ARIMA_NSE': np.nan\n",
    "                })\n",
    "        except Exception as e:\n",
    "            result.update({'ARIMA_Pred_2030': np.nan, 'ARIMA_Order': 'Error', 'ARIMA_RMSE': np.nan,\n",
    "                          'ARIMA_MAE': np.nan, 'ARIMA_NSE': np.nan, 'ARIMA_Lower_2030': np.nan, 'ARIMA_Upper_2030': np.nan, 'ARIMA_AIC': np.nan})\n",
    "        \n",
    "        try:\n",
    "            hw_result = fit_holtwinters_model(monthly_data['WaterLevel_m'], months_to_forecast)\n",
    "            if hw_result:\n",
    "                predictions['HoltWinters'] = hw_result\n",
    "                result.update({\n",
    "                    'HW_Pred_2030': hw_result['forecast'].iloc[-1] if len(hw_result['forecast']) > 0 else np.nan,\n",
    "                    'HW_Lower_2030': hw_result['forecast_lower'].iloc[-1] if len(hw_result['forecast_lower']) > 0 else np.nan,\n",
    "                    'HW_Upper_2030': hw_result['forecast_upper'].iloc[-1] if len(hw_result['forecast_upper']) > 0 else np.nan,\n",
    "                    'HW_RMSE': hw_result['rmse'],\n",
    "                    'HW_MAE': hw_result['mae'],\n",
    "                    'HW_NSE': hw_result['nse']\n",
    "                })\n",
    "            else:\n",
    "                result.update({\n",
    "                    'HW_Pred_2030': np.nan, 'HW_Lower_2030': np.nan, 'HW_Upper_2030': np.nan,\n",
    "                    'HW_RMSE': np.nan, 'HW_MAE': np.nan, 'HW_NSE': np.nan\n",
    "                })\n",
    "        except Exception as e:\n",
    "            result.update({'HW_Pred_2030': np.nan, 'HW_RMSE': np.nan, 'HW_MAE': np.nan, 'HW_NSE': np.nan,\n",
    "                          'HW_Lower_2030': np.nan, 'HW_Upper_2030': np.nan})\n",
    "        \n",
    "        if PROPHET_AVAILABLE and len(monthly_data) >= 24:\n",
    "            try:\n",
    "                prophet_result = fit_prophet_model(monthly_data['WaterLevel_m'], months_to_forecast)\n",
    "                if prophet_result:\n",
    "                    predictions['Prophet'] = prophet_result\n",
    "                    result.update({\n",
    "                        'Prophet_Pred_2030': prophet_result['forecast'][-1] if len(prophet_result['forecast']) > 0 else np.nan,\n",
    "                        'Prophet_Lower_2030': prophet_result['forecast_lower'][-1] if len(prophet_result['forecast_lower']) > 0 else np.nan,\n",
    "                        'Prophet_Upper_2030': prophet_result['forecast_upper'][-1] if len(prophet_result['forecast_upper']) > 0 else np.nan,\n",
    "                        'Prophet_RMSE': prophet_result['rmse'],\n",
    "                        'Prophet_MAE': prophet_result['mae'],\n",
    "                        'Prophet_NSE': prophet_result['nse']\n",
    "                    })\n",
    "                else:\n",
    "                    result.update({\n",
    "                        'Prophet_Pred_2030': np.nan, 'Prophet_Lower_2030': np.nan, 'Prophet_Upper_2030': np.nan,\n",
    "                        'Prophet_RMSE': np.nan, 'Prophet_MAE': np.nan, 'Prophet_NSE': np.nan\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                result.update({'Prophet_Pred_2030': np.nan, 'Prophet_RMSE': np.nan,\n",
    "                              'Prophet_MAE': np.nan, 'Prophet_NSE': np.nan, 'Prophet_Lower_2030': np.nan, 'Prophet_Upper_2030': np.nan})\n",
    "        else:\n",
    "            result.update({\n",
    "                'Prophet_Pred_2030': np.nan, 'Prophet_Lower_2030': np.nan, 'Prophet_Upper_2030': np.nan,\n",
    "                'Prophet_RMSE': np.nan, 'Prophet_MAE': np.nan, 'Prophet_NSE': np.nan\n",
    "            })\n",
    "        \n",
    "        if len(monthly_data) >= 48:\n",
    "            try:\n",
    "                lstm_result = fit_lstm_model(monthly_data['WaterLevel_m'], months_to_forecast, LSTM_LOOKBACK)\n",
    "                if lstm_result:\n",
    "                    predictions['LSTM'] = lstm_result\n",
    "                    result.update({\n",
    "                        'LSTM_Pred_2030': lstm_result['forecast'][-1] if len(lstm_result['forecast']) > 0 else np.nan,\n",
    "                        'LSTM_Lower_2030': lstm_result['forecast_lower'][-1] if len(lstm_result['forecast_lower']) > 0 else np.nan,\n",
    "                        'LSTM_Upper_2030': lstm_result['forecast_upper'][-1] if len(lstm_result['forecast_upper']) > 0 else np.nan,\n",
    "                        'LSTM_RMSE': lstm_result['rmse'],\n",
    "                        'LSTM_MAE': lstm_result['mae'],\n",
    "                        'LSTM_NSE': lstm_result['nse']\n",
    "                    })\n",
    "                    del lstm_result['model']\n",
    "                    K.clear_session()\n",
    "                else:\n",
    "                    result.update({\n",
    "                        'LSTM_Pred_2030': np.nan, 'LSTM_Lower_2030': np.nan, 'LSTM_Upper_2030': np.nan,\n",
    "                        'LSTM_RMSE': np.nan, 'LSTM_MAE': np.nan, 'LSTM_NSE': np.nan\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                result.update({'LSTM_Pred_2030': np.nan, 'LSTM_RMSE': np.nan,\n",
    "                              'LSTM_MAE': np.nan, 'LSTM_NSE': np.nan, 'LSTM_Lower_2030': np.nan, 'LSTM_Upper_2030': np.nan})\n",
    "        else:\n",
    "            result.update({\n",
    "                'LSTM_Pred_2030': np.nan, 'LSTM_Lower_2030': np.nan, 'LSTM_Upper_2030': np.nan,\n",
    "                'LSTM_RMSE': np.nan, 'LSTM_MAE': np.nan, 'LSTM_NSE': np.nan\n",
    "            })\n",
    "        \n",
    "        if len(monthly_data) >= 60:\n",
    "            try:\n",
    "                bilstm_result = fit_bilstm_model(monthly_data['WaterLevel_m'], months_to_forecast, LSTM_LOOKBACK)\n",
    "                if bilstm_result:\n",
    "                    predictions['BiLSTM'] = bilstm_result\n",
    "                    result.update({\n",
    "                        'BiLSTM_Pred_2030': bilstm_result['forecast'][-1] if len(bilstm_result['forecast']) > 0 else np.nan,\n",
    "                        'BiLSTM_Lower_2030': bilstm_result['forecast_lower'][-1] if len(bilstm_result['forecast_lower']) > 0 else np.nan,\n",
    "                        'BiLSTM_Upper_2030': bilstm_result['forecast_upper'][-1] if len(bilstm_result['forecast_upper']) > 0 else np.nan,\n",
    "                        'BiLSTM_RMSE': bilstm_result['rmse'],\n",
    "                        'BiLSTM_MAE': bilstm_result['mae'],\n",
    "                        'BiLSTM_NSE': bilstm_result['nse']\n",
    "                    })\n",
    "                    del bilstm_result['model']\n",
    "                    K.clear_session()\n",
    "                else:\n",
    "                    result.update({\n",
    "                        'BiLSTM_Pred_2030': np.nan, 'BiLSTM_Lower_2030': np.nan, 'BiLSTM_Upper_2030': np.nan,\n",
    "                        'BiLSTM_RMSE': np.nan, 'BiLSTM_MAE': np.nan, 'BiLSTM_NSE': np.nan\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                result.update({'BiLSTM_Pred_2030': np.nan, 'BiLSTM_RMSE': np.nan,\n",
    "                              'BiLSTM_MAE': np.nan, 'BiLSTM_NSE': np.nan, 'BiLSTM_Lower_2030': np.nan, 'BiLSTM_Upper_2030': np.nan})\n",
    "        else:\n",
    "            result.update({\n",
    "                'BiLSTM_Pred_2030': np.nan, 'BiLSTM_Lower_2030': np.nan, 'BiLSTM_Upper_2030': np.nan,\n",
    "                'BiLSTM_RMSE': np.nan, 'BiLSTM_MAE': np.nan, 'BiLSTM_NSE': np.nan\n",
    "            })\n",
    "        \n",
    "        ensemble = calculate_ensemble_prediction(predictions)\n",
    "        if ensemble:\n",
    "            result.update({\n",
    "                'Ensemble_Pred_2030': ensemble['ensemble_mean'],\n",
    "                'Ensemble_Lower_2030': ensemble['ensemble_lower'],\n",
    "                'Ensemble_Upper_2030': ensemble['ensemble_upper'],\n",
    "                'Ensemble_Std': ensemble['ensemble_std'],\n",
    "                'Ensemble_N_Models': int(ensemble['n_models'])\n",
    "            })\n",
    "        else:\n",
    "            result.update({\n",
    "                'Ensemble_Pred_2030': np.nan, 'Ensemble_Lower_2030': np.nan,\n",
    "                'Ensemble_Upper_2030': np.nan, 'Ensemble_Std': np.nan, 'Ensemble_N_Models': 0\n",
    "            })\n",
    "        \n",
    "        current_level = result['WL_Current']\n",
    "        for model in ['ARIMA', 'HW', 'Prophet', 'LSTM', 'BiLSTM', 'Ensemble']:\n",
    "            pred_col = f'{model}_Pred_2030'\n",
    "            change_col = f'{model}_Change_2030'\n",
    "            if pred_col in result and not np.isnan(result.get(pred_col, np.nan)):\n",
    "                result[change_col] = result[pred_col] - current_level\n",
    "            else:\n",
    "                result[change_col] = np.nan\n",
    "        \n",
    "        well_results.append(result)\n",
    "        processed_wells.add(station_code)\n",
    "        successful_analyses += 1\n",
    "        \n",
    "        if (idx + 1) % 5 == 0:\n",
    "            save_progress(processed_wells, well_results, idx, total_wells)\n",
    "            gc.collect()\n",
    "            K.clear_session()\n",
    "    \n",
    "    except Exception as e:\n",
    "        write_output(f\"CRITICAL ERROR processing well {station_code}: {e}\")\n",
    "        processed_wells.add(station_code)\n",
    "        failed_analyses += 1\n",
    "        save_progress(processed_wells, well_results, idx, total_wells)\n",
    "        continue\n",
    "\n",
    "save_progress(processed_wells, well_results, total_wells, total_wells)\n",
    "\n",
    "df_well_results = pd.DataFrame(well_results)\n",
    "\n",
    "write_output(f\"\\nAnalysis complete!\")\n",
    "write_output(f\"Successfully analyzed: {successful_analyses:,} wells\")\n",
    "write_output(f\"Failed analyses: {failed_analyses:,} wells\")\n",
    "write_output(f\"Skipped (already processed): {skipped_wells:,} wells\")\n",
    "\n",
    "write_output(f\"\\nModel availability summary:\")\n",
    "write_output(f\"   - Linear trend: {df_well_results['Linear_Slope_m_yr'].notna().sum():,} wells\")\n",
    "write_output(f\"   - Mann-Kendall: {(df_well_results['MK_Trend'] != 'N/A').sum():,} wells\")\n",
    "write_output(f\"   - Sen's Slope: {df_well_results['Sens_Slope_m_yr'].notna().sum():,} wells\")\n",
    "write_output(f\"   - ARIMA: {df_well_results['ARIMA_Pred_2030'].notna().sum():,} wells\")\n",
    "write_output(f\"   - Holt-Winters: {df_well_results['HW_Pred_2030'].notna().sum():,} wells\")\n",
    "write_output(f\"   - Prophet: {df_well_results['Prophet_Pred_2030'].notna().sum():,} wells\")\n",
    "write_output(f\"   - LSTM: {df_well_results['LSTM_Pred_2030'].notna().sum():,} wells\")\n",
    "write_output(f\"   - BiLSTM: {df_well_results['BiLSTM_Pred_2030'].notna().sum():,} wells\")\n",
    "write_output(f\"   - Ensemble: {df_well_results['Ensemble_Pred_2030'].notna().sum():,} wells\")\n",
    "\n",
    "write_output(\"\\n\" + \"=\"*80)\n",
    "write_output(\"SECTION 3: AGGREGATION BY SPATIAL UNITS\")\n",
    "write_output(\"=\"*80)\n",
    "\n",
    "def aggregate_by_unit(df, unit_col, unit_name):\n",
    "    write_output(f\"\\nAggregating by {unit_name}...\")\n",
    "    df_valid = df[df[unit_col].notna() & (df[unit_col] != '')].copy()\n",
    "    if len(df_valid) == 0:\n",
    "        write_output(f\"No valid data for {unit_name}\")\n",
    "        return pd.DataFrame()\n",
    "    agg_results = []\n",
    "    for unit in df_valid[unit_col].unique():\n",
    "        unit_data = df_valid[df_valid[unit_col] == unit]\n",
    "        if len(unit_data) == 0:\n",
    "            continue\n",
    "        total_wells = len(unit_data)\n",
    "        wells_decreasing_linear = len(unit_data[unit_data['Linear_Trend'] == 'Decreasing'])\n",
    "        wells_decreasing_mk = len(unit_data[unit_data['MK_Direction'] == 'Decreasing'])\n",
    "        wells_decreasing_consensus = len(unit_data[unit_data['Consensus_Trend'] == 'Decreasing'])\n",
    "        wells_significant_linear = len(unit_data[unit_data['Linear_Significant'] == True])\n",
    "        wells_significant_mk = len(unit_data[unit_data['MK_Significant'] == True])\n",
    "        result = {\n",
    "            unit_name: unit,\n",
    "            'Total_Wells': total_wells,\n",
    "            'Wells_Decreasing_Linear': wells_decreasing_linear,\n",
    "            'Wells_Decreasing_MK': wells_decreasing_mk,\n",
    "            'Wells_Decreasing_Consensus': wells_decreasing_consensus,\n",
    "            'Pct_Decreasing_Linear': (wells_decreasing_linear / total_wells * 100) if total_wells > 0 else 0,\n",
    "            'Pct_Decreasing_MK': (wells_decreasing_mk / total_wells * 100) if total_wells > 0 else 0,\n",
    "            'Pct_Decreasing_Consensus': (wells_decreasing_consensus / total_wells * 100) if total_wells > 0 else 0,\n",
    "            'Wells_Significant_Linear': wells_significant_linear,\n",
    "            'Wells_Significant_MK': wells_significant_mk,\n",
    "            'Pct_Significant_Linear': (wells_significant_linear / total_wells * 100) if total_wells > 0 else 0,\n",
    "            'Pct_Significant_MK': (wells_significant_mk / total_wells * 100) if total_wells > 0 else 0,\n",
    "            'Year_Range_Start': int(unit_data['Year_Start'].min()),\n",
    "            'Year_Range_End': int(unit_data['Year_End'].max()),\n",
    "            'Avg_Years_Record': unit_data['Years_Span'].mean(),\n",
    "            'Avg_Records_Per_Well': unit_data['N_Records'].mean(),\n",
    "            'Avg_Records_Per_Year': unit_data['Records_Per_Year'].mean(),\n",
    "            'Avg_Linear_Slope_m_yr': unit_data['Linear_Slope_m_yr'].mean(),\n",
    "            'Median_Linear_Slope_m_yr': unit_data['Linear_Slope_m_yr'].median(),\n",
    "            'Std_Linear_Slope_m_yr': unit_data['Linear_Slope_m_yr'].std(),\n",
    "            'Max_Decline_Linear_m_yr': unit_data['Linear_Slope_m_yr'].max(),\n",
    "            'Max_Rise_Linear_m_yr': unit_data['Linear_Slope_m_yr'].min(),\n",
    "            'Avg_Sens_Slope_m_yr': unit_data['Sens_Slope_m_yr'].mean(),\n",
    "            'Median_Sens_Slope_m_yr': unit_data['Sens_Slope_m_yr'].median(),\n",
    "            'Avg_Current_Level_m': unit_data['WL_Current'].mean(),\n",
    "            'Std_Current_Level_m': unit_data['WL_Current'].std(),\n",
    "            'Min_Current_Level_m': unit_data['WL_Current'].min(),\n",
    "            'Max_Current_Level_m': unit_data['WL_Current'].max(),\n",
    "            'Avg_Total_Change_m': unit_data['Total_Change_m'].mean(),\n",
    "            'Max_Total_Decline_m': unit_data['Total_Change_m'].max(),\n",
    "            'Wells_With_ARIMA': unit_data['ARIMA_Pred_2030'].notna().sum(),\n",
    "            'Avg_ARIMA_Change_2030': unit_data['ARIMA_Change_2030'].mean(),\n",
    "            'Median_ARIMA_Change_2030': unit_data['ARIMA_Change_2030'].median(),\n",
    "            'Avg_ARIMA_NSE': unit_data['ARIMA_NSE'].mean(),\n",
    "            'Wells_With_LSTM': unit_data['LSTM_Pred_2030'].notna().sum(),\n",
    "            'Avg_LSTM_Change_2030': unit_data['LSTM_Change_2030'].mean(),\n",
    "            'Avg_LSTM_NSE': unit_data['LSTM_NSE'].mean(),\n",
    "            'Wells_With_Prophet': unit_data['Prophet_Pred_2030'].notna().sum(),\n",
    "            'Avg_Prophet_Change_2030': unit_data['Prophet_Change_2030'].mean(),\n",
    "            'Avg_Prophet_NSE': unit_data['Prophet_NSE'].mean(),\n",
    "            'Wells_With_Ensemble': unit_data['Ensemble_Pred_2030'].notna().sum(),\n",
    "            'Avg_Ensemble_Change_2030': unit_data['Ensemble_Change_2030'].mean(),\n",
    "            'Median_Ensemble_Change_2030': unit_data['Ensemble_Change_2030'].median(),\n",
    "            'Avg_Ensemble_Uncertainty': unit_data['Ensemble_Std'].mean()\n",
    "        }\n",
    "        agg_results.append(result)\n",
    "    df_agg = pd.DataFrame(agg_results)\n",
    "    if len(df_agg) > 0:\n",
    "        df_agg = df_agg.sort_values('Pct_Decreasing_Consensus', ascending=False)\n",
    "        df_agg.insert(0, 'Rank', range(1, len(df_agg) + 1))\n",
    "        df_agg['Criticality'] = pd.cut(\n",
    "            df_agg['Pct_Decreasing_Consensus'],\n",
    "            bins=[0, 25, 50, 75, 100],\n",
    "            labels=['Low', 'Moderate', 'High', 'Critical'],\n",
    "            include_lowest=True\n",
    "        )\n",
    "    write_output(f\"Aggregated {len(df_agg)} {unit_name} units\")\n",
    "    return df_agg\n",
    "\n",
    "df_shac_results = aggregate_by_unit(df_well_results, 'SHAC', 'SHAC')\n",
    "df_cuenca_results = aggregate_by_unit(df_well_results, 'Cuenca', 'Cuenca')\n",
    "df_comuna_results = aggregate_by_unit(df_well_results, 'Comuna', 'Comuna')\n",
    "df_region_results = aggregate_by_unit(df_well_results, 'Region', 'Region')\n",
    "\n",
    "write_output(\"\\n\" + \"=\"*80)\n",
    "write_output(\"SECTION 4: SUMMARY STATISTICS\")\n",
    "write_output(\"=\"*80)\n",
    "\n",
    "total_wells_analyzed = len(df_well_results)\n",
    "wells_with_decreasing_linear = len(df_well_results[df_well_results['Linear_Trend'] == 'Decreasing'])\n",
    "wells_with_decreasing_mk = len(df_well_results[df_well_results['MK_Direction'] == 'Decreasing'])\n",
    "wells_with_decreasing_consensus = len(df_well_results[df_well_results['Consensus_Trend'] == 'Decreasing'])\n",
    "wells_with_significant_linear = len(df_well_results[df_well_results['Linear_Significant'] == True])\n",
    "wells_with_significant_mk = len(df_well_results[df_well_results['MK_Significant'] == True])\n",
    "\n",
    "write_output(f\"\\nOVERALL STATISTICS:\")\n",
    "write_output(f\"Total wells analyzed: {total_wells_analyzed:,}\")\n",
    "write_output(f\"\\nTrend Detection Results:\")\n",
    "write_output(f\"Linear Regression - Decreasing: {wells_with_decreasing_linear:,} ({wells_with_decreasing_linear/total_wells_analyzed*100:.1f}%)\")\n",
    "write_output(f\"Linear Regression - Significant: {wells_with_significant_linear:,} ({wells_with_significant_linear/total_wells_analyzed*100:.1f}%)\")\n",
    "write_output(f\"Mann-Kendall - Decreasing: {wells_with_decreasing_mk:,} ({wells_with_decreasing_mk/total_wells_analyzed*100:.1f}%)\")\n",
    "write_output(f\"Mann-Kendall - Significant: {wells_with_significant_mk:,} ({wells_with_significant_mk/total_wells_analyzed*100:.1f}%)\")\n",
    "write_output(f\"Consensus - Decreasing: {wells_with_decreasing_consensus:,} ({wells_with_decreasing_consensus/total_wells_analyzed*100:.1f}%)\")\n",
    "\n",
    "valid_linear_slopes = df_well_results['Linear_Slope_m_yr'].dropna()\n",
    "valid_sens_slopes = df_well_results['Sens_Slope_m_yr'].dropna()\n",
    "\n",
    "write_output(f\"\\nTREND MAGNITUDE STATISTICS:\")\n",
    "write_output(f\"Linear OLS Trend (n={len(valid_linear_slopes)}):\")\n",
    "write_output(f\"   Mean: {valid_linear_slopes.mean():.4f} m/year\")\n",
    "write_output(f\"   Median: {valid_linear_slopes.median():.4f} m/year\")\n",
    "write_output(f\"   Std Dev: {valid_linear_slopes.std():.4f} m/year\")\n",
    "\n",
    "write_output(f\"\\nSen's Slope (robust, n={len(valid_sens_slopes)}):\")\n",
    "write_output(f\"   Mean: {valid_sens_slopes.mean():.4f} m/year\")\n",
    "write_output(f\"   Median: {valid_sens_slopes.median():.4f} m/year\")\n",
    "\n",
    "write_output(f\"\\nMODEL PERFORMANCE SUMMARY (NSE):\")\n",
    "for model in ['ARIMA', 'HW', 'Prophet', 'LSTM', 'BiLSTM']:\n",
    "    nse_col = f'{model}_NSE'\n",
    "    if nse_col in df_well_results.columns:\n",
    "        valid_nse = df_well_results[nse_col].dropna()\n",
    "        if len(valid_nse) > 0:\n",
    "            write_output(f\"   {model}: Mean NSE = {valid_nse.mean():.3f}, Median NSE = {valid_nse.median():.3f} (n={len(valid_nse)})\")\n",
    "\n",
    "write_output(f\"\\nPREDICTION SUMMARY (to {PREDICTION_YEAR}):\")\n",
    "for model_name in ['ARIMA', 'HW', 'Prophet', 'LSTM', 'Ensemble']:\n",
    "    change_col = f'{model_name}_Change_2030'\n",
    "    valid_preds = df_well_results[change_col].dropna()\n",
    "    if len(valid_preds) > 0:\n",
    "        write_output(f\"\\n{model_name} (n={len(valid_preds)}):\")\n",
    "        write_output(f\"   Mean change: {valid_preds.mean():+.2f} m\")\n",
    "        write_output(f\"   Median change: {valid_preds.median():+.2f} m\")\n",
    "\n",
    "write_output(\"\\n\" + \"=\"*80)\n",
    "write_output(\"SECTION 5: SAVING RESULTS\")\n",
    "write_output(\"=\"*80)\n",
    "\n",
    "excel_path = os.path.join(output_folder, 'Excel', 'Groundwater_Trend_Analysis_Complete.xlsx')\n",
    "\n",
    "write_output(f\"\\nSaving Excel file: {os.path.basename(excel_path)}\")\n",
    "\n",
    "with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "    summary_data = {\n",
    "        'Parameter': [\n",
    "            'Analysis Date',\n",
    "            'Total Wells Analyzed',\n",
    "            'Wells with Decreasing Trend (Linear)',\n",
    "            'Wells with Decreasing Trend (Mann-Kendall)',\n",
    "            'Wells with Decreasing Trend (Consensus)',\n",
    "            'Percentage Decreasing (Consensus)',\n",
    "            'Wells with Significant Linear Trend',\n",
    "            'Wells with Significant MK Trend',\n",
    "            'Mean Linear Slope (m/year)',\n",
    "            'Median Linear Slope (m/year)',\n",
    "            'Mean Sens Slope (m/year)',\n",
    "            'Mean ARIMA NSE',\n",
    "            'Mean LSTM NSE',\n",
    "            'Mean Ensemble Predicted Change (m)',\n",
    "            'Minimum Recent Year Required',\n",
    "            'Minimum Records Required',\n",
    "            'Minimum Years Span Required',\n",
    "            'Prediction Target Year',\n",
    "            'Significance Level',\n",
    "            'Confidence Level',\n",
    "            'Working CRS'\n",
    "        ],\n",
    "        'Value': [\n",
    "            datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            total_wells_analyzed,\n",
    "            wells_with_decreasing_linear,\n",
    "            wells_with_decreasing_mk,\n",
    "            wells_with_decreasing_consensus,\n",
    "            f\"{wells_with_decreasing_consensus/total_wells_analyzed*100:.2f}%\",\n",
    "            wells_with_significant_linear,\n",
    "            wells_with_significant_mk,\n",
    "            f\"{valid_linear_slopes.mean():.4f}\",\n",
    "            f\"{valid_linear_slopes.median():.4f}\",\n",
    "            f\"{valid_sens_slopes.mean():.4f}\",\n",
    "            f\"{df_well_results['ARIMA_NSE'].mean():.3f}\" if df_well_results['ARIMA_NSE'].notna().any() else \"N/A\",\n",
    "            f\"{df_well_results['LSTM_NSE'].mean():.3f}\" if df_well_results['LSTM_NSE'].notna().any() else \"N/A\",\n",
    "            f\"{df_well_results['Ensemble_Change_2030'].mean():.2f}\" if df_well_results['Ensemble_Change_2030'].notna().any() else \"N/A\",\n",
    "            MIN_RECENT_YEAR,\n",
    "            MIN_RECORDS,\n",
    "            MIN_YEARS_SPAN,\n",
    "            PREDICTION_YEAR,\n",
    "            SIGNIFICANCE_LEVEL,\n",
    "            CONFIDENCE_LEVEL,\n",
    "            CRS_WORKING\n",
    "        ]\n",
    "    }\n",
    "    pd.DataFrame(summary_data).to_excel(writer, sheet_name='Summary', index=False)\n",
    "    df_well_results.to_excel(writer, sheet_name='All_Wells_Details', index=False)\n",
    "    if len(df_region_results) > 0:\n",
    "        df_region_results.to_excel(writer, sheet_name='Rankings_Region', index=False)\n",
    "    if len(df_cuenca_results) > 0:\n",
    "        df_cuenca_results.to_excel(writer, sheet_name='Rankings_Cuenca', index=False)\n",
    "    if len(df_comuna_results) > 0:\n",
    "        df_comuna_results.to_excel(writer, sheet_name='Rankings_Comuna', index=False)\n",
    "    if len(df_shac_results) > 0:\n",
    "        df_shac_results.to_excel(writer, sheet_name='Rankings_SHAC', index=False)\n",
    "    critical_summary = []\n",
    "    for scale, df_scale, col_name in [\n",
    "        ('Region', df_region_results, 'Region'),\n",
    "        ('Cuenca', df_cuenca_results, 'Cuenca'),\n",
    "        ('Comuna', df_comuna_results, 'Comuna'),\n",
    "        ('SHAC', df_shac_results, 'SHAC')\n",
    "    ]:\n",
    "        if len(df_scale) > 0:\n",
    "            for threshold in [90, 75, 50]:\n",
    "                critical = df_scale[df_scale['Pct_Decreasing_Consensus'] >= threshold]\n",
    "                critical_summary.append({\n",
    "                    'Spatial_Scale': scale,\n",
    "                    'Threshold': f'>={threshold}% declining',\n",
    "                    'Count': len(critical),\n",
    "                    'Units': ', '.join(critical[col_name].astype(str).tolist()[:10]) + ('...' if len(critical) > 10 else '')\n",
    "                })\n",
    "    if critical_summary:\n",
    "        pd.DataFrame(critical_summary).to_excel(writer, sheet_name='Critical_Areas_Summary', index=False)\n",
    "\n",
    "write_output(\"Excel file saved successfully\")\n",
    "\n",
    "txt_path = os.path.join(output_folder, 'Text_Output', 'Groundwater_Trend_Analysis_Complete_Report.txt')\n",
    "try:\n",
    "    import shutil\n",
    "    shutil.copy(log_file_path, txt_path)\n",
    "    write_output(f\"Text report saved: {os.path.basename(txt_path)}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if os.path.exists(progress_file):\n",
    "    try:\n",
    "        os.remove(progress_file)\n",
    "        write_output(\"Progress tracker file cleaned up\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "write_output(\"\\n\" + \"=\"*80)\n",
    "write_output(\"ANALYSIS COMPLETED SUCCESSFULLY\")\n",
    "write_output(\"=\"*80)\n",
    "write_output(f\"Output Directory: {output_folder}\")\n",
    "write_output(f\"Analysis completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "write_output(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
