{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a01d32f-43a8-4667-8bdb-54b483f7e4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import re\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "EXCEL_PATH = r\"\\assessment_of_wells_chile\\data\\DGA\\DGA_consolidado_nacional\\outputs_v2\\Registros_en_Chile_arreglado_v2.xlsx\"\n",
    "OUTPUT_FOLDER = r\"\\assessment_of_wells_chile\\data\\DGA\\DGA_dataset_analysis_output\"\n",
    "\n",
    "GDB_PATH = r\"\\assessment_of_wells_chile\\arcgis\\assessment_of_wells_chile\\Default.gdb\"\n",
    "\n",
    "REFERENCE_LAYERS = [\n",
    "    {\n",
    "        'path': GDB_PATH,\n",
    "        'layer_name': 'CHL_Municipalities',\n",
    "        'prefix': 'Muni',\n",
    "        'name_col': 'NAME',\n",
    "        'code_col': 'Code_Muni',\n",
    "        'native_crs': 'EPSG:3857',\n",
    "        'is_gdb': True\n",
    "    },\n",
    "    {\n",
    "        'path': GDB_PATH,\n",
    "        'layer_name': 'CHL_Regions',\n",
    "        'prefix': 'Region',\n",
    "        'name_col': 'NAME',\n",
    "        'code_col': 'ID',\n",
    "        'native_crs': 'EPSG:3857',\n",
    "        'is_gdb': True\n",
    "    },\n",
    "    {\n",
    "        'path': r\"\\assessment_of_wells_chile\\data\\Basins\\Cuencas_BNA\\Cuencas_BNA.shp\",\n",
    "        'layer_name': None,\n",
    "        'prefix': 'Cuenca',\n",
    "        'name_col': 'NOM_CUEN',\n",
    "        'code_col': 'COD_CUEN',\n",
    "        'native_crs': 'EPSG:32719',\n",
    "        'is_gdb': False\n",
    "    },\n",
    "    {\n",
    "        'path': r\"\\assessment_of_wells_chile\\data\\Aquifers\\INV_ACUIFEROS_SHAC_202302\\INV_ACUIFEROS_SHAC.shp\",\n",
    "        'layer_name': None,\n",
    "        'prefix': 'SHAC',\n",
    "        'name_col': 'SHAC',\n",
    "        'code_col': 'COD_SHAC',\n",
    "        'native_crs': 'EPSG:32719',\n",
    "        'is_gdb': False\n",
    "    }\n",
    "]\n",
    "\n",
    "TARGET_CRS = \"EPSG:4326\"\n",
    "\n",
    "DATE_COL_PRIMARY = 'Fecha Toma Razón'\n",
    "DATE_COL_SECONDARY = 'Fecha de Resolución/ Envío al Juez/ Inscripción C.B.R.'\n",
    "\n",
    "UNIT_CONVERSION = {\n",
    "    'Lt/s': 1.0,\n",
    "    'Lt/min': 1.0 / 60.0,\n",
    "    'm3/h': 1000.0 / 3600.0,\n",
    "    'm3/año': 1000.0 / (365.25 * 24 * 3600),\n",
    "    'Lt/h': 1.0 / 3600.0,\n",
    "    'lt/día': 1.0 / 86400.0,\n",
    "    'm3/dia': 1000.0 / 86400.0,\n",
    "    'm3/s': 1000.0,\n",
    "    'm3/mes': 1000.0 / (30.0 * 24 * 3600)\n",
    "}\n",
    "\n",
    "EXCLUDED_UNITS = ['%', 'Acciones']\n",
    "\n",
    "MAX_REALISTIC_FLOW_LS = 500.0\n",
    "MIN_REALISTIC_FLOW_LS = 0.0\n",
    "\n",
    "def create_output_folder(path):\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Output folder ready: {path}\")\n",
    "\n",
    "def load_reference_layer(layer_config):\n",
    "    path = layer_config['path']\n",
    "    prefix = layer_config['prefix']\n",
    "    native_crs = layer_config['native_crs']\n",
    "    is_gdb = layer_config['is_gdb']\n",
    "    layer_name = layer_config.get('layer_name')\n",
    "    name_col = layer_config['name_col']\n",
    "    code_col = layer_config['code_col']\n",
    "    \n",
    "    print(f\"\\n  Loading {prefix} layer...\")\n",
    "    print(f\"    Path: {path}\")\n",
    "    \n",
    "    try:\n",
    "        if is_gdb:\n",
    "            print(f\"    Layer in GDB: {layer_name}\")\n",
    "            gdf = gpd.read_file(path, layer=layer_name)\n",
    "        else:\n",
    "            gdf = gpd.read_file(path)\n",
    "        \n",
    "        print(f\"    Records loaded: {len(gdf):,}\")\n",
    "        print(f\"    Native CRS: {gdf.crs}\")\n",
    "        \n",
    "        if gdf.crs is None:\n",
    "            print(f\"    WARNING: No CRS defined, assuming {native_crs}\")\n",
    "            gdf = gdf.set_crs(native_crs)\n",
    "        \n",
    "        if gdf.crs.to_string() != TARGET_CRS:\n",
    "            print(f\"    Reprojecting from {gdf.crs} to {TARGET_CRS}...\")\n",
    "            gdf = gdf.to_crs(TARGET_CRS)\n",
    "        \n",
    "        available_cols = gdf.columns.tolist()\n",
    "        print(f\"    Available columns: {available_cols}\")\n",
    "        \n",
    "        if name_col not in available_cols:\n",
    "            print(f\"    WARNING: Name column '{name_col}' not found!\")\n",
    "            for col in available_cols:\n",
    "                if 'name' in col.lower() or 'nom' in col.lower():\n",
    "                    print(f\"    Using '{col}' as name column instead\")\n",
    "                    name_col = col\n",
    "                    break\n",
    "        \n",
    "        if code_col not in available_cols:\n",
    "            print(f\"    WARNING: Code column '{code_col}' not found!\")\n",
    "            for col in available_cols:\n",
    "                if 'cod' in col.lower() or 'id' in col.lower():\n",
    "                    print(f\"    Using '{col}' as code column instead\")\n",
    "                    code_col = col\n",
    "                    break\n",
    "        \n",
    "        cols_to_keep = ['geometry']\n",
    "        if name_col in gdf.columns:\n",
    "            cols_to_keep.append(name_col)\n",
    "        if code_col in gdf.columns and code_col != name_col:\n",
    "            cols_to_keep.append(code_col)\n",
    "        \n",
    "        gdf = gdf[cols_to_keep].copy()\n",
    "        \n",
    "        rename_dict = {}\n",
    "        if name_col in gdf.columns:\n",
    "            rename_dict[name_col] = f'{prefix}_Name'\n",
    "        if code_col in gdf.columns and code_col != name_col:\n",
    "            rename_dict[code_col] = f'{prefix}_Code'\n",
    "        \n",
    "        gdf = gdf.rename(columns=rename_dict)\n",
    "        \n",
    "        print(f\"    Final columns: {gdf.columns.tolist()}\")\n",
    "        print(f\"    Final CRS: {gdf.crs}\")\n",
    "        \n",
    "        return gdf, prefix\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    ERROR loading {prefix}: {str(e)}\")\n",
    "        return None, prefix\n",
    "\n",
    "def load_excel_data(excel_path):\n",
    "    print(f\"\\nLoading Excel file: {excel_path}\")\n",
    "    df = pd.read_excel(excel_path)\n",
    "    print(f\"  -> Loaded {len(df):,} records\")\n",
    "    return df\n",
    "\n",
    "def convert_flow_to_ls(row):\n",
    "    flow = row['Caudal Anual Prom']\n",
    "    unit = row['Unidad de Caudal']\n",
    "\n",
    "    if pd.isna(flow) or pd.isna(unit):\n",
    "        return np.nan\n",
    "\n",
    "    if unit in EXCLUDED_UNITS:\n",
    "        return np.nan\n",
    "\n",
    "    unit_clean = str(unit).strip()\n",
    "    if unit_clean in UNIT_CONVERSION:\n",
    "        return float(flow) * UNIT_CONVERSION[unit_clean]\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def classify_anomaly(row):\n",
    "    flow_ls = row['Caudal_Ls']\n",
    "    unit = row['Unidad de Caudal']\n",
    "\n",
    "    if pd.isna(flow_ls):\n",
    "        if pd.isna(unit):\n",
    "            return 'Missing unit'\n",
    "        if str(unit).strip() in EXCLUDED_UNITS:\n",
    "            return 'Non-volumetric unit (excluded)'\n",
    "        return 'Unconvertible unit'\n",
    "\n",
    "    if flow_ls < MIN_REALISTIC_FLOW_LS:\n",
    "        return 'Negative value'\n",
    "\n",
    "    if flow_ls > MAX_REALISTIC_FLOW_LS:\n",
    "        return 'Unrealistically high value'\n",
    "\n",
    "    return 'Valid'\n",
    "\n",
    "def get_anomaly_severity(row):\n",
    "    status = row['Anomaly_Status']\n",
    "    flow_ls = row['Caudal_Ls']\n",
    "\n",
    "    if status == 'Valid':\n",
    "        return 'None'\n",
    "    elif status == 'Negative value':\n",
    "        return 'High - Data Entry Error'\n",
    "    elif status == 'Unrealistically high value':\n",
    "        if flow_ls > 10000:\n",
    "            return 'Critical - Possible Unit Confusion'\n",
    "        elif flow_ls > 1000:\n",
    "            return 'High - Review Required'\n",
    "        else:\n",
    "            return 'Medium - Verify Against Resolution'\n",
    "    else:\n",
    "        return 'Low - Missing Data'\n",
    "\n",
    "def create_geodataframe(df, lat_col='lat_wgs84', lon_col='lon_wgs84'):\n",
    "    mask = (\n",
    "        df[lat_col].notna() &\n",
    "        df[lon_col].notna() &\n",
    "        (df[lat_col] != 0) &\n",
    "        (df[lon_col] != 0) &\n",
    "        (df[lat_col] >= -56) & (df[lat_col] <= -17) &\n",
    "        (df[lon_col] >= -76) & (df[lon_col] <= -66)\n",
    "    )\n",
    "    \n",
    "    if 'Coord_Valida_Chile_BBox' in df.columns:\n",
    "        mask = mask & (df['Coord_Valida_Chile_BBox'] == True)\n",
    "\n",
    "    df_spatial = df[mask].copy()\n",
    "\n",
    "    if len(df_spatial) == 0:\n",
    "        print(\"  WARNING: No valid coordinates found!\")\n",
    "        return None\n",
    "\n",
    "    print(f\"  Creating GeoDataFrame with {len(df_spatial):,} valid points...\")\n",
    "    \n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        df_spatial,\n",
    "        geometry=gpd.points_from_xy(df_spatial[lon_col], df_spatial[lat_col]),\n",
    "        crs=TARGET_CRS\n",
    "    )\n",
    "\n",
    "    return gdf\n",
    "\n",
    "def perform_spatial_join(gdf_points, gdf_polygons, prefix):\n",
    "    if gdf_points is None or len(gdf_points) == 0:\n",
    "        print(f\"  WARNING: No points to join for {prefix}\")\n",
    "        return gdf_points\n",
    "    \n",
    "    if gdf_polygons is None or len(gdf_polygons) == 0:\n",
    "        print(f\"  WARNING: No polygons available for {prefix}\")\n",
    "        return gdf_points\n",
    "\n",
    "    print(f\"  Performing spatial join with {prefix}...\")\n",
    "    print(f\"    Points CRS: {gdf_points.crs}\")\n",
    "    print(f\"    Polygons CRS: {gdf_polygons.crs}\")\n",
    "    \n",
    "    if gdf_points.crs != gdf_polygons.crs:\n",
    "        print(f\"    Reprojecting polygons to match points CRS...\")\n",
    "        gdf_polygons = gdf_polygons.to_crs(gdf_points.crs)\n",
    "\n",
    "    cols_to_drop = [col for col in gdf_points.columns if col.startswith('index_')]\n",
    "    if cols_to_drop:\n",
    "        gdf_points = gdf_points.drop(columns=cols_to_drop)\n",
    "\n",
    "    gdf_points = gdf_points.reset_index(drop=True)\n",
    "    gdf_polygons = gdf_polygons.reset_index(drop=True)\n",
    "\n",
    "    try:\n",
    "        gdf_joined = gpd.sjoin(\n",
    "            gdf_points, \n",
    "            gdf_polygons, \n",
    "            how='left', \n",
    "            predicate='within'\n",
    "        )\n",
    "        \n",
    "        cols_to_drop = [col for col in gdf_joined.columns if col.startswith('index_')]\n",
    "        if cols_to_drop:\n",
    "            gdf_joined = gdf_joined.drop(columns=cols_to_drop)\n",
    "\n",
    "        name_col = f'{prefix}_Name'\n",
    "        if name_col in gdf_joined.columns:\n",
    "            n_matched = gdf_joined[name_col].notna().sum()\n",
    "            print(f\"    Matched {n_matched:,} of {len(gdf_joined):,} points ({100*n_matched/len(gdf_joined):.1f}%)\")\n",
    "        \n",
    "        gdf_joined = gdf_joined.reset_index(drop=True)\n",
    "        \n",
    "        return gdf_joined\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"    ERROR in spatial join: {str(e)}\")\n",
    "        return gdf_points\n",
    "\n",
    "def calculate_statistics(gdf, groupby_col, flow_col='Caudal_Ls'):\n",
    "    if groupby_col not in gdf.columns:\n",
    "        print(f\"  WARNING: Column '{groupby_col}' not found in data\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    gdf_filtered = gdf[gdf[groupby_col].notna()].copy()\n",
    "\n",
    "    if len(gdf_filtered) == 0:\n",
    "        print(f\"  WARNING: No valid data for grouping by '{groupby_col}'\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df_filtered = pd.DataFrame(gdf_filtered.drop(columns='geometry'))\n",
    "\n",
    "    stats = df_filtered.groupby(groupby_col).agg(\n",
    "        N_Permits=(flow_col, 'count'),\n",
    "        Total_Flow_Ls=(flow_col, 'sum'),\n",
    "        Mean_Flow_Ls=(flow_col, 'mean'),\n",
    "        Median_Flow_Ls=(flow_col, 'median'),\n",
    "        Std_Flow_Ls=(flow_col, 'std'),\n",
    "        Min_Flow_Ls=(flow_col, 'min'),\n",
    "        Max_Flow_Ls=(flow_col, 'max'),\n",
    "        Q25_Flow_Ls=(flow_col, lambda x: x.quantile(0.25)),\n",
    "        Q75_Flow_Ls=(flow_col, lambda x: x.quantile(0.75))\n",
    "    ).reset_index()\n",
    "\n",
    "    stats = stats.sort_values('Total_Flow_Ls', ascending=False)\n",
    "\n",
    "    numeric_cols = ['Total_Flow_Ls', 'Mean_Flow_Ls', 'Median_Flow_Ls', \n",
    "                    'Std_Flow_Ls', 'Min_Flow_Ls', 'Max_Flow_Ls', \n",
    "                    'Q25_Flow_Ls', 'Q75_Flow_Ls']\n",
    "    for col in numeric_cols:\n",
    "        if col in stats.columns:\n",
    "            stats[col] = stats[col].round(2)\n",
    "\n",
    "    return stats\n",
    "\n",
    "def parse_chilean_date(date_str):\n",
    "    if pd.isna(date_str) or str(date_str).strip() == '':\n",
    "        return None, False\n",
    "\n",
    "    date_str = str(date_str).strip()\n",
    "\n",
    "    formats_to_try = [\n",
    "        '%d-%m-%Y',\n",
    "        '%d/%m/%Y',\n",
    "        '%d-%m-%y',\n",
    "        '%d/%m/%y',\n",
    "        '%Y-%m-%d',\n",
    "        '%d.%m.%Y',\n",
    "    ]\n",
    "\n",
    "    for fmt in formats_to_try:\n",
    "        try:\n",
    "            parsed_date = datetime.strptime(date_str, fmt)\n",
    "            if 1900 <= parsed_date.year <= 2100:\n",
    "                return parsed_date, True\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    year_match = re.search(r'(\\d{4})', date_str)\n",
    "    if year_match:\n",
    "        year = int(year_match.group(1))\n",
    "        if 1900 <= year <= 2100:\n",
    "            return datetime(year, 1, 1), True\n",
    "\n",
    "    return None, False\n",
    "\n",
    "def convert_to_american_format(dt):\n",
    "    if dt is None:\n",
    "        return None\n",
    "    return dt.strftime('%m-%d-%Y')\n",
    "\n",
    "def extract_year(dt):\n",
    "    if dt is None:\n",
    "        return None\n",
    "    return dt.year\n",
    "\n",
    "def process_dates(df, primary_col, secondary_col):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PROCESSING DATES (Chilean Format)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    total_records = len(df)\n",
    "\n",
    "    df['Fecha_Parsed'] = None\n",
    "    df['Fecha_Formato_US'] = None\n",
    "    df['Ano'] = None\n",
    "    df['Fecha_Source'] = None\n",
    "\n",
    "    stats = {\n",
    "        'total_records': total_records,\n",
    "        'primary_col_present': 0,\n",
    "        'primary_col_parsed': 0,\n",
    "        'secondary_col_present': 0,\n",
    "        'secondary_col_parsed': 0,\n",
    "        'total_with_date': 0,\n",
    "        'total_without_date': 0,\n",
    "        'recovered_from_secondary': 0\n",
    "    }\n",
    "\n",
    "    primary_exists = primary_col in df.columns\n",
    "    secondary_exists = secondary_col in df.columns\n",
    "\n",
    "    if primary_exists:\n",
    "        print(f\"Primary date column found: '{primary_col}'\")\n",
    "        stats['primary_col_present'] = df[primary_col].notna().sum()\n",
    "        print(f\"  Records with non-empty primary date: {stats['primary_col_present']:,}\")\n",
    "    else:\n",
    "        print(f\"WARNING: Primary date column '{primary_col}' NOT FOUND\")\n",
    "\n",
    "    if secondary_exists:\n",
    "        print(f\"Secondary date column found: '{secondary_col}'\")\n",
    "        stats['secondary_col_present'] = df[secondary_col].notna().sum()\n",
    "        print(f\"  Records with non-empty secondary date: {stats['secondary_col_present']:,}\")\n",
    "\n",
    "    print(\"\\nParsing dates...\")\n",
    "\n",
    "    for idx in df.index:\n",
    "        parsed_date = None\n",
    "        source = None\n",
    "        \n",
    "        if primary_exists and pd.notna(df.at[idx, primary_col]):\n",
    "            parsed_date, success = parse_chilean_date(df.at[idx, primary_col])\n",
    "            if success:\n",
    "                source = 'Fecha Toma Razón'\n",
    "                stats['primary_col_parsed'] += 1\n",
    "        \n",
    "        if parsed_date is None and secondary_exists and pd.notna(df.at[idx, secondary_col]):\n",
    "            parsed_date, success = parse_chilean_date(df.at[idx, secondary_col])\n",
    "            if success:\n",
    "                source = 'Fecha Resolución/Envío'\n",
    "                stats['secondary_col_parsed'] += 1\n",
    "                stats['recovered_from_secondary'] += 1\n",
    "        \n",
    "        if parsed_date is not None:\n",
    "            df.at[idx, 'Fecha_Parsed'] = parsed_date\n",
    "            df.at[idx, 'Fecha_Formato_US'] = convert_to_american_format(parsed_date)\n",
    "            df.at[idx, 'Ano'] = extract_year(parsed_date)\n",
    "            df.at[idx, 'Fecha_Source'] = source\n",
    "            stats['total_with_date'] += 1\n",
    "        else:\n",
    "            stats['total_without_date'] += 1\n",
    "\n",
    "    df['Ano'] = pd.to_numeric(df['Ano'], errors='coerce')\n",
    "    df.loc[df['Ano'].notna(), 'Ano'] = df.loc[df['Ano'].notna(), 'Ano'].astype(int)\n",
    "\n",
    "    print(\"\\n\" + \"-\"*50)\n",
    "    print(\"DATE PROCESSING SUMMARY\")\n",
    "    print(\"-\"*50)\n",
    "    print(f\"Total records in dataset: {stats['total_records']:,}\")\n",
    "    print(f\"\\nPrimary column ('{primary_col}'):\")\n",
    "    print(f\"  - Records with data: {stats['primary_col_present']:,} ({100*stats['primary_col_present']/total_records:.1f}%)\")\n",
    "    print(f\"  - Successfully parsed: {stats['primary_col_parsed']:,} ({100*stats['primary_col_parsed']/total_records:.1f}%)\")\n",
    "\n",
    "    if secondary_exists:\n",
    "        print(f\"\\nSecondary column ('{secondary_col}'):\")\n",
    "        print(f\"  - Records with data: {stats['secondary_col_present']:,} ({100*stats['secondary_col_present']/total_records:.1f}%)\")\n",
    "        print(f\"  - Used as fallback: {stats['recovered_from_secondary']:,} ({100*stats['recovered_from_secondary']/total_records:.1f}%)\")\n",
    "\n",
    "    print(f\"\\nFINAL RESULTS:\")\n",
    "    print(f\"  ✓ Records WITH valid date: {stats['total_with_date']:,} ({100*stats['total_with_date']/total_records:.1f}%)\")\n",
    "    print(f\"  ✗ Records WITHOUT valid date: {stats['total_without_date']:,} ({100*stats['total_without_date']/total_records:.1f}%)\")\n",
    "\n",
    "    valid_years = df['Ano'].dropna()\n",
    "    if len(valid_years) > 0:\n",
    "        print(f\"\\nYear range: {int(valid_years.min())} - {int(valid_years.max())}\")\n",
    "\n",
    "    if df['Fecha_Source'].notna().any():\n",
    "        print(\"\\nDate source distribution:\")\n",
    "        source_counts = df['Fecha_Source'].value_counts()\n",
    "        for source, count in source_counts.items():\n",
    "            print(f\"  {source}: {count:,} ({100*count/stats['total_with_date']:.1f}%)\")\n",
    "\n",
    "    return df, stats\n",
    "\n",
    "def perform_temporal_analysis(gdf_valid, date_quality_stats):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TEMPORAL ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    df_temp = pd.DataFrame(gdf_valid.drop(columns='geometry'))\n",
    "    df_temp = df_temp[df_temp['Ano'].notna()].copy()\n",
    "    df_temp['Year'] = df_temp['Ano'].astype(int)\n",
    "\n",
    "    current_year = datetime.now().year\n",
    "    df_temp = df_temp[(df_temp['Year'] >= 1900) & (df_temp['Year'] <= current_year)]\n",
    "\n",
    "    if len(df_temp) == 0:\n",
    "        print(\"WARNING: No valid dates found for temporal analysis\")\n",
    "        return None, \"No valid dates found for temporal analysis.\"\n",
    "\n",
    "    print(f\"Records with valid dates for analysis: {len(df_temp):,}\")\n",
    "    print(f\"Year range: {df_temp['Year'].min()} - {df_temp['Year'].max()}\")\n",
    "\n",
    "    yearly_stats = df_temp.groupby('Year').agg(\n",
    "        N_Permits=('Caudal_Ls', 'count'),\n",
    "        Total_Flow_Ls=('Caudal_Ls', 'sum'),\n",
    "        Mean_Flow_Ls=('Caudal_Ls', 'mean'),\n",
    "        Median_Flow_Ls=('Caudal_Ls', 'median'),\n",
    "        Max_Flow_Ls=('Caudal_Ls', 'max'),\n",
    "        Min_Flow_Ls=('Caudal_Ls', 'min'),\n",
    "        Std_Flow_Ls=('Caudal_Ls', 'std'),\n",
    "        Q25_Flow_Ls=('Caudal_Ls', lambda x: x.quantile(0.25)),\n",
    "        Q75_Flow_Ls=('Caudal_Ls', lambda x: x.quantile(0.75))\n",
    "    ).reset_index()\n",
    "\n",
    "    yearly_stats = yearly_stats.sort_values('Year')\n",
    "    yearly_stats['Cumulative_Permits'] = yearly_stats['N_Permits'].cumsum()\n",
    "    yearly_stats['Cumulative_Flow_Ls'] = yearly_stats['Total_Flow_Ls'].cumsum()\n",
    "    yearly_stats['YoY_Permits_Change'] = yearly_stats['N_Permits'].diff()\n",
    "    yearly_stats['YoY_Permits_Pct_Change'] = yearly_stats['N_Permits'].pct_change() * 100\n",
    "    yearly_stats['YoY_Flow_Change'] = yearly_stats['Total_Flow_Ls'].diff()\n",
    "    yearly_stats['YoY_Flow_Pct_Change'] = yearly_stats['Total_Flow_Ls'].pct_change() * 100\n",
    "    yearly_stats['MA5_Permits'] = yearly_stats['N_Permits'].rolling(window=5, center=True, min_periods=1).mean()\n",
    "    yearly_stats['MA5_Flow'] = yearly_stats['Total_Flow_Ls'].rolling(window=5, center=True, min_periods=1).mean()\n",
    "\n",
    "    df_temp['Decade'] = (df_temp['Year'] // 10) * 10\n",
    "    decade_stats = df_temp.groupby('Decade').agg(\n",
    "        N_Permits=('Caudal_Ls', 'count'),\n",
    "        Total_Flow_Ls=('Caudal_Ls', 'sum'),\n",
    "        Mean_Flow_Ls=('Caudal_Ls', 'mean'),\n",
    "        Median_Flow_Ls=('Caudal_Ls', 'median'),\n",
    "        Max_Flow_Ls=('Caudal_Ls', 'max'),\n",
    "        Std_Flow_Ls=('Caudal_Ls', 'std')\n",
    "    ).reset_index()\n",
    "    decade_stats['Decade_Label'] = decade_stats['Decade'].astype(str) + 's'\n",
    "    decade_stats = decade_stats.sort_values('Decade')\n",
    "\n",
    "    df_temp['Period_5yr'] = (df_temp['Year'] // 5) * 5\n",
    "    period5_stats = df_temp.groupby('Period_5yr').agg(\n",
    "        N_Permits=('Caudal_Ls', 'count'),\n",
    "        Total_Flow_Ls=('Caudal_Ls', 'sum'),\n",
    "        Mean_Flow_Ls=('Caudal_Ls', 'mean'),\n",
    "        Median_Flow_Ls=('Caudal_Ls', 'median'),\n",
    "        Max_Flow_Ls=('Caudal_Ls', 'max')\n",
    "    ).reset_index()\n",
    "    period5_stats['Period_Label'] = period5_stats['Period_5yr'].astype(str) + '-' + (period5_stats['Period_5yr'] + 4).astype(str)\n",
    "    period5_stats = period5_stats.sort_values('Period_5yr')\n",
    "\n",
    "    peak_permits_year = yearly_stats.loc[yearly_stats['N_Permits'].idxmax()]\n",
    "    peak_flow_year = yearly_stats.loc[yearly_stats['Total_Flow_Ls'].idxmax()]\n",
    "\n",
    "    significant_years = yearly_stats[yearly_stats['N_Permits'] >= 10]\n",
    "    peak_mean_flow_year = None\n",
    "    if len(significant_years) > 0:\n",
    "        peak_mean_flow_year = significant_years.loc[significant_years['Mean_Flow_Ls'].idxmax()]\n",
    "\n",
    "    top10_permits_years = yearly_stats.nlargest(10, 'N_Permits')\n",
    "    top10_flow_years = yearly_stats.nlargest(10, 'Total_Flow_Ls')\n",
    "    \n",
    "    if len(significant_years) >= 10:\n",
    "        top10_mean_flow_years = significant_years.nlargest(10, 'Mean_Flow_Ls')\n",
    "    else:\n",
    "        top10_mean_flow_years = significant_years.nlargest(len(significant_years), 'Mean_Flow_Ls')\n",
    "\n",
    "    p75_permits = yearly_stats['N_Permits'].quantile(0.75)\n",
    "    p75_flow = yearly_stats['Total_Flow_Ls'].quantile(0.75)\n",
    "    p90_permits = yearly_stats['N_Permits'].quantile(0.90)\n",
    "    p90_flow = yearly_stats['Total_Flow_Ls'].quantile(0.90)\n",
    "\n",
    "    yearly_stats['High_Permits'] = yearly_stats['N_Permits'] > p75_permits\n",
    "    yearly_stats['High_Flow'] = yearly_stats['Total_Flow_Ls'] > p75_flow\n",
    "\n",
    "    high_permit_years = yearly_stats[yearly_stats['High_Permits']]['Year'].tolist()\n",
    "    high_flow_years = yearly_stats[yearly_stats['High_Flow']]['Year'].tolist()\n",
    "\n",
    "    correlation_permits_flow = yearly_stats['N_Permits'].corr(yearly_stats['Total_Flow_Ls'])\n",
    "    correlation_permits_mean_flow = yearly_stats['N_Permits'].corr(yearly_stats['Mean_Flow_Ls'])\n",
    "\n",
    "    trend_info = None\n",
    "    recent_years = yearly_stats[yearly_stats['Year'] >= 1990].copy()\n",
    "    if len(recent_years) >= 5:\n",
    "        try:\n",
    "            from scipy import stats as scipy_stats\n",
    "            x = recent_years['Year'].values\n",
    "            y_permits = recent_years['N_Permits'].values\n",
    "            y_flow = recent_years['Total_Flow_Ls'].values\n",
    "            \n",
    "            slope_permits, intercept_permits, r_permits, p_permits, se_permits = scipy_stats.linregress(x, y_permits)\n",
    "            slope_flow, intercept_flow, r_flow, p_flow, se_flow = scipy_stats.linregress(x, y_flow)\n",
    "            \n",
    "            trend_info = {\n",
    "                'permits_slope': slope_permits,\n",
    "                'permits_r_squared': r_permits**2,\n",
    "                'permits_p_value': p_permits,\n",
    "                'flow_slope': slope_flow,\n",
    "                'flow_r_squared': r_flow**2,\n",
    "                'flow_p_value': p_flow\n",
    "            }\n",
    "        except ImportError:\n",
    "            print(\"  scipy not available for trend analysis\")\n",
    "\n",
    "    regional_yearly = None\n",
    "    regional_peaks = None\n",
    "\n",
    "    if 'Region_Name' in df_temp.columns:\n",
    "        regional_yearly = df_temp.groupby(['Year', 'Region_Name']).agg(\n",
    "            N_Permits=('Caudal_Ls', 'count'),\n",
    "            Total_Flow_Ls=('Caudal_Ls', 'sum'),\n",
    "            Mean_Flow_Ls=('Caudal_Ls', 'mean')\n",
    "        ).reset_index()\n",
    "        \n",
    "        regional_peaks = regional_yearly.loc[\n",
    "            regional_yearly.groupby('Region_Name')['N_Permits'].idxmax()\n",
    "        ][['Region_Name', 'Year', 'N_Permits', 'Total_Flow_Ls']].rename(\n",
    "            columns={'Year': 'Peak_Year', 'N_Permits': 'Peak_Permits', 'Total_Flow_Ls': 'Peak_Flow_Ls'}\n",
    "        )\n",
    "\n",
    "    def find_consecutive_periods(years_list, min_gap=2):\n",
    "        if len(years_list) == 0:\n",
    "            return []\n",
    "        years_sorted = sorted(years_list)\n",
    "        periods = []\n",
    "        period_start = years_sorted[0]\n",
    "        period_end = years_sorted[0]\n",
    "        for year in years_sorted[1:]:\n",
    "            if year - period_end <= min_gap:\n",
    "                period_end = year\n",
    "            else:\n",
    "                periods.append((period_start, period_end))\n",
    "                period_start = year\n",
    "                period_end = year\n",
    "        periods.append((period_start, period_end))\n",
    "        return periods\n",
    "\n",
    "    high_permit_periods = find_consecutive_periods(high_permit_years)\n",
    "    high_flow_periods = find_consecutive_periods(high_flow_years)\n",
    "\n",
    "    temporal_stats = {\n",
    "        'yearly': yearly_stats,\n",
    "        'decade': decade_stats,\n",
    "        'period_5yr': period5_stats,\n",
    "        'peak_permits_year': peak_permits_year,\n",
    "        'peak_flow_year': peak_flow_year,\n",
    "        'peak_mean_flow_year': peak_mean_flow_year,\n",
    "        'top10_permits_years': top10_permits_years,\n",
    "        'top10_flow_years': top10_flow_years,\n",
    "        'top10_mean_flow_years': top10_mean_flow_years,\n",
    "        'correlation_permits_flow': correlation_permits_flow,\n",
    "        'correlation_permits_mean_flow': correlation_permits_mean_flow,\n",
    "        'trend_info': trend_info,\n",
    "        'regional_yearly': regional_yearly,\n",
    "        'regional_peaks': regional_peaks,\n",
    "        'date_quality_stats': date_quality_stats,\n",
    "        'total_with_dates': len(df_temp),\n",
    "        'year_range': (df_temp['Year'].min(), df_temp['Year'].max()),\n",
    "        'p75_permits': p75_permits,\n",
    "        'p75_flow': p75_flow,\n",
    "        'p90_permits': p90_permits,\n",
    "        'p90_flow': p90_flow,\n",
    "        'high_permit_years': high_permit_years,\n",
    "        'high_flow_years': high_flow_years,\n",
    "        'high_permit_periods': high_permit_periods,\n",
    "        'high_flow_periods': high_flow_periods\n",
    "    }\n",
    "\n",
    "    return temporal_stats, None\n",
    "\n",
    "def generate_temporal_summary(temporal_stats):\n",
    "    if temporal_stats is None:\n",
    "        return \"\\nTEMPORAL ANALYSIS: Not available\\n\"\n",
    "\n",
    "    lines = []\n",
    "    lines.append(\"\\n\" + \"=\"*80)\n",
    "    lines.append(\"TEMPORAL ANALYSIS OF WATER RIGHTS ALLOCATION IN CHILE\")\n",
    "    lines.append(\"=\"*80)\n",
    "\n",
    "    dq = temporal_stats['date_quality_stats']\n",
    "    lines.append(\"\\n\" + \"-\"*60)\n",
    "    lines.append(\"DATE DATA QUALITY REPORT\")\n",
    "    lines.append(\"-\"*60)\n",
    "    lines.append(f\"Total records in dataset: {dq['total_records']:,}\")\n",
    "    lines.append(f\"Records with valid dates: {dq['total_with_date']:,} ({100*dq['total_with_date']/dq['total_records']:.1f}%)\")\n",
    "    lines.append(f\"Records without valid dates: {dq['total_without_date']:,} ({100*dq['total_without_date']/dq['total_records']:.1f}%)\")\n",
    "\n",
    "    lines.append(\"\\n\" + \"-\"*60)\n",
    "    lines.append(\"PEAK YEARS\")\n",
    "    lines.append(\"-\"*60)\n",
    "    \n",
    "    peak_permits = temporal_stats['peak_permits_year']\n",
    "    lines.append(f\"\\nPeak year for permits: {int(peak_permits['Year'])} ({int(peak_permits['N_Permits']):,} permits)\")\n",
    "    \n",
    "    peak_flow = temporal_stats['peak_flow_year']\n",
    "    lines.append(f\"Peak year for total flow: {int(peak_flow['Year'])} ({peak_flow['Total_Flow_Ls']:,.2f} L/s)\")\n",
    "\n",
    "    lines.append(\"\\n\" + \"-\"*60)\n",
    "    lines.append(\"TOP 10 YEARS BY PERMITS\")\n",
    "    lines.append(\"-\"*60)\n",
    "    for rank, (idx, row) in enumerate(temporal_stats['top10_permits_years'].iterrows(), 1):\n",
    "        lines.append(f\"  {rank}. {int(row['Year'])}: {int(row['N_Permits']):,} permits, {row['Total_Flow_Ls']:,.2f} L/s\")\n",
    "\n",
    "    lines.append(\"\\n\" + \"-\"*60)\n",
    "    lines.append(\"DECADE ANALYSIS\")\n",
    "    lines.append(\"-\"*60)\n",
    "    for idx, row in temporal_stats['decade'].iterrows():\n",
    "        lines.append(f\"  {row['Decade_Label']}: {int(row['N_Permits']):,} permits, {row['Total_Flow_Ls']:,.2f} L/s total\")\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def generate_analysis_summary(gdf_valid, gdf_anomalies, stats_dict, temporal_stats=None):\n",
    "    \n",
    "    df_valid = pd.DataFrame(gdf_valid.drop(columns='geometry')) if gdf_valid is not None else pd.DataFrame()\n",
    "    df_anomalies = pd.DataFrame(gdf_anomalies.drop(columns='geometry')) if gdf_anomalies is not None else pd.DataFrame()\n",
    "\n",
    "    summary_lines = []\n",
    "    summary_lines.append(\"=\"*80)\n",
    "    summary_lines.append(\"DGA GROUNDWATER RIGHTS DATABASE ANALYSIS SUMMARY\")\n",
    "    summary_lines.append(\"(All spatial assignments from shapefiles via spatial joins)\")\n",
    "    summary_lines.append(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    summary_lines.append(\"=\"*80)\n",
    "\n",
    "    summary_lines.append(\"\\n\" + \"=\"*60)\n",
    "    summary_lines.append(\"1. DATASET OVERVIEW\")\n",
    "    summary_lines.append(\"=\"*60)\n",
    "    summary_lines.append(f\"Valid records for analysis: {len(df_valid):,}\")\n",
    "    summary_lines.append(f\"Anomalous records: {len(df_anomalies):,}\")\n",
    "\n",
    "    if len(df_valid) > 0:\n",
    "        summary_lines.append(\"\\n--- Flow Rate Statistics (Valid Records Only, L/s) ---\")\n",
    "        summary_lines.append(f\"  Total allocated flow: {df_valid['Caudal_Ls'].sum():,.2f} L/s\")\n",
    "        summary_lines.append(f\"  Mean flow per permit: {df_valid['Caudal_Ls'].mean():.2f} L/s\")\n",
    "        summary_lines.append(f\"  Median flow per permit: {df_valid['Caudal_Ls'].median():.2f} L/s\")\n",
    "\n",
    "    for scale_name, stats_df in stats_dict.items():\n",
    "        summary_lines.append(\"\\n\" + \"=\"*60)\n",
    "        summary_lines.append(f\"{scale_name.upper()} ANALYSIS (from shapefile spatial join)\")\n",
    "        summary_lines.append(\"=\"*60)\n",
    "        if len(stats_df) > 0:\n",
    "            summary_lines.append(f\"Number of {scale_name} units with data: {len(stats_df)}\")\n",
    "            summary_lines.append(f\"\\nTop 10 by Total Allocated Flow:\")\n",
    "            for idx, row in stats_df.head(10).iterrows():\n",
    "                name_col = [c for c in row.index if '_Name' in c or c == scale_name][0] if any('_Name' in c or c == scale_name for c in row.index) else row.index[0]\n",
    "                summary_lines.append(f\"  {row[name_col]}: {row['Total_Flow_Ls']:,.2f} L/s ({row['N_Permits']:,} permits)\")\n",
    "        else:\n",
    "            summary_lines.append(\"  No data available\")\n",
    "\n",
    "    if temporal_stats is not None:\n",
    "        summary_lines.append(generate_temporal_summary(temporal_stats))\n",
    "\n",
    "    return \"\\n\".join(summary_lines)\n",
    "\n",
    "def main():\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DGA GROUNDWATER RIGHTS DATABASE ANALYSIS\")\n",
    "    print(\"ALL SPATIAL ASSIGNMENTS FROM SHAPEFILES (NOT Excel columns)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "    create_output_folder(OUTPUT_FOLDER)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"LOADING REFERENCE LAYERS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    reference_gdfs = {}\n",
    "    for layer_config in REFERENCE_LAYERS:\n",
    "        gdf, prefix = load_reference_layer(layer_config)\n",
    "        if gdf is not None:\n",
    "            reference_gdfs[prefix] = gdf\n",
    "        else:\n",
    "            print(f\"  WARNING: Failed to load {prefix} layer\")\n",
    "\n",
    "    print(f\"\\nSuccessfully loaded {len(reference_gdfs)} reference layers: {list(reference_gdfs.keys())}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"LOADING DGA DATA\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    df = load_excel_data(EXCEL_PATH)\n",
    "\n",
    "    print(\"\\n--- Available columns in Excel ---\")\n",
    "    for i, col in enumerate(df.columns):\n",
    "        print(f\"  {i+1}. {col}\")\n",
    "\n",
    "    df, date_quality_stats = process_dates(df, DATE_COL_PRIMARY, DATE_COL_SECONDARY)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"UNIT CONVERSION AND ANOMALY DETECTION\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    print(\"Converting flow rates to L/s...\")\n",
    "    df['Caudal_Ls'] = df.apply(convert_flow_to_ls, axis=1)\n",
    "\n",
    "    print(\"Classifying anomalies...\")\n",
    "    df['Anomaly_Status'] = df.apply(classify_anomaly, axis=1)\n",
    "    df['Anomaly_Severity'] = df.apply(get_anomaly_severity, axis=1)\n",
    "\n",
    "    print(\"\\n--- Anomaly Classification Summary ---\")\n",
    "    for status, count in df['Anomaly_Status'].value_counts().items():\n",
    "        pct = 100 * count / len(df)\n",
    "        print(f\"  {status}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "    df_valid = df[df['Anomaly_Status'] == 'Valid'].copy()\n",
    "    df_anomalies = df[df['Anomaly_Status'].isin(['Negative value', 'Unrealistically high value'])].copy()\n",
    "    df_excluded = df[~df['Anomaly_Status'].isin(['Valid', 'Negative value', 'Unrealistically high value'])].copy()\n",
    "\n",
    "    print(f\"\\nValid records for analysis: {len(df_valid):,}\")\n",
    "    print(f\"Anomalous records (negative/unrealistic): {len(df_anomalies):,}\")\n",
    "    print(f\"Excluded records (non-volumetric/missing): {len(df_excluded):,}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CREATING GEODATAFRAMES (WGS84)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    gdf_valid = create_geodataframe(df_valid)\n",
    "    gdf_anomalies = create_geodataframe(df_anomalies)\n",
    "\n",
    "    if gdf_valid is not None:\n",
    "        print(f\"  Valid records with coordinates: {len(gdf_valid):,}\")\n",
    "    if gdf_anomalies is not None:\n",
    "        print(f\"  Anomalous records with coordinates: {len(gdf_anomalies):,}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PERFORMING SPATIAL JOINS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    for prefix, ref_gdf in reference_gdfs.items():\n",
    "        gdf_valid = perform_spatial_join(gdf_valid, ref_gdf, prefix)\n",
    "        if gdf_anomalies is not None:\n",
    "            gdf_anomalies = perform_spatial_join(gdf_anomalies, ref_gdf, prefix)\n",
    "\n",
    "    print(\"\\n--- Columns after spatial joins ---\")\n",
    "    spatial_cols = [c for c in gdf_valid.columns if '_Name' in c or '_Code' in c]\n",
    "    print(f\"  Spatial columns added: {spatial_cols}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CALCULATING STATISTICS (from spatial joins)\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    stats_dict = {}\n",
    "\n",
    "    for prefix in reference_gdfs.keys():\n",
    "        name_col = f'{prefix}_Name'\n",
    "        if name_col in gdf_valid.columns:\n",
    "            print(f\"\\nCalculating {prefix} statistics...\")\n",
    "            stats = calculate_statistics(gdf_valid, name_col)\n",
    "            if len(stats) > 0:\n",
    "                stats_dict[prefix] = stats\n",
    "                print(f\"  {prefix} units analyzed: {len(stats)}\")\n",
    "            else:\n",
    "                print(f\"  WARNING: No valid statistics for {prefix}\")\n",
    "\n",
    "    temporal_stats, temporal_error = perform_temporal_analysis(gdf_valid, date_quality_stats)\n",
    "\n",
    "    if temporal_error:\n",
    "        print(f\"WARNING: {temporal_error}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SAVING OUTPUTS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    output_valid_path = Path(OUTPUT_FOLDER) / \"DGA_Valid_Data_Analysis_SpatialJoin.xlsx\"\n",
    "    print(f\"Saving valid data analysis to: {output_valid_path}\")\n",
    "    \n",
    "    with pd.ExcelWriter(output_valid_path, engine='openpyxl') as writer:\n",
    "        summary_data = {\n",
    "            'Metric': [\n",
    "                'Total Valid Records',\n",
    "                'Total Anomalous Records',\n",
    "                'Records with Valid Dates',\n",
    "                'Total Allocated Flow (L/s)',\n",
    "                'Mean Flow per Permit (L/s)',\n",
    "                'Median Flow per Permit (L/s)',\n",
    "                'Analysis Date',\n",
    "                'Note'\n",
    "            ],\n",
    "            'Value': [\n",
    "                len(gdf_valid) if gdf_valid is not None else 0,\n",
    "                len(gdf_anomalies) if gdf_anomalies is not None else 0,\n",
    "                date_quality_stats['total_with_date'],\n",
    "                f\"{gdf_valid['Caudal_Ls'].sum():,.2f}\" if gdf_valid is not None else 0,\n",
    "                f\"{gdf_valid['Caudal_Ls'].mean():.2f}\" if gdf_valid is not None else 0,\n",
    "                f\"{gdf_valid['Caudal_Ls'].median():.2f}\" if gdf_valid is not None else 0,\n",
    "                datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'All spatial assignments from shapefiles (not Excel columns)'\n",
    "            ]\n",
    "        }\n",
    "        pd.DataFrame(summary_data).to_excel(writer, sheet_name='Summary', index=False)\n",
    "        \n",
    "        for scale_name, stats_df in stats_dict.items():\n",
    "            if len(stats_df) > 0:\n",
    "                stats_df.to_excel(writer, sheet_name=f'Stats_{scale_name}', index=False)\n",
    "        \n",
    "        if temporal_stats is not None:\n",
    "            temporal_stats['yearly'].to_excel(writer, sheet_name='Temporal_Yearly', index=False)\n",
    "            temporal_stats['decade'].to_excel(writer, sheet_name='Temporal_Decade', index=False)\n",
    "            temporal_stats['period_5yr'].to_excel(writer, sheet_name='Temporal_5Year', index=False)\n",
    "            if temporal_stats['regional_peaks'] is not None:\n",
    "                temporal_stats['regional_peaks'].to_excel(writer, sheet_name='Temporal_Regional_Peaks', index=False)\n",
    "\n",
    "    output_gdf_path = Path(OUTPUT_FOLDER) / \"DGA_Data_With_Spatial_Joins.xlsx\"\n",
    "    print(f\"Saving data with spatial joins to: {output_gdf_path}\")\n",
    "    \n",
    "    df_export = pd.DataFrame(gdf_valid.drop(columns='geometry'))\n",
    "    \n",
    "    key_cols = ['Código de Expediente', 'Caudal_Ls', 'Ano', 'lat_wgs84', 'lon_wgs84']\n",
    "    spatial_cols = [c for c in df_export.columns if '_Name' in c or '_Code' in c]\n",
    "    key_cols.extend(spatial_cols)\n",
    "    \n",
    "    available_cols = [c for c in key_cols if c in df_export.columns]\n",
    "    df_export[available_cols].to_excel(output_gdf_path, index=False)\n",
    "\n",
    "    output_clean_path = Path(OUTPUT_FOLDER) / \"DGA_Data_Clean_With_Spatial_Joins.xlsx\"\n",
    "    print(f\"Saving clean dataset to: {output_clean_path}\")\n",
    "    df_export.to_excel(output_clean_path, index=False)\n",
    "\n",
    "    if temporal_stats is not None:\n",
    "        output_temporal_path = Path(OUTPUT_FOLDER) / \"DGA_Temporal_Analysis.xlsx\"\n",
    "        print(f\"Saving temporal analysis to: {output_temporal_path}\")\n",
    "        with pd.ExcelWriter(output_temporal_path, engine='openpyxl') as writer:\n",
    "            temporal_stats['yearly'].to_excel(writer, sheet_name='Yearly_Statistics', index=False)\n",
    "            temporal_stats['decade'].to_excel(writer, sheet_name='Decade_Statistics', index=False)\n",
    "            temporal_stats['period_5yr'].to_excel(writer, sheet_name='5Year_Period_Stats', index=False)\n",
    "            temporal_stats['top10_permits_years'].to_excel(writer, sheet_name='Top10_Years_Permits', index=False)\n",
    "            temporal_stats['top10_flow_years'].to_excel(writer, sheet_name='Top10_Years_Flow', index=False)\n",
    "\n",
    "    print(\"\\nGenerating analysis summary...\")\n",
    "    summary_text = generate_analysis_summary(gdf_valid, gdf_anomalies, stats_dict, temporal_stats)\n",
    "\n",
    "    output_txt_path = Path(OUTPUT_FOLDER) / \"DGA_Analysis_Summary_SpatialJoin.txt\"\n",
    "    print(f\"Saving text summary to: {output_txt_path}\")\n",
    "    with open(output_txt_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(summary_text)\n",
    "\n",
    "    print(\"\\n\" + summary_text)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ANALYSIS COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Finished: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"\\nOutput files saved to: {OUTPUT_FOLDER}\")\n",
    "    print(\"\\nIMPORTANT: All spatial assignments (Region, Municipality, Basin, SHAC)\")\n",
    "    print(\"           were determined via spatial joins with reference shapefiles,\")\n",
    "    print(\"           NOT from existing Excel columns.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-water]",
   "language": "python",
   "name": "conda-env-.conda-water-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
